@Preamble{ " \newcommand{\noop}[1]{} " }


@article{roth_when_2021,
	title = {When {Is} {Parallel} {Trends} {Sensitive} to {Functional} {Form}?},
	journal = {Econometrica},
	author = {Roth, Jonathan and Sant'Anna, Pedro H. C.},
	year = {2023}
}

@article{mackinnon_wild_2018,
	title = {The wild bootstrap for few (treated) clusters},
	volume = {21},
	issn = {1368-423X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12107},
	doi = {10.1111/ectj.12107},
	abstract = {Inference based on cluster-robust standard errors in linear regression models, using either the Student's t-distribution or the wild cluster bootstrap, is known to fail when the number of treated clusters is very small. We propose a family of new procedures called the subcluster wild bootstrap, which includes the ordinary wild bootstrap as a limiting case. In the case of pure treatment models, where all observations within clusters are either treated or not, the latter procedure can work remarkably well. The key requirement is that all cluster sizes, regardless of treatment, should be similar. Unfortunately, the analogue of this requirement is not likely to hold for difference-in-differences regressions. Our theoretical results are supported by extensive simulations and an empirical example.},
	language = {en},
	number = {2},
	urldate = {2022-01-12},
	journal = {The Econometrics Journal},
	author = {MacKinnon, James G. and Webb, Matthew D.},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/ectj.12107},
	keywords = {Cluster-robust variance estimator, Clustered data, Difference-in-differences, Grouped data, Robust inference, Subclustering, Treatment model, Wild bootstrap, Wild cluster bootstrap},
	pages = {114--135},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/M7UDAH58/MacKinnon and Webb - 2018 - The wild bootstrap for few (treated) clusters.pdf:application/pdf},
}


@article{chernozhukov_exact_2021,
	title = {An {Exact} and {Robust} {Conformal} {Inference} {Method} for {Counterfactual} and {Synthetic} {Controls}},
	url = {http://arxiv.org/abs/1712.09089},
	abstract = {We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. We recast the causal inference problem as a counterfactual prediction and a structural breaks testing problem. This allows us to exploit insights from conformal prediction and structural breaks testing to develop permutation inference procedures that accommodate modern high-dimensional estimators, are valid under weak and easy-to-verify conditions, and are provably robust against misspecification. Our methods work in conjunction with many different approaches for predicting counterfactual mean outcomes in the absence of the policy intervention. Examples include synthetic controls, difference-in-differences, factor and matrix completion models, and (fused) time series panel data models. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution. Open-source software for implementing our conformal inference methods is available.},
	urldate = {2022-01-12},
	journal = {arXiv:1712.09089 [econ, stat]},
	author = {Chernozhukov, Victor and Wüthrich, Kaspar and Zhu, Yinchu},
	month = may,
	year = {2021},
	note = {arXiv: 1712.09089},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/YQRVS5L3/Chernozhukov et al. - 2021 - An Exact and Robust Conformal Inference Method for.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/PNMIBG2L/1712.html:text/html},
}



@article{gruber_incidence_1994,
	title = {The {Incidence} of {Mandated} {Maternity} {Benefits}},
	volume = {84},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/2118071},
	abstract = {I consider the labor-market effects of mandates which raise the costs of employing a demographically identifiable group. The efficiency of these policies will be largely dependent on the extent to which their costs are shifted to group-specific wages. I study several state and federal mandates which stipulated that childbirth be covered comprehensively in health insurance plans, raising the relative cost of insuring women of childbearing age. I find substantial shifting of the costs of these mandates to the wages of the targeted group. Correspondingly, I find little effect on total labor input for that group.},
	number = {3},
	urldate = {2022-01-12},
	journal = {The American Economic Review},
	author = {Gruber, Jonathan},
	year = {1994},
	note = {Publisher: American Economic Association},
	pages = {622--641},
	file = {JSTOR Full Text PDF:/Users/jonathanroth/Zotero/storage/AJTB4LXZ/Gruber - 1994 - The Incidence of Mandated Maternity Benefits.pdf:application/pdf},
}


@article{callaway_difference--differences_2020,
	title = {Difference-in-{Differences} with multiple time periods},
	doi = {https://doi.org/10.1016/j.jeconom.2020.12.001},
	abstract = {In this article, we consider identification, estimation, and inference procedures for treatment effect parameters using Difference-in-Differences (DiD) with (i) multiple time periods, (ii) variation in treatment timing, and (iii) when the “parallel trends assumption” holds potentially only after conditioning on observed covariates. We show that a family of causal effect parameters are identified in staggered DiD setups, even if differences in observed characteristics create non-parallel outcome dynamics between groups. Our identification results allow one to use outcome regression, inverse probability weighting, or doubly-robust estimands. We also propose different aggregation schemes that can be used to highlight treatment effect heterogeneity across different dimensions as well as to summarize the overall effect of participating in the treatment. We establish the asymptotic properties of the proposed estimators and prove the validity of a computationally convenient bootstrap procedure to conduct asymptotically valid simultaneous (instead of pointwise) inference. Finally, we illustrate the relevance of our proposed tools by analyzing the effect of the minimum wage on teen employment from 2001–2007. Open-source software is available for implementing the proposed methods.},
	journal = {Journal of Econometrics},
	author = {Callaway, Brantly and Sant’Anna, Pedro H. C.},
	year = {2021},
	volume = {225},
	number = {2},
	pages = {200--230}
}


@article{de_chaisemartin_two-way_2020,
	title = {Two-{Way} {Fixed} {Effects} {Estimators} with {Heterogeneous} {Treatment} {Effects}},
	volume = {110},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257%2Faer.20181169},
	doi = {10.1257/aer.20181169},
	abstract = {Linear regressions with period and group fixed effects are widely used to estimate treatment effects. We show that they estimate weighted sums of the average treatment effects (ATE) in each group and period, with weights that may be negative. Due to the negative weights, the linear regression coefficient may for instance be negative while all the ATEs are positive. We propose another estimator that solves this issue. In the two applications we revisit, it is significantly different from the linear regression estimator.},
	language = {en},
	number = {9},
	urldate = {2020-09-24},
	journal = {American Economic Review},
	author = {de Chaisemartin, Clément and D'Haultfœuille, Xavier},
	year = {2020},
	keywords = {Media, Single Equation Models, Single Variables: Panel Data Models, Single Variables: Cross-Sectional Models, Spatial Models, Quantile Regressions, Single Equation Models, Spatio-temporal Models, Political Processes: Rent-seeking, Lobbying, Elections, Legislatures, and Voting Behavior, Wage Level and Structure, Treatment Effect Models, Wage Differentials, Trade Unions: Objectives, Structure, and Effects, Entertainment},
	pages = {2964--2996},
	file = {Submitted Version:/Users/jonathanroth/Zotero/storage/XF2Q9KJK/de Chaisemartin and D'Haultfœuille - 2020 - Two-Way Fixed Effects Estimators with Heterogeneou.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/95BGA235/articles.html:text/html},
}


@techreport{dettmann_flexpaneldid_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Flexpaneldid: {A} {Stata} {Toolbox} for {Causal} {Analysis} with {Varying} {Treatment} {Time} and {Duration}},
	shorttitle = {Flexpaneldid},
	url = {https://papers.ssrn.com/abstract=3692458},
	abstract = {The paper presents a modification of the matching and difference-in-differences approach of Heckman et al. (1998) for the staggered treatment adoption design and a Stata tool that implements the approach. This flexible conditional difference-in-differences approach is particularly useful for causal analysis of treatments with varying start dates and varying treatment durations. Introducing more flexibility enables the user to consider individual treatment periods for the treated observations and thus circumventing problems arising in canonical difference-in-differences approaches.The open-source flexpaneldid toolbox for Stata implements the developed approach and allows comprehensive robustness checks and quality tests. The core of the paper gives comprehensive examples to explain the use of the commands and its options on the basis of a publicly accessible data set.},
	language = {en},
	number = {ID 3692458},
	urldate = {2021-12-30},
	institution = {Social Science Research Network},
	author = {Dettmann, Eva},
	year = {2020},
	doi = {10.2139/ssrn.3692458},
	keywords = {causal inference, conditional difference-in-differences, effect heterogeneity, event study design, matching, staggered treatment adoption, variation in treatment timing},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/IDKN683N/Dettmann - 2020 - Flexpaneldid A Stata Toolbox for Causal Analysis .pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/5P2G6QE6/papers.html:text/html},
}

@article{baker_how_2021,
	title = {How {Much} {Should} {We} {Trust} {Staggered} {Difference}-{In}-{Differences} {Estimates}?},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3794018},
	doi = {10.2139/ssrn.3794018},
	abstract = {Diﬀerence-in-diﬀerences analysis with staggered treatment timing is frequently used to assess the impact of policy changes on corporate outcomes in academic research. However, recent advances in econometric theory show that such designs are likely to be biased in the presence of treatment eﬀect heterogeneity. Given the pronounced use of staggered treatment designs in applied corporate ﬁnance and accounting research, this ﬁnding potentially impacts a large swath of prior ﬁndings in these ﬁelds. We survey the nascent literature and document how and when such bias arises from treatment eﬀect heterogeneity. We apply recently proposed methods to a set of prior published results, and ﬁnd that correcting for the bias induced by the staggered nature of policy adoption frequently impacts the estimated eﬀect from standard diﬀerence-indiﬀerence studies. In many cases, the reported eﬀects in prior research become indistinguishable from zero.},
	language = {en},
	urldate = {2021-12-30},
	journal = {SSRN Electronic Journal},
	author = {Baker, Andrew and Larcker, David F. and Wang, Charles C. Y.},
	year = {2021},
	file = {Baker et al. - 2021 - How Much Should We Trust Staggered Difference-In-D.pdf:/Users/jonathanroth/Zotero/storage/CFPXRWRN/Baker et al. - 2021 - How Much Should We Trust Staggered Difference-In-D.pdf:application/pdf},
}


@article{Marcus2021,
author = {Marcus, Michelle and Sant'Anna, Pedro H. C.},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Marcus, Sant'Anna - 2021 - The role of parallel trends in event study settings An application to environmental economics.pdf:pdf},
journal = {Journal of the Association of Environmental and Resource Economists},
number = {2},
pages = {235--275},
title = {{The role of parallel trends in event study settings : An application to environmental economics}},
volume = {8},
year = {2021}
}

@techreport{abadie_when_2017,
	type = {Working {Paper}},
	title = {When {Should} {You} {Adjust} {Standard} {Errors} for {Clustering}?},
	url = {https://www.nber.org/papers/w24003},
	abstract = {In empirical work in economics it is common to report standard errors that account for clustering of units. Typically, the motivation given for the clustering adjustments is that unobserved components in outcomes for units within clusters are correlated. However, because correlation may occur across more than one dimension, this motivation makes it difficult to justify why researchers use clustering in some dimensions, such as geographic, but not others, such as age cohorts or gender. This motivation also makes it difficult to explain why one should not cluster with data from a randomized experiment. In this paper, we argue that clustering is in essence a design problem, either a sampling design or an experimental design issue. It is a sampling design issue if sampling follows a two stage process where in the first stage, a subset of clusters were sampled randomly from a population of clusters, and in the second stage, units were sampled randomly from the sampled clusters. In this case the clustering adjustment is justified by the fact that there are clusters in the population that we do not see in the sample. Clustering is an experimental design issue if the assignment is correlated within the clusters. We take the view that this second perspective best fits the typical setting in economics where clustering adjustments are used. This perspective allows us to shed new light on three questions: (i) when should one adjust the standard errors for clustering, (ii) when is the conventional adjustment for clustering appropriate, and (iii) when does the conventional adjustment of the standard errors matter.},
	number = {24003},
	institution = {National Bureau of Economic Research},
	author = {Abadie, Alberto and Athey, Susan and Imbens, Guido W. and Wooldridge, Jeffrey},
	year = {2017},
	doi = {10.3386/w24003},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/5K6AKXKM/Abadie et al. - 2017 - When Should You Adjust Standard Errors for Cluster.pdf:application/pdf},
}
@article{Wooldridge2021a,
author = {Wooldridge, Jeffrey M},
file = {:C$\backslash$:/Users/psantanna/Downloads/two{\_}way{\_}mundlak{\_}20210928.pdf:pdf},
journal = {Working Paper},
pages = {1--89},
title = {{Two-Way Fixed Effects, the Two-Way Mundlak Regression, and Difference-in-Differences Estimators}},
year = {2021}
}

@article{rambachan_design-based_2020,
	title = {Design-{Based} {Uncertainty} for {Quasi}-{Experiments}},
	url = {http://arxiv.org/abs/2008.00602},
	abstract = {Social scientists are often interested in estimating causal effects in settings where all units in the population are observed (e.g. all 50 US states). Design-based approaches, which view the treatment as the random object of interest, may be more appealing than standard sampling-based approaches in such contexts. This paper develops a design-based theory of uncertainty suitable for quasi-experimental settings, in which the researcher estimates the treatment effect as if treatment was randomly assigned, but in reality treatment probabilities may depend in unknown ways on the potential outcomes. We first study the properties of the simple difference-in-means (SDIM) estimator. The SDIM is unbiased for a finite-population design-based analog to the average treatment effect on the treated (ATT) if treatment probabilities are uncorrelated with the potential outcomes in a finite population sense. We further derive expressions for the variance of the SDIM estimator and a central limit theorem under sequences of finite populations with growing sample size. We then show how our results can be applied to analyze the distribution and estimand of difference-in-differences (DiD) and two-stage least squares (2SLS) from a design-based perspective when treatment is not completely randomly assigned.},
	urldate = {2020-08-31},
	journal = {arXiv:2008.00602 [econ, stat]},
	author = {Rambachan, Ashesh and Roth, Jonathan},
	year = {2020},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/2U6EFZ32/2008.html:text/html;arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/9SAN4358/Rambachan and Roth - 2020 - Design-Based Uncertainty for Quasi-Experiments.pdf:application/pdf},
}


@article{butts_difference--differences_2021,
	title = {Difference-in-{Differences} {Estimation} with {Spatial} {Spillovers}},
	url = {http://arxiv.org/abs/2105.03737},
	abstract = {Empirical work often uses treatment assigned following geographic boundaries. When the effects of treatment cross over borders, classical difference-in-differences estimation produces biased estimates for the average treatment effect. In this paper, I introduce a potential outcomes framework to model spillover effects and decompose the estimate's bias in two parts: (1) the control group no longer identifies the counterfactual trend because their outcomes are affected by treatment and (2) changes in treated units' outcomes reflect the effect of their own treatment status and the effect from the treatment status of "close" units. I propose estimation strategies that can remove both sources of bias and semi-parametrically estimate the spillover effects themselves including in settings with staggered treatment timing. To highlight the importance of spillover effects, I revisit analyses of three place-based interventions.},
	urldate = {2021-12-29},
	journal = {arXiv:2105.03737 [econ]},
	author = {Butts, Kyle},
	year = {2021},
	keywords = {Economics - Econometrics},
	annote = {Comment: 41 pages, 3 figures, 4 tables},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/4TE98KC5/Butts - 2021 - Difference-in-Differences Estimation with Spatial .pdf:application/pdf},
}
@article{Chang2020,
author = {Chang, Neng-Chieh},
doi = {10.1093/ectj/utaa001},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/others/utaa001.pdf:pdf},
journal = {Econometrics Journal},
keywords = {causal inference,difference-in-differences,high-dimensional data,machine learn-},
pages = {177--191},
title = {{Double/debiased machine learning for difference-in-differences}},
volume = {23},
year = {2020}
}

@article{Khan2010,
author = {Khan, Shakeeb and Tamer, Elie},
doi = {10.3982/ECTA7372},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Khan, Tamer - 2010 - Irregular Identification, Support Conditions, and Inverse Weight Estimation.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {6},
pages = {2021--2042},
title = {{Irregular Identification, Support Conditions, and Inverse Weight Estimation}},
volume = {78},
year = {2010}
}

@article{Hagemann2021,
archivePrefix = {arXiv},
arxivId = {1907.01049},
author = {Hagemann, Andreas},
eprint = {1907.01049},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/hagemann{\_}rperm.pdf:pdf},
journal = {arXiv:1907.01049 [econ.EM]},
keywords = {behrens-,cluster-robust inference,permutation,randomization},
title = {{Permutation inference with a finite number of heterogeneous clusters}},
year = {2021}
}

@article{Meyer1995_aer,
author = {Meyer, Bruce D. and Viscusi, W. K. and Durbin, David L},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meyer, Viscusi, Durbin - 1995 - Workers' Compensation and Injury Duration Evidence from a Natural Experiment.pdf:pdf},
journal = {The American Economic Review},
number = {3},
pages = {322--340},
title = {{Workers' Compensation and Injury Duration: Evidence from a Natural Experiment}},
volume = {85},
year = {1995}
}


@article{Meyer1995_jbes,
author = {Meyer, Bruce D.},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meyer - 1995 - Natural and Quasi-Experiments in Economics.pdf:pdf},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {comparison groups,control groups,difference in differences,exogeneity,experimental design,observational studies},
number = {2},
pages = {151--161},
title = {{Natural and Quasi-Experiments in Economics}},
volume = {13},
year = {1995}
}

@article{Chernozhukov2020,
abstract = {We propose strategies to estimate and make inference on key features of heterogeneous effects in randomized experiments. These key features include best linear predictors of the effects on machine learning proxies, average effects sorted by impact groups, and average characteristics of most and least impacted units. The approach is valid in high dimensional settings, where the effects are proxied by machine learning methods. We post-process these proxies into the estimates of the key features. Our approach is generic, it can be used in conjunction with penalized methods, deep and shallow neural networks, canonical and new random forests, boosted trees, and ensemble methods. Estimation and inference are based on repeated data splitting to avoid overfitting and achieve validity. For inference, we take medians of p-values and medians of confidence intervals, resulting from many different data splits, and then adjust their nominal level to guarantee uniform validity. This variational inference method, which quantifies the uncertainty coming from both parameter estimation and data splitting, is shown to be uniformly valid for a large class of data generating processes. We illustrate the use of the approach with a randomized field experiment that evaluated a combination of nudges to stimulate demand for immunization in India.},
archivePrefix = {arXiv},
arxivId = {1712.04802},
author = {Chernozhukov, Victor and Demirer, Mert and Duflo, Esther and Fern{\'{a}}ndez-Val, Iv{\'{a}}n},
doi = {10.1920/wp.cem.2017.6117},
eprint = {1712.04802},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/others/1712.04802.pdf:pdf},
issn = {0898-2937},
journal = {arXiv: 1712.04802},
keywords = {agnostic inference,assumption-freeness,c18,c21,causal effects,confidence intervals,d14,g21,heterogeneous effects,immunization incentives,jel,machine learning,multiple splitting,nudges,o16,quantification of uncertainty,sample splitting,uniformly valid inference,variational p-values and},
pages = {1--52},
title = {{Generic Machine Learning Inference on Heterogeneous Treatment Effects in Randomized Experiments}},
year = {2020}
}


@article{Heckman1997,
author = {Heckman, James J. and Ichimura, Hidehiko and Todd, Petra},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heckman, Ichimura, Todd - 1997 - Matching as an econometric evaluation estimator Evidence from evaluating a job training programme.pdf:pdf},
journal = {The Review of Economic Studies},
number = {4},
pages = {605--654},
title = {{Matching as an econometric evaluation estimator: Evidence from evaluating a job training programme}},
volume = {64},
year = {1997}
}

@article{Wager2018,
abstract = {Many scientific and engineering challenges—ranging from personalized medicine to customized marketing recommendations—require an understanding of treatment effect heterogeneity. In this article, we develop a nonparametric causal forest for estimating heterogeneous treatment effects that extends Breiman's widely used random forest algorithm. In the potential outcomes framework with unconfoundedness, we show that causal forests are pointwise consistent for the true treatment effect and have an asymptotically Gaussian and centered sampling distribution. We also discuss a practical method for constructing asymptotic confidence intervals for the true treatment effect that are centered at the causal forest estimates. Our theoretical results rely on a generic Gaussian theory for a large family of random forest algorithms. To our knowledge, this is the first set of results that allows any type of random forest, including classification and regression forests, to be used for provably valid statistical inference. In experiments, we find causal forests to be substantially more powerful than classical methods based on nearest-neighbor matching, especially in the presence of irrelevant covariates.},
archivePrefix = {arXiv},
arxivId = {1510.04342},
author = {Wager, Stefan and Athey, Susan},
doi = {10.1080/01621459.2017.1319839},
eprint = {1510.04342},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/others/wager2017 (1).pdf:pdf},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Adaptive nearest neighbors matching,Asymptotic normality,Potential outcomes,Unconfoundedness},
number = {523},
pages = {1228--1242},
title = {{Estimation and Inference of Heterogeneous Treatment Effects using Random Forests}},
volume = {113},
year = {2018}
}

@article{Card1994,
abstract = {On April 1, 1992, New Jersey's minimum wage rose from {\$}4.25 to {\$}5.05 per hour. To evaluate the impact of the law we surveyed 410 fast-food restaurants in New Jersey and eastern Pennsylvania before and after the rise. Comparisons of employment growth at stores in New Jersey and Pennsylvania (where the minimum wage was constant) provide simple estimates of the effect of the higher minimum wage. We also compare employment changes at stores in New Jersey that were initially paying high wages (above {\$}5) to the changes at lower-wage stores. We find no indication that the rise in the minimum wage reduced employment.},
author = {Card, David and Krueger, Alan B},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Card, Krueger - 1994 - Minimum Wages and Employment A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania.pdf:pdf},
isbn = {00028282},
issn = {0002-8282},
journal = {American Economic Review},
number = {4},
pages = {772--793},
title = {{Minimum Wages and Employment: A Case Study of the Fast-Food Industry in New Jersey and Pennsylvania}},
volume = {84},
year = {1994}
}


@article{Lee2017,
abstract = {In this paper, we propose a doubly robust method to estimate the heterogeneity of the average treatment effect with respect to observed covariates of interest. We consider a situation where a large number of covariates are needed for identifying the average treatment effect but the covariates of interest for analyzing heterogeneity are of much lower dimension. Our proposed estimator is doubly robust and avoids the curse of dimensionality. We propose a uniform confidence band that is easy to compute, and we illustrate its usefulness via Monte Carlo experiments and an application to the effects of smoking on birth weights.},
archivePrefix = {arXiv},
arxivId = {1601.02801},
author = {Lee, Sokbae and Okui, Ryo and Whang, Yoon-Jae Jae},
doi = {10.1002/jae.2574},
eprint = {1601.02801},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee, Okui, Whang - 2017 - Doubly robust uniform confidence band for the conditional average treatment effect function(2).pdf:pdf},
issn = {10991255},
journal = {Journal of Applied Econometrics},
number = {7},
pages = {1207--1225},
title = {{Doubly robust uniform confidence band for the conditional average treatment effect function}},
volume = {32},
year = {2017}
}


@article{neyman_application_1923,
	title = {On the {Application} of {Probability} {Theory} to {Agricultural} {Experiments}. {Essay} on {Principles}. {Section} 9.},
	volume = {5},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/2245382},
	abstract = {In the portion of the paper translated here, Neyman introduces a model for the analysis of field experiments conducted for the purpose of comparing a number of crop varieties, which makes use of a double-indexed array of unknown potential yields, one index corresponding to varieties and the other to plots. The yield corresponding to only one variety will be observed on any given plot, but through an urn model embodying sampling without replacement from this doubly indexed array, Neyman obtains a formula for the variance of the difference between the averages of the observed yields of two varieties. This variance involves the variance over all plots of the potential yields and the correlation coefficient r between the potential yields of the two varieties on the same plot. Since it is impossible to estimate r directly, Neyman advises taking r = 1, observing that in practice this may lead to using too large an estimated standard deviation, when comparing two variety means.},
	number = {4},
	urldate = {2020-04-21},
	journal = {Statistical Science},
	author = {Neyman, Jerzy},
	year = {1923},
	pages = {465--472}
}



@book{fisher_design_1935,
	address = {Oxford, England},
	series = {The design of experiments},
	title = {The design of experiments},
	abstract = {Different types of experimentation are considered with reference to their logical structure, to show that valid conclusions may be drawn from them without using the disputed theory of inductive inferences, i.e., of arguing from observation to explanatory theory. This is possible if a null hypothesis is explicitly formulated when the experiment is designed; this hypothesis can never be proved, but may be disproved with whatever probability one will accept as demonstrating a positive result. Chapters II, III, and IV illustrate simple applications of the principles involved in sensitiveness, significance, tests of wider hypotheses, validity, and estimation and elimination of error. More elaborate structures are treated in later chapters. Chapter titles are: (V) the Latin square; (VI) factorial design in experimentation; (VII) confounding; (VIII) special cases of partial confounding; (IX) increase of precision by concomitant measurements: statistical control; (X) generalization of null hypotheses: fiducial probability; (XI) measurement of amount of information in general. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Oliver \& Boyd},
	author = {Fisher, R. A.},
	year = {1935},
	note = {Pages: xi, 251},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/8CSG85FP/1939-04964-000.html:text/html},
}


@article{Hagemann2020,
archivePrefix = {arXiv},
arxivId = {2010.04076},
author = {Hagemann, Andreas},
eprint = {2010.04076},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/hagemann{\_}rea.pdf:pdf},
journal = {arXiv:2010.04076 [econ.EM]},
keywords = {cluster-robust inference,clustered data,dependence,difference in differences,effects,heterogeneity,two-way fixed},
pages = {1--23},
title = {{Inference with a single treated cluster}},
year = {2020}
}


@article{robins_new_1986,
	title = {A new approach to causal inference in mortality studies with a sustained exposure period—application to control of the healthy worker survivor effect},
	volume = {7},
	issn = {0270-0255},
	url = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
	doi = {10.1016/0270-0255(86)90088-6},
	abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
	language = {en},
	number = {9},
	urldate = {2021-12-29},
	journal = {Mathematical Modelling},
	author = {Robins, James},
	month = jan,
	year = {1986},
	pages = {1393--1512},
	file = {ScienceDirect Full Text PDF:/Users/jonathanroth/Zotero/storage/QR2P32MJ/Robins - 1986 - A new approach to causal inference in mortality st.pdf:application/pdf},
}



@article{lin_agnostic_2013,
	title = {Agnostic notes on regression adjustments to experimental data: {Reexamining} {Freedman}’s critique},
	volume = {7},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Agnostic notes on regression adjustments to experimental data},
	url = {https://projecteuclid.org/euclid.aoas/1365527200},
	doi = {10.1214/12-AOAS583},
	abstract = {Freedman [Adv. in Appl. Math. 40 (2008) 180–193; Ann. Appl. Stat. 2 (2008) 176–196] critiqued ordinary least squares regression adjustment of estimated treatment effects in randomized experiments, using Neyman’s model for randomization inference. Contrary to conventional wisdom, he argued that adjustment can lead to worsened asymptotic precision, invalid measures of precision, and small-sample bias. This paper shows that in sufficiently large samples, those problems are either minor or easily fixed. OLS adjustment cannot hurt asymptotic precision when a full set of treatment–covariate interactions is included. Asymptotically valid confidence intervals can be constructed with the Huber–White sandwich standard error estimator. Checks on the asymptotic approximations are illustrated with data from Angrist, Lang, and Oreopoulos’s [Am. Econ. J.: Appl. Econ. 1:1 (2009) 136–163] evaluation of strategies to improve college students’ achievement. The strongest reasons to support Freedman’s preference for unadjusted estimates are transparency and the dangers of specification search.},
	language = {EN},
	number = {1},
	urldate = {2020-06-12},
	journal = {Annals of Applied Statistics},
	author = {Lin, Winston},
	year = {2013},
	mrnumber = {MR3086420},
	zmnumber = {06171273},
	keywords = {Analysis of covariance, covariate adjustment, program evaluation, randomization inference, robust standard errors, sandwich estimator, social experiments},
	pages = {295--318},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/SH225AFH/1365527200.html:text/html;Full Text PDF:/Users/jonathanroth/Zotero/storage/8EWCYIXH/Lin - 2013 - Agnostic notes on regression adjustments to experi.pdf:application/pdf},
}

@article{shaikh_randomization_2021,
	title = {Randomization {Tests} in {Observational} {Studies} with {Staggered} {Adoption} of {Treatment}},
	url = {http://arxiv.org/abs/1912.10610},
	abstract = {This paper considers the problem of inference in observational studies with time-varying adoption of treatment. In addition to an unconfoundedness assumption that the potential outcomes are independent of the times at which units adopt treatment conditional on the units' observed characteristics, our analysis assumes that the time at which each unit adopts treatment follows a Cox proportional hazards model. This assumption permits the time at which each unit adopts treatment to depend on the observed characteristics of the unit, but imposes the restriction that the probability of multiple units adopting treatment at the same time is zero. In this context, we study Fisher-style randomization tests of a null hypothesis that specifies that there is no treatment effect for all units and all time periods in a distributional sense. We first show that an infeasible test that treats the parameters of the Cox model as known has rejection probability no greater than the nominal level in finite samples. We then establish that the feasible test that replaces these parameters with consistent estimators has limiting rejection probability no greater than the nominal level. In a simulation study, we examine the practical relevance of our theoretical results, including robustness to misspecification of the model for the time at which each unit adopts treatment. Finally, we provide an empirical application of our methodology using the synthetic control-based test statistic and tobacco legislation data found in Abadie et. al. (2010).},
	urldate = {2021-12-29},
	journal = {arXiv:1912.10610 [stat]},
	author = {Shaikh, Azeem and Toulis, Panos},
	year = {2021},
	keywords = {Statistics - Applications, Statistics - Methodology},
	annote = {Comment: 30 pages, 2 figures, 6 tables},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/9T4M5ZSL/Shaikh and Toulis - 2021 - Randomization Tests in Observational Studies with .pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/ZK8W5392/1912.html:text/html},
}

@article{mckenzie_beyond_2012,
	title = {Beyond baseline and follow-up: {The} case for more {T} in experiments},
	volume = {99},
	issn = {0304-3878},
	shorttitle = {Beyond baseline and follow-up},
	url = {https://econpapers.repec.org/article/eeedeveco/v_3a99_3ay_3a2012_3ai_3a2_3ap_3a210-221.htm},
	abstract = {The vast majority of randomized experiments in economics rely on a single baseline and single follow-up survey. While such a design is suitable for study of highly autocorrelated and relatively precisely measured outcomes in the health and education domains, it is unlikely to be optimal for measuring noisy and relatively less autocorrelated outcomes such as business profits, and household incomes and expenditures. Taking multiple measurements of such outcomes at relatively short intervals allows one to average out noise, increasing power. When the outcomes have low autocorrelation and budget is limited, it can make sense to do no baseline at all. Moreover, I show how for such outcomes, more power can be achieved with multiple follow-ups than allocating the same total sample size over a single follow-up and baseline. I also highlight the large gains in power from ANCOVA analysis rather than difference-in-differences analysis when autocorrelations are low.},
	number = {2},
	urldate = {2020-06-25},
	journal = {Journal of Development Economics},
	author = {McKenzie, David},
	year = {2012},
	keywords = {Multiple measurements, Program evaluation, Randomized experiments},
	pages = {210--221},
	file = {RePEc Snapshot:/Users/jonathanroth/Zotero/storage/3R8W247U/v_3a99_3ay_3a2012_3ai_3a2_3ap_3a210-221.html:text/html},
}


@article{roth_efficient_2021,
	title = {Efficient {Estimation} for {Staggered} {Rollout} {Designs}},
	url = {http://arxiv.org/abs/2102.01291},
	abstract = {This paper studies efficient estimation of causal effects when treatment is (quasi-) randomly rolled out to units at different points in time. We solve for the most efficient estimator in a class of estimators that nests two-way fixed effects models and other popular generalized difference-in-differences methods. A feasible plug-in version of the efficient estimator is asymptotically unbiased with efficiency (weakly) dominating that of existing approaches. We provide both \$t\$-based and permutation-test based methods for inference. We illustrate the performance of the plug-in efficient estimator in simulations and in an application to Wood et al. (2020a)'s study of the staggered rollout of a procedural justice training program for police officers. We find that confidence intervals based on the plug-in efficient estimator have good coverage and can be as much as five times shorter than confidence intervals based on existing state-of-the-art methods. As an empirical contribution of independent interest, our application provides the most precise estimates to date on the effectiveness of procedural justice training programs for police officers.},
	urldate = {2021-11-03},
	journal = {arXiv:2102.01291 [econ, math, stat]},
	author = {Roth, Jonathan and Sant'Anna, Pedro H. C.},
	year = {2021},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/PZYI653A/Roth and Sant'Anna - 2021 - Efficient Estimation for Staggered Rollout Designs.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/YRMVNWX7/2102.html:text/html},
}


@article{callaway_quantile_2019,
	title = {Quantile treatment effects in difference in differences models with panel data},
	volume = {10},
	issn = {1759-7323},
	url = {http://qeconomics.org/ojs/index.php/qe/article/view/704},
	doi = {10.3982/QE935},
	language = {en},
	number = {4},
	urldate = {2020-04-23},
	journal = {Quantitative Economics},
	author = {Callaway, Brantly and Li, Tong},
	year = {2019},
	pages = {1579--1618},
	file = {Callaway and Li - 2019 - Quantile treatment effects in difference in differ.pdf:/Users/jonathanroth/Zotero/storage/APLYNLBI/Callaway and Li - 2019 - Quantile treatment effects in difference in differ.pdf:application/pdf},
}


@article{Conley2011,
author = {Conley, Timothy G. and Taber, Christopher R.},
doi = {10.1162/REST_a_00049},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Conley, Taber - 2011 - Inference with ``Difference in Differences'' with a Small Number of Policy Changes.pdf:pdf},
issn = {0034-6535},
journal = {Review of Economics and Statistics},
number = {1},
pages = {113--125},
title = {{Inference with ``Difference in Differences'' with a Small Number of Policy Changes}},
volume = {93},
year = {2011}
}
@article{Moulton1986,
abstract = {When explanatory variable data in a regression model are drawn from a population with grouped structure, the regression errors are often correlated within groups. Error component and random coefficient regression models are considered as models of the intraclass correlation. This paper analyzes several empirical examples to investigate the applicability of random effects models and the consequences of inappropriately using ordinary least squares (OLS) estimation in the presence of random group effects. The principal findings are that the assumption of independent errors is usually incorrect and the unadjusted OLS standard errors often have a substantial downward bias, suggesting a considerable danger of spurious regression. {\textcopyright} 1986.},
author = {Moulton, Brent R.},
doi = {10.1016/0304-4076(86)90021-7},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/moulton1986.pdf:pdf},
issn = {03044076},
journal = {Journal of Econometrics},
number = {3},
pages = {385--397},
title = {{Random group effects and the precision of regression estimates}},
volume = {32},
year = {1986}
}

@article{Canay2017,
abstract = {This paper develops a theory of randomization tests under an approximate symmetry assumption. Randomization tests provide a general means of constructing tests that control size in finite samples whenever the distribution of the observed data exhibits symmetry under the null hypothesis. Here, by exhibits symmetry we mean that the distribution remains invariant under a group of transformations. In this paper, we provide conditions under which the same construction can be used to construct tests that asymptotically control the probability of a false rejection whenever the distribution of the observed data exhibits approximate symmetry in the sense that the limiting distribution of a function of the data exhibits symmetry under the null hypothesis. An important application of this idea is in settings where the data may be grouped into a fixed number of "clusters" with a large number of observations within each cluster. In such settings, we show that the distribution of the observed data satisfies our approximate symmetry requirement under weak assumptions. In particular, our results allow for the clusters to be heterogeneous and also have dependence not only within each cluster, but also across clusters. This approach enjoys several advantages over other approaches in these settings.},
author = {Canay, Ivan A. and Romano, Joseph P. and Shaikh, Azeem M.},
doi = {10.3982/ecta13081},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/ecta13081.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {3},
pages = {1013--1030},
title = {{Randomization Tests Under an Approximate Symmetry Assumption}},
volume = {85},
year = {2017}
}

@article{Ibragimov2016,
abstract = {Suppose estimating a model on each of a small number of potentially heterogeneous clusters yields approximately independent, unbiased, and Gaussian parameter estimators. We make two contributions in this setup. First, we showhowto compare a scalar parameter of interest between treatment and control units using a Two-Sample T-Statistic, extending previous results for the One-Sample T-Statistic. Second, we develop a test for the appropriate level of clustering, it tests the null hypothesis that clustered standard errors from a much finer partition are correct. We illustrate the approach by revisiting empirical studies involving clustered, time series, and spatially correlated data.},
author = {Ibragimov, Rustam and M{\"{u}}ller, Ulrich K.},
doi = {10.1162/REST_a_00545},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/BehrensFisher.pdf:pdf},
issn = {15309142},
journal = {Review of Economics and Statistics},
number = {1},
pages = {83--96},
title = {{Inference with few heterogeneous clusters}},
volume = {98},
year = {2016}
}
@article{Canay2021,
abstract = {This paper studies the wild bootstrap–based test proposed in Cameron, Gelbach, and Miller (2008). Existing analyses of its properties require that number of clusters is “large.” In an asymptotic framework in which the number of clusters is “small,” we provide conditions under which an unstudentized version of the test is valid. These conditions include homogeneity-like restrictions on the distribution of covariates. We further establish that a studentized version of the test may only overreject the null hypothesis by a “small” amount that decreases exponentially with the number of clusters. We obtain a qualitatively similar result for “score” bootstrap-based tests, which permit testing in nonlinear models.},
author = {Canay, Ivan A. and Santos, Andres and Shaikh, Azeem M.},
doi = {10.1162/rest_a_00887},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/wildfewclusters.pdf:pdf},
issn = {15309142},
journal = {Review of Economics and Statistics},
number = {2},
pages = {346--363},
title = {{The wild bootstrap with a “small” number of “large” clusters}},
volume = {103},
year = {2021}
}
@article{Cameron2008,
abstract = {Researchers have increasingly realized the need to account for within-group dependence in estimating standard errors of regression parameter estimates. The usual solution is to calculate cluster-robust standard errors that permit heteroskedasticity and within-cluster error correlation, but presume that the number of clusters is large. Standard asymptotic tests can over-reject, however, with few (five to thirty) clusters. We investigate inference using cluster bootstrap-t procedures that provide asymptotic refinement. These procedures are evaluated using Monte Carlos, including the example of Bertrand, Duflo, and Mullain-athan (2004). Rejection rates of 10{\%} using standard methods can be reduced to the nominal size of 5{\%} using our methods.},
author = {Cameron, A Colin and Gelbach, Jonah B and Miller, Douglas L},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cameron, Gelbach, Miller - 2008 - Bootstrap-Based Improvements for Inference With Clustered Errors.pdf:pdf},
journal = {Review of Economics and Statistics},
number = {3},
pages = {414--427},
title = {{Bootstrap-Based Improvements for Inference With Clustered Errors}},
volume = {90},
year = {2008}
}


@article{Ferman2018,
abstract = {We derive an inference method that works in differences-in-differences settings with few treated and many control groups in the presence of heteroskedasticity. As a leading example, we provide theoretical justification and empirical evidence that heteroskedasticity generated by variation in group sizes can invalidate existing inference methods, even in data sets with a large number of observations per group. In contrast, our inference method remains valid in this case. Our test can also be combined with feasible generalized least squares, providing a safeguard against misspecification of the serial correlation.},
archivePrefix = {arXiv},
arxivId = {suresh govindarajan},
author = {Ferman, Bruno and Pinto, Cristine},
doi = {10.1162/rest_a_00759},
eprint = {suresh govindarajan},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferman, Pinto - 2019 - Inference in Differences-in-Differences with Few Treated Groups and Heteroskedasticity.pdf:pdf},
isbn = {2007510134049},
issn = {0034-6535},
journal = {The Review of Economics and Statistics},
keywords = {and gabriel ulyssea for,andre portela,bernardo guimaraes,bootstrap,c12,c21,c33,chris taber,clustering,comments and suggestions,differences-in-differences,few clusters,heteroskedasticity,inference,jel codes,lance lochner,rodrigo soares,sergio firpo,thank josh angrist,vitor possebom,vladimir ponczek,we would like to},
number = {3},
pages = {452--467},
pmid = {17411427},
title = {{Inference in Differences-in-Differences with Few Treated Groups and Heteroskedasticity}},
url = {https://www.mitpressjournals.org/doi/abs/10.1162/rest{\_}a{\_}00759},
volume = {101},
year = {2019}
}


@article{currie_technology_2020,
	title = {Technology and {Big} {Data} {Are} {Changing} {Economics}: {Mining} {Text} to {Track} {Methods}},
	volume = {110},
	issn = {2574-0768},
	shorttitle = {Technology and {Big} {Data} {Are} {Changing} {Economics}},
	url = {https://www.aeaweb.org/articles?id=10.1257/pandp.20201058},
	doi = {10.1257/pandp.20201058},
	abstract = {The last 40 years have seen huge innovations in computing and in the availability of data. Data derived from millions of administrative records or by using (as we do) new methods of data generation such as text mining are now common. New data often requires new methods, which in turn can inspire new data collection. If history is any guide, some methods will stick and others will prove to be a flash in the pan. However, the larger trends toward demanding greater credibility and transparency from researchers in applied economics and a 'collage' approach to assembling evidence will likely continue.},
	language = {en},
	urldate = {2021-01-12},
	journal = {AEA Papers and Proceedings},
	author = {Currie, Janet and Kleven, Henrik and Zwiers, Esmée},
	month = may,
	year = {2020},
	keywords = {Computer Programs: General, Large Data Sets: Modeling and Analysis, Data Collection and Data Estimation Methodology},
	pages = {42--48},
	file = {Submitted Version:C\:\\Users\\jorot\\Zotero\\storage\\V8382NI6\\Currie et al. - 2020 - Technology and Big Data Are Changing Economics Mi.pdf:application/pdf;Snapshot:C\:\\Users\\jorot\\Zotero\\storage\\SPI38EDJ\\articles.html:text/html}
}



@article{roth_whats_2022,
	title = {What's {Trending} in {Difference}-in-{Differences}? {A} {Synthesis} of the {Recent} {Econometrics} {Literature}},
	shorttitle = {What's {Trending} in {Difference}-in-{Differences}?},
	url = {http://arxiv.org/abs/2201.01194},
	abstract = {This paper synthesizes recent advances in the econometrics of difference-in-differences (DiD) and provides concrete recommendations for practitioners. We begin by articulating a simple set of "canonical" assumptions under which the econometrics of DiD are well-understood. We then argue that recent advances in DiD methods can be broadly classified as relaxing some components of the canonical DiD setup, with a focus on \$(i)\$ multiple periods and variation in treatment timing, \$(ii)\$ potential violations of parallel trends, or \$(iii)\$ alternative frameworks for inference. Our discussion highlights the different ways that the DiD literature has advanced beyond the canonical model, and helps to clarify when each of the papers will be relevant for empirical work. We conclude by discussing some promising areas for future research.},
	urldate = {2022-01-13},
	journal = {arXiv:2201.01194 [econ, stat]},
	author = {Roth, Jonathan and Sant'Anna, Pedro H. C. and Bilinski, Alyssa and Poe, John},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.01194},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/QGXU2QBE/Roth et al. - 2022 - What's Trending in Difference-in-Differences A Sy.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/LCKXEB3E/2201.html:text/html},
}


@article{roth_when_2021,
	title = {When {Is} {Parallel} {Trends} {Sensitive} to {Functional} {Form}?},
	url = {http://arxiv.org/abs/2010.04814},
	abstract = {This paper assesses when the validity of difference-in-differences and related estimators depends on functional form. We provide a novel characterization: the parallel trends assumption holds under all strictly monotonic transformations of the outcome if and only if a stronger "parallel trends"-type condition holds for the cumulative distribution function of untreated potential outcomes. This condition is satisfied if and essentially only if the population can be partitioned into a subgroup for which treatment is effectively randomly assigned and a remaining subgroup for which the distribution of untreated potential outcomes is stable over time. We show further that it is impossible to construct any estimator that is consistent (or unbiased) for the average treatment effect on the treated (ATT) without either imposing functional form restrictions or imposing assumptions that identify the full distribution of untreated potential outcomes. Our results suggest that researchers who wish to point-identify the ATT should justify one of the following: (i) why treatment is as-if randomly assigned, (ii) why the chosen functional form is correct at the exclusion of others, or (iii) a method for inferring the entire counterfactual distribution of untreated potential outcomes.},
	urldate = {2021-02-01},
	journal = {arXiv:2010.04814 [econ, stat]},
	author = {Roth, Jonathan and Sant'Anna, Pedro H. C.},
	year = {2021},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/BHRPFINY/2010.html:text/html;arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/59Y83WKG/Roth and Sant'Anna - 2021 - When Is Parallel Trends Sensitive to Functional Fo.pdf:application/pdf},
}

@article{dette_difference--differences_2020,
	title = {Difference-in-{Differences} {Estimation} {Under} {Non}-{Parallel} {Trends}},
	journal = {Working Paper},
	author = {Dette, Holger and Schumann, Martin},
	year = {2020},
}

@article{roth_should_2018,
	title = {Should {We} {Adjust} for the {Test} for {Pre}-trends in {Difference}-in-{Difference} {Designs}?},
	url = {http://arxiv.org/abs/1804.01208},
	abstract = {The common practice in difference-in-difference (DiD) designs is to check for parallel trends prior to treatment assignment, yet typical estimation and inference does not account for the fact that this test has occurred. I analyze the properties of the traditional DiD estimator conditional on having passed (i.e. not rejected) the test for parallel pre-trends. When the DiD design is valid and the test for pre-trends confirms it, the typical DiD estimator is unbiased, but traditional standard errors are overly conservative. Additionally, there exists an alternative unbiased estimator that is more efficient than the traditional DiD estimator under parallel trends. However, when in population there is a non-zero pre-trend but we fail to reject the hypothesis of parallel pre-trends, the DiD estimator is generally biased relative to the population DiD coefficient. Moreover, if the trend is monotone, then under reasonable assumptions the bias from conditioning exacerbates the bias relative to the true treatment effect. I propose new estimation and inference procedures that account for the test for parallel trends, and compare their performance to that of the traditional estimator in a Monte Carlo simulation.},
	urldate = {2021-12-23},
	journal = {arXiv:1804.01208 [econ, math, stat]},
	author = {Roth, Jonathan},
	year = {2018},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/FJ3Y3BXP/Roth - 2018 - Should We Adjust for the Test for Pre-trends in Di.pdf:application/pdf},
}


@techreport{de_chaisemartin_two-way-survey_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Two-{Way} {Fixed} {Effects} and {Differences}-in-{Differences} with {Heterogeneous} {Treatment} {Effects}: {A} {Survey}},
	shorttitle = {Two-{Way} {Fixed} {Effects} and {Differences}-in-{Differences} with {Heterogeneous} {Treatment} {Effects}},
	url = {https://papers.ssrn.com/abstract=3980758},
	abstract = {Linear regressions with period and group fixed effects are widely used to estimate policies' effects: 26 of the 100 most cited papers published by the American Economic Review from 2015 to 2019 estimate such regressions. It has recently been show that those regressions may produce misleading estimates, if the policy's effect is heterogeneous between groups or over time, as is often the case. This survey reviews a fast-growing literature that documents this issue, and that proposes alternative estimators robust to heterogeneous effects.},
	language = {en},
	number = {ID 3980758},
	urldate = {2021-12-23},
	institution = {Social Science Research Network},
	author = {de Chaisemartin, Clément and D'Haultfœuille, Xavier},
	year = {2021},
	doi = {10.2139/ssrn.3980758},
	keywords = {Clément de Chaisemartin, SSRN, Two-Way Fixed Effects and Differences-in-Differences with Heterogeneous Treatment Effects: A Survey, Xavier D'Haultfœuille},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/MYHQVDI3/de Chaisemartin and D'Haultfœuille - 2021 - Two-Way Fixed Effects and Differences-in-Differenc.pdf:application/pdf},
}

@article{callaway_difference--differences_2021,
	title = {Difference-in-{Differences} with a {Continuous} {Treatment}},
	url = {http://arxiv.org/abs/2107.02637},
	abstract = {This paper analyzes difference-in-differences setups with a continuous treatment. We show that treatment effect on the treated-type parameters can be identified under a generalized parallel trends assumption that is similar to the binary treatment setup. However, interpreting differences in these parameters across different values of the treatment can be particularly challenging due to treatment effect heterogeneity. We discuss alternative, typically stronger, assumptions that alleviate these challenges. We also provide a variety of treatment effect decomposition results, highlighting that parameters associated with popular two-way fixed-effect specifications can be hard to interpret, even when there are only two time periods. We introduce alternative estimation strategies that do not suffer from these drawbacks. Our results also cover cases where (i) there is no available untreated comparison group and (ii) there are multiple periods and variation in treatment timing, which are both common in empirical work.},
	urldate = {2021-12-23},
	journal = {arXiv:2107.02637 [econ]},
	author = {Callaway, Brantly and Goodman-Bacon, Andrew and Sant'Anna, Pedro H. C.},
	year = {2021},
	annote = {Comment: 74 pages, 10 figures},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/WQCUD8ZQ/Callaway et al. - 2021 - Difference-in-Differences with a Continuous Treatm.pdf:application/pdf},
}

@techreport{de_chaisemartin_two-way_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Two-way {Fixed} {Effects} {Regressions} with {Several} {Treatments}},
	url = {https://papers.ssrn.com/abstract=3751060},
	abstract = {We study regressions with period and group fixed effects and several treatment variables. Under a parallel trends assumption, the coefficient on each treatment identifies the sum of two terms. The first term is a weighted sum of the effect of that treatment in each group and period, with weights that may be negative and sum to one. The second term is a sum of the effects of the other treatments, with weights summing to zero. Accordingly, coefficients in those regressions are not robust to heterogeneous effects, and may be contaminated by the effect of other treatments. We propose alternative estimators that are robust to heterogeneous effects, and that do not suffer from the contamination problem.},
	language = {en},
	number = {ID 3751060},
	urldate = {2021-12-23},
	institution = {Social Science Research Network},
	author = {de Chaisemartin, Clément and D'Haultfœuille, Xavier},
	year = {2021},
	doi = {10.2139/ssrn.3751060},
	keywords = {Clément de Chaisemartin, SSRN, Two-way Fixed Effects Regressions with Several Treatments, Xavier D'Haultfœuille},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/ZZ9Y3S2M/de Chaisemartin and D'Haultfœuille - 2020 - Two-way Fixed Effects Regressions with Several Tre.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/XEXD6IXC/papers.html:text/html},
}

@techreport{de_chaisemartin_two-way-several_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Two-way {Fixed} {Effects} {Regressions} with {Several} {Treatments}},
	url = {https://papers.ssrn.com/abstract=3751060},
	abstract = {We study regressions with period and group fixed effects and several treatment variables. Under a parallel trends assumption, the coefficient on each treatment identifies the sum of two terms. The first term is a weighted sum of the effect of that treatment in each group and period, with weights that may be negative and sum to one. The second term is a sum of the effects of the other treatments, with weights summing to zero. Accordingly, coefficients in those regressions are not robust to heterogeneous effects, and may be contaminated by the effect of other treatments. We propose alternative estimators that are robust to heterogeneous effects, and that do not suffer from the contamination problem.},
	language = {en},
	number = {ID 3751060},
	urldate = {2021-12-23},
	institution = {Social Science Research Network},
	author = {de Chaisemartin, Clément and D'Haultfœuille, Xavier},
	year = {2020},
	doi = {10.2139/ssrn.3751060},
	keywords = {Clément de Chaisemartin, SSRN, Two-way Fixed Effects Regressions with Several Treatments, Xavier D'Haultfœuille},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/ZZ9Y3S2M/de Chaisemartin and D'Haultfœuille - 2020 - Two-way Fixed Effects Regressions with Several Tre.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/XEXD6IXC/papers.html:text/html},
}



@techreport{de_chaisemartin_difference--differences_2020,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Difference-in-{Differences} {Estimators} of {Intertemporal} {Treatment} {Effects}},
	url = {https://papers.ssrn.com/abstract=3731856},
	abstract = {We consider the estimation of the effect of a treatment, using panel data where groups of units are exposed to different doses of the treatment at different times. We consider two sets of parameters of interest. The first are the average effects of having changed treatment for the first time \${\textbackslash}ell\$ periods ago. Those parameters generalize the average effect of having started receiving the treatment \${\textbackslash}ell\$ periods ago that has often been estimated in applications with a binary treatment and staggered adoption. We also consider cost-benefit ratios a planner may use to compare the treatments actually assigned to a counterfactual status quo scenario where groups receive their period-one treatment throughout the panel. We show that under common trends conditions, all these parameters are unbiasedly estimated by weighted sums of differences-in-differences. Our estimators are valid if the treatment effect is heterogeneous, contrary to the commonly-used dynamic two-way fixed effects regressions. We also propose estimators of a dynamic linear model, with group-specific but time-invariant effects of the current and lagged treatments, which may be used to evaluate ex-ante the effect of future policies. In an application, we find that our estimators differ substantially from those obtained with dynamic two-way fixed effects regressions.},
	language = {en},
	number = {ID 3731856},
	urldate = {2021-12-23},
	institution = {Social Science Research Network},
	author = {de Chaisemartin, Clément and D'Haultfœuille, Xavier},
	year = {2020},
	doi = {10.2139/ssrn.3731856},
	keywords = {Clément de Chaisemartin, Difference-in-Differences Estimators of Intertemporal Treatment Effects, SSRN, Xavier D'Haultfœuille},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/T57QP62W/de Chaisemartin and D'Haultfœuille - 2020 - Difference-in-Differences Estimators of Intertempo.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/R7CQFAUG/papers.html:text/html},
}


@article{gardner_two-stage_2021,
	title = {Two-stage diﬀerences in diﬀerences},
	abstract = {A recent literature has shown that when adoption of a treatment is staggered and average treatment eﬀects vary across groups and over time, diﬀerence-in-diﬀerences regression does not identify an easily interpretable measure of the typical eﬀect of the treatment. In this paper, I extend this literature in two ways. First, I provide some simple underlying intuition for why diﬀerence-in-diﬀerences regression does not identify a group×period average treatment eﬀect. Second, I propose an alternative twostage estimation framework, motivated by this intuition. In this framework, group and period eﬀects are identiﬁed in a ﬁrst stage from the sample of untreated observations, and average treatment eﬀects are identiﬁed in a second stage by comparing treated and untreated outcomes, after removing these group and period eﬀects. The two-stage approach is robust to treatment-eﬀect heterogeneity under staggered adoption, and can be used to identify a host of diﬀerent average treatment eﬀect measures. It is also simple, intuitive, and easy to implement. I establish the theoretical properties of the two-stage approach and demonstrate its eﬀectiveness and applicability using Monte-Carlo evidence and an example from the literature.},
	language = {en},
	journal = {Working Paper},
	author = {Gardner, John},
	file = {Gardner - Two-stage diﬀerences in diﬀerences.pdf:/Users/jonathanroth/Zotero/storage/NZWFZ8II/Gardner - Two-stage diﬀerences in diﬀerences.pdf:application/pdf},
	year={2021}
}

@article{borusyak_revisiting_2021,
	title = {Revisiting {Event} {Study} {Designs}: {Robust} and {Efficient} {Estimation}},
	shorttitle = {Revisiting {Event} {Study} {Designs}},
	url = {http://arxiv.org/abs/2108.12419},
	abstract = {A broad empirical literature uses "event study," or "difference-in-differences with staggered rollout," research designs for treatment effect estimation: settings in which units in the panel receive treatment at different times. We show a series of problems with conventional regression-based two-way fixed effects estimators, both static and dynamic. These problems arise when researchers conflate the identifying assumptions of parallel trends and no anticipatory effects, implicit assumptions that restrict treatment effect heterogeneity, and the specification of the estimand as a weighted average of treatment effects. We then derive the efficient estimator robust to treatment effect heterogeneity for this setting, show that it has a particularly intuitive "imputation" form when treatment-effect heterogeneity is unrestricted, characterize its asymptotic behavior, provide tools for inference, and illustrate its attractive properties in simulations. We further discuss appropriate tests for parallel trends, and show how our estimation approach extends to many settings beyond standard event studies.},
	urldate = {2021-12-23},
	journal = {arXiv:2108.12419 [econ]},
	author = {Borusyak, Kirill and Jaravel, Xavier and Spiess, Jann},
	year = {2021},
	keywords = {Economics - Econometrics},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/N5JJH897/Borusyak et al. - 2021 - Revisiting Event Study Designs Robust and Efficie.pdf:application/pdf},
}


@article{abadie_synthetic_2010,
	title = {Synthetic {Control} {Methods} for {Comparative} {Case} {Studies}: {Estimating} the {Effect} of {California}’s {Tobacco} {Control} {Program}},
	volume = {105},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Synthetic {Control} {Methods} for {Comparative} {Case} {Studies}},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2009.ap08746},
	doi = {10.1198/jasa.2009.ap08746},
	language = {en},
	number = {490},
	urldate = {2019-05-28},
	journal = {Journal of the American Statistical Association},
	author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
	month = jun,
	year = {2010},
	pages = {493--505},
	file = {Abadie et al. - 2010 - Synthetic Control Methods for Comparative Case Stu.pdf:/Users/abilinski/Zotero/storage/GHGPLTES/Abadie et al. - 2010 - Synthetic Control Methods for Comparative Case Stu.pdf:application/pdf}
}

@article{li_statistical_2019,
	title = {Statistical {Inference} for {Average} {Treatment} {Effects} {Estimated} by {Synthetic} {Control} {Methods}},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2019.1686986},
	doi = {10.1080/01621459.2019.1686986},
	abstract = {The synthetic control method (SCM), a powerful tool for estimating average treatment eﬀects (ATE), is increasingly popular in ﬁelds such as statistics, economics,marketing and management. The SCM is particularly suitable for estimating ATE with a single (or a few) treated unit(s), a ﬁxed number of control units, and both pre- and posttreatment periods are large (we term this type data as ‘long panels’). To date, there has been no formal inference theory for synthetic control ATE estimator with long panels under general conditions. Existing work mostly use placebo tests for inference or by some permutation methods when the post-treatment period is small. In this paper I derive the asymptotic distribution of the synthetic control and modiﬁed synthetic control ATE estimators using projection theory and show that a properly designed subsampling method can be used to obtain conﬁdence intervals and conduct inference whereas the standard bootstrap cannot. Simulations and an empirical application examine the eﬀect of opening a physical showroom by an e-tailer and demonstrate the usefulness of the modiﬁed synthetic control method in applications.},
	language = {en},
	urldate = {2020-02-26},
	journal = {Journal of the American Statistical Association},
	author = {Li, Kathleen T.},
	month = dec,
	year = {2019},
	pages = {1--16},
	file = {Li - 2019 - Statistical Inference for Average Treatment Effect.pdf:/Users/abilinski/Zotero/storage/476RX2KT/Li - 2019 - Statistical Inference for Average Treatment Effect.pdf:application/pdf}
}

@article{chernozhukov_exact_2019,
	title = {An {Exact} and {Robust} {Conformal} {Inference} {Method} for {Counterfactual} and {Synthetic} {Controls}},
	url = {http://arxiv.org/abs/1712.09089},
	abstract = {We introduce new inference procedures for counterfactual and synthetic control methods for policy evaluation. Our methods work in conjunction with many diﬀerent approaches for predicting counterfactual mean outcomes in the absence of a policy intervention. Examples include synthetic controls, diﬀerence-in-diﬀerences, factor and matrix completion models, and (fused) time series panel data models. The proposed procedures are valid under weak and easy-to-verify conditions and are provably robust against misspeciﬁcation. Our approach demonstrates an excellent small-sample performance in simulations and is taken to a data application where we re-evaluate the consequences of decriminalizing indoor prostitution.},
	language = {en},
	urldate = {2020-02-26},
	journal = {arXiv:1712.09089 [econ, stat]},
	author = {Chernozhukov, Victor and Wuthrich, Kaspar and Zhu, Yinchu},
	month = nov,
	year = {2019},
	note = {arXiv: 1712.09089},
	keywords = {Economics - Econometrics, Statistics - Methodology},
	file = {Chernozhukov et al. - 2019 - An Exact and Robust Conformal Inference Method for.pdf:/Users/abilinski/Zotero/storage/JVFLYJUA/Chernozhukov et al. - 2019 - An Exact and Robust Conformal Inference Method for.pdf:application/pdf}
}

@techreport{chernozhukov_inference_2019,
	title = {Inference on average treatment effects in aggregate panel data settings},
	url = {https://ideas.repec.org/p/ifs/cemmap/32-19.html},
	abstract = {This paper studies inference on treatment effects in aggregate panel data settings with a single treated unit and many control units. We propose new methods for making inference on average treatment effects in settings where both the number of pre-treatment and the number of post-treatment periods are large. We use linear models to approximate the counterfactual mean outcomes in the absence of the treatment. The counterfactuals are estimated using constrained Lasso, an essentially tuning free regression approach that nests difference-in-differences and synthetic control as special cases. We propose a K-fold cross-fitting procedure to remove the bias induced by regularization. To avoid the estimation of the long run variance, we construct a self-normalized t-statistic. The test statistic has an asymptotically pivotal distribution (a student t-distribution with K - 1 degrees of freedom), which makes our procedure very easy to implement. Our approach has several theoretical advantages. First, it does not rely on any sparsity assumptions. Second, it is fully robust against misspecification of the linear model. Third, it is more efficient than difference-in-means and difference-in-differences estimators. The proposed method demonstrates an excellent performance in simulation experiments, and is taken to a data application, where we re-evaluate the economic consequences of terrorism.},
	language = {en},
	number = {CWP32/19},
	urldate = {2020-02-26},
	institution = {Centre for Microdata Methods and Practice, Institute for Fiscal Studies},
	author = {Chernozhukov, Victor and Wüthrich, Kaspar and Zhu, Yinchu},
	month = jun,
	year = {2019},
	file = {Fullext PDF:/Users/abilinski/Zotero/storage/C46AKY5H/Chernozhukov et al. - 2019 - Inference on average treatment effects in aggregat.pdf:application/pdf;Snapshot:/Users/abilinski/Zotero/storage/3IHHY8I4/32-19.html:text/html}
}

@article{abadie_comparative_2015,
	title = {Comparative {Politics} and the {Synthetic} {Control} {Method}},
	volume = {59},
	issn = {1540-5907},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ajps.12116},
	doi = {10.1111/ajps.12116},
	language = {en},
	number = {2},
	urldate = {2019-05-28},
	journal = {American Journal of Political Science},
	author = {Abadie, Alberto and Diamond, Alexis and Hainmueller, Jens},
	month = feb,
	year = {2015},
	pages = {495--510},
	file = {Snapshot:/Users/abilinski/Zotero/storage/846FXEZC/ajps.html:text/html}
}


@article{abadie_economic_2003,
	title = {The {Economic} {Costs} of {Conflict}: {A} {Case} {Study} of the {Basque} {Country}},
	volume = {93},
	issn = {0002-8282},
	shorttitle = {The {Economic} {Costs} of {Conflict}},
	url = {https://www.aeaweb.org/articles?id=10.1257/000282803321455188},
	doi = {10.1257/000282803321455188},
	abstract = {This article investigates the economic effects of conflict, using the terrorist conflict in the Basque Country as a case study. We find that, after the outbreak of terrorism in the late 1960's, per capita GDP in the Basque Country declined about 10 percentage points relative to a synthetic control region without terrorism. In addition, we use the 1998-1999 truce as a natural experiment. We find that stocks of firms with a significant part of their business in the Basque Country showed a positive relative performance when truce became credible, and a negative relative performance at the end of the cease-fire.},
	language = {en},
	number = {1},
	urldate = {2019-05-28},
	journal = {American Economic Review},
	author = {Abadie, Alberto and Gardeazabal, Javier},
	month = mar,
	year = {2003},
	pages = {113--132},
	file = {Snapshot:/Users/abilinski/Zotero/storage/Y296I664/articles.html:text/html}
}

@article{ferman_revisiting_2016,
	type = {{MPRA} {Paper}},
	title = {Revisiting the {Synthetic} {Control} {Estimator}},
	url = {https://mpra.ub.uni-muenchen.de/75128/},
	abstract = {The synthetic control (SC) method has been recently proposed as an alternative to estimate treatment effects in comparative case studies. In this paper, we revisit the SC method in a linear factor model setting and consider the asymptotic properties of the SC estimator when the number of pre-treatment periods (T\_0) goes to infinity. Differently from  Abadie et al. (2010), we do not condition the analysis on a close-to-perfect pre-treatment fit, as the probability that this happens goes to zero when T\_0 is large. We show that, even when a close-to-perfect fit is not achieved, the SC method can substantially improve relative to the difference-in-differences (DID) estimator, both in terms of bias and variance. However, we show that, in our setting, the SC estimator is asymptotically biased if treatment assignment is correlated with the unobserved heterogeneity. If common factors are stationary, then the asymptotic bias of the SC estimator  goes to zero when the variance of the transitory shocks is small, which is also the case in which it is more likely that the pre-treatment fit will be good. If a subset of the common factors is non-stationary, then the SC estimator can be asymptotically biased even conditional on a close-to-perfect  fit. In this case, the identification assumption relies on orthogonality between treatment assignment and the stationary common factors. Finally, we also consider the statistical properties of the permutation tests suggested in Abadie et al. (2010).},
	language = {en},
	urldate = {2020-02-24},
	author = {Ferman, Bruno and Pinto, Cristine},
	month = nov,
	year = {2016},
	file = {Full Text PDF:/Users/abilinski/Zotero/storage/KVCKGLEY/Ferman and Pinto - 2016 - Revisiting the Synthetic Control Estimator.pdf:application/pdf;Snapshot:/Users/abilinski/Zotero/storage/XGN9JW6I/75128.html:text/html}
}

@article{amjad_mrsc_nodate,
	title = {{mRSC}: {Multi}-dimensional {Robust} {Synthetic} {Control}},
	volume = {3},
	language = {en},
	number = {2},
	year = 2018,
	author = {Amjad, Muhammad and Misra, Vishal and Shah, Devavrat and Shen, Dennis},
	pages = {28},
	file = {Amjad et al. - mRSC Multi-dimensional Robust Synthetic Control.pdf:/Users/abilinski/Zotero/storage/HHMAYJN3/Amjad et al. - mRSC Multi-dimensional Robust Synthetic Control.pdf:application/pdf}
}

@article{powell_imperfect_2018,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Imperfect {Synthetic} {Controls}: {Did} the {Massachusetts} {Health} {Care} {Reform} {Save} {Lives}?},
	shorttitle = {Imperfect {Synthetic} {Controls}},
	url = {https://papers.ssrn.com/abstract=3192710},
	abstract = {In 2006, Massachusetts enacted comprehensive health care reform which served as a model for the Affordable Care Act. I study the mortality effects of the reform using synthetic control estimation, relaxing two critical assumptions required to implement this method. The traditional approach assumes the existence of a perfect synthetic control, which cannot exist if the outcomes of the treated unit are outside of the "convex hull" or functions of transitory shocks. I propose simple modifications to relax these restrictions. The new estimator outperforms the traditional method in simulations. I estimate that the Massachusetts Health Care Reform reduced mortality by 3\%.},
	language = {en},
	number = {ID 3192710},
	urldate = {2019-05-28},
	institution = {Social Science Research Network},
	author = {Powell, David},
	month = may,
	year = {2018},
	keywords = {Affordable Care Act, Difference-in-Differences, Health Care Reform, Health Insurance, Mortality, Parallel Trends, Synthetic Control Estimation, Xed Effects},
	file = {Snapshot:/Users/abilinski/Zotero/storage/V8L4DL3X/papers.html:text/html}
}


@article{kaul_synthetic_2015,
	type = {{MPRA} {Paper}},
	title = {Synthetic {Control} {Methods}: {Never} {Use} {All} {Pre}-{Intervention} {Outcomes} {Together} {With} {Covariates}},
	shorttitle = {Synthetic {Control} {Methods}},
	url = {https://econpapers.repec.org/paper/pramprapa/83790.htm},
	abstract = {It is becoming increasingly popular in applications of synthetic control methods to include the entire pre-treatment path of the outcome variable as economic predictors. We demonstrate both theoretically and empirically that using all outcome lags as separate predictors renders all other covariates irrelevant. This finding holds irrespective of how important these covariates are for accurately predicting post-treatment values of the outcome, potentially threatening the estimator's unbiasedness. We show that estimation results and corresponding policy conclusions can change considerably when the usage of outcome lags as predictors is restricted, resulting in other covariates obtaining positive weights.},
	urldate = {2019-05-28},
	institution = {University Library of Munich, Germany},
	author = {Kaul, Ashok and Klößner, Stefan and Pfeifer, Gregor and Schieler, Manuel},
	month = mar,
	year = {2015},
	keywords = {Counterfactuals, Economic Predictors, Policy Evaluation., Synthetic Control Methods},
	file = {RePEc PDF:/Users/abilinski/Zotero/storage/8XS9QJQE/Kaul et al. - 2015 - Synthetic Control Methods Never Use All Pre-Inter.pdf:application/pdf;RePEc Snapshot:/Users/abilinski/Zotero/storage/EL794P6H/83790.html:text/html}
}

@article{doudchenko_balancing_2016-1,
	title = {Balancing, {Regression}, {Difference}-{In}-{Differences} and {Synthetic} {Control} {Methods}: {A} {Synthesis}},
	shorttitle = {Balancing, {Regression}, {Difference}-{In}-{Differences} and {Synthetic} {Control} {Methods}},
	url = {http://arxiv.org/abs/1610.07748},
	abstract = {In a seminal paper Abadie, Diamond, and Hainmueller [2010] (ADH), see also Abadie and Gardeazabal [2003], Abadie et al. [2014], develop the synthetic control procedure for estimating the eﬀect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that selected covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units (the synthetic control). The weights are restricted to be nonnegative and sum to one, which is important because it allows the procedure to obtain unique weights even when the number of lagged outcomes is modest relative to the number of control units, a common setting in applications. In the current paper we propose a generalization that allows the weights to be negative, and their sum to diﬀer from one, and that allows for a permanent additive diﬀerence between the treated unit and the controls, similar to diﬀerence-in-diﬀerence procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units.},
	language = {en},
	urldate = {2019-05-28},
	journal = {arXiv:1610.07748 [stat]},
	author = {Doudchenko, Nikolay and Imbens, Guido W.},
	month = oct,
	year = {2016},
	note = {arXiv: 1610.07748},
	keywords = {Statistics - Applications, Statistics - Machine Learning},
	file = {Doudchenko and Imbens - 2016 - Balancing, Regression, Difference-In-Differences a.pdf:/Users/abilinski/Zotero/storage/NPY8EGEP/Doudchenko and Imbens - 2016 - Balancing, Regression, Difference-In-Differences a.pdf:application/pdf}
}

@article{kinn_synthetic_2018,
	title = {Synthetic {Control} {Methods} and {Big} {Data}},
	url = {http://arxiv.org/abs/1803.00096},
	abstract = {Many macroeconomic policy questions may be assessed in a case study framework, where the time series of a treated unit is compared to a counterfactual constructed from a large pool of control units. I provide a general framework for this setting, tailored to predict the counterfactual by minimizing a tradeoff between underfitting (bias) and overfitting (variance). The framework nests recently proposed structural and reduced form machine learning approaches as special cases. Furthermore, difference-in-differences with matching and the original synthetic control are restrictive cases of the framework, in general not minimizing the bias-variance objective. Using simulation studies I find that machine learning methods outperform traditional methods when the number of potential controls is large or the treated unit is substantially different from the controls. Equipped with a toolbox of approaches, I revisit a study on the effect of economic liberalisation on economic growth. I find effects for several countries where no effect was found in the original study. Furthermore, I inspect how a systematically important bank respond to increasing capital requirements by using a large pool of banks to estimate the counterfactual. Finally, I assess the effect of a changing product price on product sales using a novel scanner dataset.},
	urldate = {2019-05-28},
	journal = {arXiv:1803.00096 [econ]},
	author = {Kinn, Daniel},
	month = feb,
	year = {2018},
	note = {arXiv: 1803.00096},
	keywords = {Economics - Econometrics},
	file = {arXiv\:1803.00096 PDF:/Users/abilinski/Zotero/storage/M2KL5GQU/Kinn - 2018 - Synthetic Control Methods and Big Data.pdf:application/pdf;arXiv.org Snapshot:/Users/abilinski/Zotero/storage/Y7PIM7A7/1803.html:text/html}
}

@article{ferman_synthetic_2019,
	title = {Synthetic {Controls} with {Imperfect} {Pre}-{Treatment} {Fit}},
	url = {http://arxiv.org/abs/1911.08521},
	abstract = {We analyze the properties of the Synthetic Control (SC) and related estimators when the pretreatment ﬁt is imperfect. In this framework, we show that these estimators are generally biased if treatment assignment is correlated with unobserved confounders, even when the number of pretreatment periods goes to inﬁnity. Still, we also show that a modiﬁed version of the SC method can substantially improve in terms of bias and variance relative to the diﬀerence-in-diﬀerence estimator. We also consider the properties of these estimators in settings with non-stationary common factors.},
	language = {en},
	urldate = {2020-02-24},
	journal = {arXiv:1911.08521 [econ]},
	author = {Ferman, Bruno and Pinto, Cristine},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.08521},
	keywords = {Economics - Econometrics},
	file = {Ferman and Pinto - 2019 - Synthetic Controls with Imperfect Pre-Treatment Fi.pdf:/Users/abilinski/Zotero/storage/LXNQ473U/Ferman and Pinto - 2019 - Synthetic Controls with Imperfect Pre-Treatment Fi.pdf:application/pdf}
}


@article{Ben-Michael2021,
abstract = {Staggered adoption of policies by different units at different times creates promising opportunities for observational causal inference. Estimation remains challenging, however, and common regression methods can give misleading results. A promising alternative is the synthetic control method (SCM), which finds a weighted average of control units that closely balances the treated unit's pre-treatment outcomes. In this paper, we generalize SCM, originally designed to study a single treated unit, to the staggered adoption setting. We first bound the error for the average effect and show that it depends on both the imbalance for each treated unit separately and the imbalance for the average of the treated units. We then propose "partially pooled" SCM weights to minimize a weighted combination of these measures; approaches that focus only on balancing one of the two components can lead to bias. We extend this approach to incorporate unit-level intercept shifts and auxiliary covariates. We assess the performance of the proposed method via extensive simulations and apply our results to the question of whether teacher collective bargaining leads to higher school spending, finding minimal impacts. We implement the proposed method in the augsynth R package.},
archivePrefix = {arXiv},
arxivId = {1912.03290},
author = {Ben-Michael, Eli and Feller, Avi and Rothstein, Jesse},
doi = {10.2139/ssrn.3861415},
eprint = {1912.03290},
file = {:C$\backslash$:/Users/psantanna/Downloads/1912.03290.pdf:pdf},
journal = {Journal of the Royal Statistical Society Series B},
title = {{Synthetic Controls with Staggered Adoption}},
volume = {Forthcoming},
year = {2021}
}


@article{abadie_penalized_nodate,
	title = {A {Penalized} {Synthetic} {Control} {Estimator} for {Disaggregated} {Data}},
	abstract = {Synthetic control methods are commonly applied in empirical research to estimate the eﬀects of treatments or interventions of interest on aggregate outcomes. A synthetic control estimator compares the outcome of a treated unit – that is, a unit exposed to the intervention of interest – to the outcome of a weighted average of untreated units that best resembles the characteristics of the treated unit before the intervention. When disaggregated data are available, constructing separate synthetic controls for each treated unit may help avoid interpolation biases. However, the problem of ﬁnding a synthetic control that best reproduces the characteristics of a treated unit may not have a unique solution. Multiplicity of solutions is a particularly daunting challenge in settings with disaggregated data, that is, when the sample includes many treated and untreated units. To address this challenge, we propose a synthetic control estimator that penalizes the pairwise discrepancies between the characteristics of the treated units and the characteristics of the units that contribute to their synthetic controls. The penalization parameter trades oﬀ pairwise matching discrepancies with respect to the characteristics of each unit in the synthetic control against matching discrepancies with respect to the characteristics of the synthetic control unit as a whole. We study the properties of this estimator and propose data driven choices of the penalization parameter.},
	language = {en},
	author = {Abadie, Alberto and L’Hour, Jeremy},
	pages = {35},
	year = {2019},
	file = {Abadie and L’Hour - A Penalized Synthetic Control Estimator for Disagg.pdf:/Users/abilinski/Zotero/storage/R8LMSSBD/Abadie and L’Hour - A Penalized Synthetic Control Estimator for Disagg.pdf:application/pdf}
}


@article{xu_generalized_2017,
	title = {Generalized {Synthetic} {Control} {Method}: {Causal} {Inference} with {Interactive} {Fixed} {Effects} {Models}},
	volume = {25},
	issn = {1047-1987, 1476-4989},
	shorttitle = {Generalized {Synthetic} {Control} {Method}},
	url = {https://www.cambridge.org/core/journals/political-analysis/article/generalized-synthetic-control-method-causal-inference-with-interactive-fixed-effects-models/B63A8BD7C239DD4141C67DA10CD0E4F3},
	doi = {10.1017/pan.2016.2},
	abstract = {Difference-in-differences (DID) is commonly used for causal inference in time-series cross-sectional data. It requires the assumption that the average outcomes of treated and control units would have followed parallel paths in the absence of treatment. In this paper, we propose a method that not only relaxes this often-violated assumption, but also unifies the synthetic control method (Abadie, Diamond, and Hainmueller 2010) with linear fixed effects models under a simple framework, of which DID is a special case. It imputes counterfactuals for each treated unit using control group information based on a linear interactive fixed effects model that incorporates unit-specific intercepts interacted with time-varying coefficients. This method has several advantages. First, it allows the treatment to be correlated with unobserved unit and time heterogeneities under reasonable modeling assumptions. Second, it generalizes the synthetic control method to the case of multiple treated units and variable treatment periods, and improves efficiency and interpretability. Third, with a built-in cross-validation procedure, it avoids specification searches and thus is easy to implement. An empirical example of Election Day Registration and voter turnout in the United States is provided.},
	language = {en},
	number = {1},
	urldate = {2019-05-28},
	journal = {Political Analysis},
	author = {Xu, Yiqing},
	month = jan,
	year = {2017},
	pages = {57--76},
	file = {Full Text PDF:/Users/abilinski/Zotero/storage/2NMJT2ND/Xu - 2017 - Generalized Synthetic Control Method Causal Infer.pdf:application/pdf;Snapshot:/Users/abilinski/Zotero/storage/HB2TWKAM/B63A8BD7C239DD4141C67DA10CD0E4F3.html:text/html}
}


@article{bai_panel_2009,
	title = {Panel {Data} {Models} with {Interactive} {Fixed} {Effects}},
	volume = {77},
	issn = {0012-9682},
	url = {http://www.jstor.org/stable/40263859},
	abstract = {[This paper considers large N and large T panel data models with unobservable multiple interactive effects, which are correlated with the regressors. In earnings studies, for example, workers' motivation, persistence, and diligence combined to influence the earnings in addition to the usual argument of innate ability. In macroeconomics, interactive effects represent unobservable common shocks and their heterogeneous impacts on cross sections. We consider identification, consistency, and the limiting distribution of the interactive-effects estimator. Under both large N and large T, the estimator is shown to be \${\textbackslash}sqrt \{NT\} \$ consistent, which is valid in the presence of correlations and heteroskedasticities of unknown form in both dimensions. We also derive the constrained estimator and its limiting distribution, imposing additivity coupled with interactive effects. The problem of testing additive versus interactive effects is also studied. In addition, we consider identification and estimation of models in the presence of a grand mean, time-invariant regressors, and common regressors. Given identification, the rate of convergence and limiting results continue to hold.]},
	number = {4},
	urldate = {2019-05-28},
	journal = {Econometrica},
	author = {Bai, Jushan},
	year = {2009},
	pages = {1229--1279}
}

@article{gobillon_regional_2015,
	title = {Regional {Policy} {Evaluation}: {Interactive} {Fixed} {Effects} and {Synthetic} {Controls}},
	volume = {98},
	issn = {0034-6535},
	shorttitle = {Regional {Policy} {Evaluation}},
	url = {https://doi.org/10.1162/REST_a_00537},
	doi = {10.1162/REST_a_00537},
	abstract = {Abstract In this paper, we investigate the use of interactive effect or linear factor models in regional policy evaluation. We contrast treatment effect estimates obtained using Bai (2009) with those obtained using difference in differences and synthetic controls (Abadie and coauthors). We show that difference in differences are generically biased, and we derive support conditions for synthetic controls. We construct Monte Carlo experiments to compare these estimation methods in small samples. As an empirical illustration, we provide an evaluation of the impact on local unemployment of an enterprise zone policy implemented in France in the 1990s.},
	number = {3},
	urldate = {2019-05-28},
	journal = {The Review of Economics and Statistics},
	author = {Gobillon, Laurent and Magnac, Thierry},
	month = aug,
	year = {2015},
	pages = {535--551},
	file = {Snapshot:/Users/abilinski/Zotero/storage/YYI367TJ/REST_a_00537.html:text/html;Submitted Version:/Users/abilinski/Zotero/storage/FKN7AJ89/Gobillon and Magnac - 2015 - Regional Policy Evaluation Interactive Fixed Effe.pdf:application/pdf}
}



@article{bilinski_seeking_2018,
	title = {Seeking evidence of absence: {Reconsidering} tests of model assumptions},
	shorttitle = {Seeking evidence of absence},
	url = {http://arxiv.org/abs/1805.03273},
	abstract = {Statistical tests can only reject the null hypothesis, never prove it. However, when researchers test modeling assumptions, they often interpret the failure to reject a null of "no violation" as evidence that the assumption holds. We discuss the statistical and conceptual problems with this approach. We show that equivalence/non-inferiority tests, while giving correct Type I error, have low power to rule out many violations that are practically significant. We suggest sensitivity analyses that may be more appropriate than hypothesis testing.},
	journal = {arXiv:1805.03273 [stat]},
	author = {Bilinski, Alyssa and Hatfield, Laura A.},
	month = may,
	year = {2018},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1805.03273 PDF:/Users/jonathanroth/Zotero/storage/3W9XAKW5/Bilinski and Hatfield - 2018 - Seeking evidence of absence Reconsidering tests o.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/H58B5IIC/1805.html:text/html}
}

@article{chabe-ferret_analysis_2015,
	title = {Analysis of the bias of {Matching} and {Difference}-in-{Difference} under alternative earnings and selection processes},
	volume = {185},
	issn = {0304-4076},
	url = {http://www.sciencedirect.com/science/article/pii/S0304407614002437},
	doi = {10.1016/j.jeconom.2014.09.013},
	abstract = {Matching and Difference in Difference (DID) are two widespread methods that use pre-treatment outcomes to correct for selection bias. I detail the sources of bias of both estimators in a model of earnings dynamics and entry into a Job Training Program (JTP) and I assess their performances using Monte Carlo simulations of the model calibrated with realistic parameter values. I find that Matching generally underestimates the average causal effect of the program and gets closer to the true effect when conditioning on an increasing number of pre-treatment outcomes. When selection bias is symmetric around the treatment date, DID is consistent when implemented symmetrically—i.e. comparing outcomes observed the same number of periods before and after the treatment date. When selection bias is not symmetric, Monte Carlo simulations show that Symmetric DID still performs better than Matching, especially in the middle of the life-cycle. These results are consistent with estimates of the bias of Matching and DID from randomly assigned JTPs. Some of the virtues of Symmetric DID extend to programs other than JTPs allocated according to a cutoff eligibility rule.},
	number = {1},
	journal = {Journal of Econometrics},
	author = {Chabé-Ferret, Sylvain},
	year = {2015},
	keywords = {Difference in Difference, Job Training Programs, Matching},
	pages = {110--123},
	file = {ScienceDirect Snapshot:/Users/jonathanroth/Zotero/storage/WTX66IZK/S0304407614002437.html:text/html}
}

@article{daw_matching_2018,
	title = {Matching and {Regression} to the {Mean} in {Difference}‐in‐{Differences} {Analysis}},
	issn = {1475-6773},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1475-6773.12993},
	doi = {10.1111/1475-6773.12993},
	language = {en},
	urldate = {2018-09-21},
	journal = {Health Services Research},
	author = {Daw, Jamie R. and Hatfield, Laura A.},
	year = {2018},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/7KCTAUIS/1475-6773.html:text/html}
}

@article{athey_design-based_2018,
	title = {Design-based {Analysis} in {Difference}-{In}-{Differences} {Settings} with {Staggered} {Adoption}},
	journal = {Journal of Econometrics},
	author = {Athey, Susan and Imbens, Guido},
	volume = {226},
	number = {1},
	pages = {62--79},
	year = {2022}
}


@article{goodman-bacon_difference--differences_2018,
	title = {Difference-in-differences with variation in treatment timing},
	journal = {Journal of Econometrics},
	number = {2},
	volume = {225},
	pages = {254--277},
	author = {Goodman-Bacon, Andrew},
	year = {2021}
}


@techreport{snyder_sniff_2018,
	type = {Working {Paper}},
	title = {Sniff {Tests} in {Economics}: {Aggregate} {Distribution} of {Their} {Probability} {Values} and {Implications} for {Publication} {Bias}},
	shorttitle = {Sniff {Tests} in {Economics}},
	url = {http://www.nber.org/papers/w25058},
	abstract = {The increasing demand for rigor in empirical economics has led to the growing use of auxiliary tests (balance, specification, over-identification, placebo, etc.) supporting the credibility of a paper's main results. We dub these "sniff tests" because standards for passing are subjective and rejection is bad news for the author. Sniff tests offer a new window into publication bias since authors prefer them to be insignificant, the reverse of standard statistical tests. Collecting a sample of nearly 30,000 sniff tests across 60 economics journals, we provide the first estimate of their aggregate probability-value (p-value) distribution. For the subsample of balance tests in randomized controlled trials (for which the distribution of p-values is known to be uniform absent publication bias, allowing reduced-form methods to be employed) estimates suggest that 45\% of failed tests remain in the "file drawer" rather than being published. For the remaining sample with an unknown distribution of p-values, structural estimates suggest an even larger file-drawer problem, as high as 91\%. Fewer significant sniff tests show up in top-tier journals, smaller tables, and more recent articles. We find no evidence of author manipulation other than a tendency to overly attribute significant sniff tests to bad luck.},
	number = {25058},
	institution = {National Bureau of Economic Research},
	author = {Snyder, Christopher and Zhuo, Ran},
	month = sep,
	year = {2018},
	doi = {10.3386/w25058},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/XFQWJ7DH/Snyder and Zhuo - 2018 - Sniff Tests in Economics Aggregate Distribution o.pdf:application/pdf}
}


@techreport{andrews_inference_2018,
	title = {Inference on winners},
	url = {https://ideas.repec.org/p/ifs/cemmap/31-18.html},
	abstract = {Many questions in econometrics can be cast as inference on a parameter selected through optimization. For example, researchers may be interested in the effectiveness of the best policy found in a randomized trial, or the best-performing investment strategy based on historical data. Such settings give rise to a winner's curse, where conventional estimates are biased and conventional confi dence intervals are unreliable. This paper develops optimal con fidence sets and median-unbiased estimators that are valid conditional on the parameter selected and so overcome this winner's curse. If one requires validity only on average over target parameters that might have been selected, we develop hybrid procedures that combine conditional and projection con fidence sets and offer further performance gains that are attractive relative to existing alternatives.},
	language = {en},
	number = {CWP31/18},
	urldate = {2018-08-23},
	institution = {Centre for Microdata Methods and Practice, Institute for Fiscal Studies},
	author = {Andrews, Isaiah and Kitagawa, Toru and McCloskey, Adam},
	month = may,
	year = {2018},
	keywords = {Selective Inference, Winner's Curse},
	file = {Fullext PDF:/Users/jonathanroth/Zotero/storage/QTS9CTX6/Andrews et al. - 2018 - Inference on winners.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/RSM73JBX/31-18.html:text/html}
}

@techreport{cunningham_decriminalizing_2014,
	type = {{NBER} {Working} {Paper}},
	title = {Decriminalizing {Indoor} {Prostitution}: {Implications} for {Sexual} {Violence} and {Public} {Health}},
	shorttitle = {Decriminalizing {Indoor} {Prostitution}},
	url = {https://econpapers.repec.org/paper/nbrnberwo/20281.htm},
	abstract = {Most governments in the world including the United States prohibit prostitution. Given these types of laws rarely change and are fairly uniform across regions, our knowledge about the impact of decriminalizing sex work is largely conjectural. We exploit the fact that a Rhode Island District Court judge unexpectedly decriminalized indoor prostitution in 2003 to provide the first causal estimates of the impact of decriminalization on the composition of the sex market, rape offenses, and sexually transmitted infection outcomes. Not surprisingly, we find that decriminalization increased the size of the indoor market. However, we also find that decriminalization caused both forcible rape offenses and gonorrhea incidence to decline for the overall population. Our synthetic control model finds 824 fewer reported rape offenses (31 percent decrease) and 1,035 fewer cases of female gonorrhea (39 percent decrease) from 2004 to 2009.},
	number = {20281},
	institution = {National Bureau of Economic Research, Inc},
	author = {Cunningham, Scott and Shah, Manisha},
	month = jul,
	year = {2014},
	file = {RePEc PDF:/Users/jonathanroth/Zotero/storage/FPMMUJKU/Cunningham and Shah - 2014 - Decriminalizing Indoor Prostitution Implications .pdf:application/pdf;RePEc Snapshot:/Users/jonathanroth/Zotero/storage/M2M39F9F/20281.html:text/html}
}

@misc{noauthor_decriminalizing_nodate,
	title = {Decriminalizing {Indoor} {Prostitution}: {Implications} for {Sexual} {Violence} and {Public} {Health} {\textbar} {The} {Review} of {Economic} {Studies} {\textbar} {Oxford} {Academic}},
	url = {https://academic.oup.com/restud/article-abstract/85/3/1683/4756165},
	urldate = {2018-06-25}
}

@article{bosch_trade-offs_2014,
	title = {The {Trade}-{Offs} of {Welfare} {Policies} in {Labor} {Markets} with {Informal} {Jobs}: {The} {Case} of the "{Seguro} {Popular}" {Program} in {Mexico}},
	volume = {6},
	issn = {1945-7731},
	shorttitle = {The {Trade}-{Offs} of {Welfare} {Policies} in {Labor} {Markets} with {Informal} {Jobs}},
	url = {https://www.aeaweb.org/articles?id=10.1257/pol.6.4.71},
	doi = {10.1257/pol.6.4.71},
	abstract = {In 2002, the Mexican government began an effort to improve health
access to the 50 million uninsured in Mexico, a program known as
Seguro Popular (SP). The SP offered virtually free health insurance
to informal workers, altering the incentives to operate in the formal
economy. We find that the SP program had a negative effect on the
number of employers and employees formally registered in small and
medium firms (up to 50 employees). Our results suggest that the positive gains of expanding health coverage should be weighed against the implications of the reallocation of labor away from the formal sector.},
	language = {en},
	number = {4},
	urldate = {2018-06-25},
	journal = {American Economic Journal: Economic Policy},
	author = {Bosch, Mariano and Campos-Vazquez, Raymundo M.},
	month = nov,
	year = {2014},
	pages = {71--99},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/S45R4NC5/articles.html:text/html}
}

@article{dube_minimum_2010,
	title = {Minimum wage effects across state borders: {Estimates} using contiguous counties},
	volume = {92},
	number = {4},
	journal = {The Review of Economics and Statistics},
	author = {Dube, Arindrajit and Lester, T William and Reich, Michael},
	year = {2010},
	note = {bibtex: dube2010minimum},
	pages = {945--964}
}

@article{open_science_collaboration_estimating_2015,
	title = {Estimating the reproducibility of psychological science},
	volume = {349},
	number = {6251},
	journal = {Science},
	author = {{Open Science Collaboration}},
	year = {2015},
	note = {bibtex: open2015estimating},
	pages = {aac4716}
}

@article{meenakshi_product_1999,
	title = {On a product of positive semidefinite matrices},
	volume = {295},
	issn = {0024-3795},
	url = {http://www.sciencedirect.com/science/article/pii/S0024379599000142},
	doi = {10.1016/S0024-3795(99)00014-2},
	abstract = {Necessary and sufficient conditions are given for the product of two positive semidefinite (psd) matrices to be EP. As a consequence, it is shown that the product of two psd matrices is psd if and only if the product is normal.},
	number = {1},
	journal = {Linear Algebra and its Applications},
	author = {Meenakshi, A. R. and Rajian, C.},
	month = jul,
	year = {1999},
	pages = {3--6},
	file = {ScienceDirect Full Text PDF:/Users/jonathanroth/Zotero/storage/FFKR55IN/Meenakshi and Rajian - 1999 - On a product of positive semidefinite matrices.pdf:application/pdf;ScienceDirect Snapshot:/Users/jonathanroth/Zotero/storage/X29SVMGT/S0024379599000142.html:text/html}
}

@article{athey_identification_2006,
	title = {Identification and {Inference} in {Nonlinear} {Difference}-in-{Differences} {Models}},
	volume = {74},
	issn = {0012-9682},
	url = {http://www.jstor.org/stable/3598807},
	abstract = {This paper develops a generalization of the widely used difference-in-differences method for evaluating the effects of policy changes. We propose a model that allows the control and treatment groups to have different average benefits from the treatment. The assumptions of the proposed model are invariant to the scaling of the outcome. We provide conditions under which the model is nonparametrically identified and propose an estimator that can be applied using either repeated cross section or panel data. Our approach provides an estimate of the entire counterfactual distribution of outcomes that would have been experienced by the treatment group in the absence of the treatment and likewise for the untreated group in the presence of the treatment. Thus, it enables the evaluation of policy interventions according to criteria such as a mean-variance trade-off. We also propose methods for inference, showing that our estimator for the average treatment effect is root-N consistent and asymptotically normal. We consider extensions to allow for covariates, discrete dependent variables, and multiple groups and time periods.},
	number = {2},
	journal = {Econometrica},
	author = {Athey, Susan and Imbens, Guido W.},
	year = {2006},
	pages = {431--497}
}

@article{abadie_semiparametric_2005,
	title = {Semiparametric {Difference}-in-{Differences} {Estimators}},
	volume = {72},
	issn = {0034-6527},
	url = {http://www.jstor.org/stable/3700681},
	abstract = {[The difference-in-differences (DID) estimator is one of the most popular tools for applied research in economics to evaluate the effects of public interventions and other treatments of interest on some relevant outcome variables. However, it is well known that the DID estimator is based on strong identifying assumptions. In particular, the conventional DID estimator requires that, in the absence of the treatment, the average outcomes for the treated and control groups would have followed parallel paths over time. This assumption may be implausible if pre-treatment characteristics that are thought to be associated with the dynamics of the outcome variable are unbalanced between the treated and the untreated. That would be the case, for example, if selection for treatment is influenced by individual-transitory shocks on past outcomes (Ashenfelter's dip). This article considers the case in which differences in observed characteristics create non-parallel outcome dynamics between treated and controls. It is shown that, in such a case, a simple two-step strategy can be used to estimate the average effect of the treatment for the treated. In addition, the estimation framework proposed in this article allows the use of covariates to describe how the average effect of the treatment varies with changes in observed characteristics.]},
	number = {1},
	journal = {The Review of Economic Studies},
	author = {Abadie, Alberto},
	year = {2005},
	pages = {1--19}
}

@article{moulton_illustration_1990,
	title = {An {Illustration} of a {Pitfall} in {Estimating} the {Effects} of {Aggregate} {Variables} on {Micro} {Unit}},
	volume = {72},
	url = {https://ideas.repec.org/a/tpr/restat/v72y1990i2p334-38.html},
	abstract = {Many economic researchers have attempted to measure the effect of aggregate market or public policy variables on micro units by merging aggregate data with micro observations by industry, occupation, or geographical location, then using multiple regression or similar statistical models to measure the effect of the aggregate variable on the micro units. The methods are usually based upon the assumption of independent disturbances, which is typically not appropriate for data from populations with grouped structure. Incorrectly using ordinary least squares can lead to standard errors that are seriously biased downward. This note illustrates the danger of spurious regression from this kind of misspecification, using as an example a wage regression estimated on data for individual workers that includes in the specification aggregate regressors for characteristics of geographical states. Copyright 1990 by MIT Press.},
	language = {en},
	number = {2},
	urldate = {2018-03-18},
	journal = {The Review of Economics and Statistics},
	author = {Moulton, Brent R.},
	year = {1990},
	pages = {334--338},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/WR2D39C7/v72y1990i2p334-38.html:text/html}
}

@article{bertrand_how_2004,
	title = {How {Much} {Should} {We} {Trust} {Differences}-{In}-{Differences} {Estimates}?},
	volume = {119},
	issn = {0033-5533},
	url = {https://academic-oup-com.ezp-prod1.hul.harvard.edu/qje/article/119/1/249/1876068},
	doi = {10.1162/003355304772839588},
	abstract = {Most papers that employ Differences-in-Differences estimation (DD) use many years of data and focus on serially correlated outcomes but ignore that the resulting standard errors are inconsistent. To illustrate the severity of this issue, we randomly generate placebo laws in state-level data on female wages from the Current Population Survey. For each law, we use OLS to compute the DD estimate of its “effect” as well as the standard error of this estimate. These conventional DD standard errors severely understate the standard deviation of the estimators: we find an “effect” significant at the 5 percent level for up to 45 percent of the placebo interventions. We use Monte Carlo simulations to investigate how well existing methods help solve this problem. Econometric corrections that place a specific parametric form on the time-series process do not perform well. Bootstrap (taking into account the autocorrelation of the data) works well when the number of states is large enough. Two corrections based on asymptotic approximation of the variance-covariance matrix work well for moderate numbers of states and one correction that collapses the time series information into a “pre”- and “post”-period and explicitly takes into account the effective sample size works well even for small numbers of states.},
	language = {en},
	number = {1},
	urldate = {2018-03-18},
	journal = {The Quarterly Journal of Economics},
	author = {Bertrand, Marianne and Duflo, Esther and Mullainathan, Sendhil},
	year = {2004},
	pages = {249--275},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/76R5MNNW/Bertrand et al. - 2004 - How Much Should We Trust Differences-In-Difference.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/XJ7838HD/1876068.html:text/html}
}

@article{donald_inference_2007,
	title = {Inference with {Difference}-in-{Differences} and {Other} {Panel} {Data}},
	volume = {89},
	url = {https://ideas.repec.org/a/tpr/restat/v89y2007i2p221-233.html},
	abstract = {We examine inference in panel data when the number of groups is small, as is typically the case for difference-in-differences estimation and when some variables are fixed within groups. In this case, standard asymptotics based on the number of groups going to infinity provide a poor approximation to the finite sample distribution. We show that in some cases the t-statistic is distributed as t and propose simple two-step estimators for these cases. We apply our analysis to two well-known papers. We confirm our theoretical analysis with Monte Carlo simulations. Copyright by the President and Fellows of Harvard College and the Massachusetts Institute of Technology.},
	language = {en},
	number = {2},
	urldate = {2018-03-18},
	journal = {The Review of Economics and Statistics},
	author = {Donald, Stephen G. and Lang, Kevin},
	year = {2007},
	pages = {221--233},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/UHD3FX87/v89y2007i2p221-233.html:text/html}
}

@article{leeb_model_2005,
	title = {Model {Selection} and {Inference}: {Facts} and {Fiction}},
	volume = {21},
	issn = {0266-4666},
	shorttitle = {Model {Selection} and {Inference}},
	url = {http://www.jstor.org/stable/3533623},
	abstract = {Model selection has an important impact on subsequent inference. Ignoring the model selection step leads to invalid inference. We discuss some intricate aspects of data-driven model selection that do not seem to have been widely appreciated in the literature. We debunk some myths about model selection, in particular the myth that consistent model selection has no effect on subsequent inference asymptotically. We also discuss an "impossibility" result regarding the estimation of the finite-sample distribution of post-model-selection estimators.},
	number = {1},
	journal = {Econometric Theory},
	author = {Leeb, Hannes and Pötscher, Benedikt M.},
	year = {2005},
	pages = {21--59}
}

@book{pfanzagl_parametric_1994,
	title = {Parametric {Statistical} {Theory}},
	isbn = {978-3-11-013863-4},
	language = {en},
	publisher = {W. de Gruyter},
	author = {Pfanzagl, Johann},
	year = {1994},
	note = {Google-Books-ID: 1S20QgAACAAJ},
	keywords = {Mathematics / General}
}

@article{camerer_evaluating_2016,
	title = {Evaluating replicability of laboratory experiments in economics},
	issn = {0036-8075},
	url = {http://science.sciencemag.org/content/early/2016/03/02/science.aaf0918},
	doi = {10.1126/science.aaf0918},
	abstract = {The reproducibility of scientific findings has been called into question. To contribute data about reproducibility in economics, we replicate 18 studies published in the American Economic Review and the Quarterly Journal of Economics in 2011-2014. All replications follow predefined analysis plans publicly posted prior to the replications, and have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We find a significant effect in the same direction as the original study for 11 replications (61\%); on average the replicated effect size is 66\% of the original. The reproducibility rate varies between 67\% and 78\% for four additional reproducibility indicators, including a prediction market measure of peer beliefs.},
	journal = {Science},
	author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, Jürgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
	year = {2016},
	note = {bibtex: Camereraaf0918}
}

@techreport{borusyak_revisiting_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Revisiting {Event} {Study} {Designs}},
	url = {https://papers.ssrn.com/abstract=2826228},
	abstract = {A broad empirical literature uses "event study" research designs for treatment effect estimation, a setting in which all units in the panel receive treatment but at random times. We make four novel points about identification and estimation of causal effects in this setting and show their practical relevance. First, we show that in the presence of unit and time fixed effects, it is impossible to identify the linear component of the path of pre-trends and dynamic treatment effects. Second, we propose graphical and statistical tests for pre-trends. Third, we consider commonly-used "static" regressions, with a treatment dummy instead of a full set of leads and lags around the treatment event, and we show that OLS does not recover a weighted average of the treatment effects: long-term effects are weighted negatively, and we introduce a different estimator that is robust to this issue. Fourth, we show that equivalent problems of under-identification and negative weighting arise in difference-in-differences settings when the control group is allowed to be on a different time trend or in the presence of unit-specific time trends. Finally, we show the practical relevance of these issues in a series of examples from the existing literature, with a focus on the estimation of the marginal propensity to consume out of tax rebates.},
	language = {en},
	number = {ID 2826228},
	urldate = {2018-03-15},
	institution = {Social Science Research Network},
	author = {Borusyak, Kirill and Jaravel, Xavier},
	year = {2018},
	keywords = {Kirill Borusyak, Revisiting Event Study Designs, SSRN, Xavier Jaravel},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/W98KA5CT/papers.html:text/html}
}


@article{Huber2021,
author = {Huber, Martin and Steinmayr, Andreas},
doi = {10.1080/07350015.2019.1668795},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/others/huber2019.pdf:pdf},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {general equilibrium effects,interference,spillover effects,treatment effects},
number = {2},
pages = {422--436},
title = {{A Framework for Separating Individual-Level Treatment Effects From Spillover Effects}},
volume = {39},
year = {2021}
}
@article{giles_pre-test_1993,
	title = {Pre-{Test} {Estimation} and {Testing} in {Econometrics}: {Recent} {Developments}},
	volume = {7},
	issn = {1467-6419},
	shorttitle = {Pre-{Test} {Estimation} and {Testing} in {Econometrics}},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-6419.1993.tb00163.x/abstract},
	doi = {10.1111/j.1467-6419.1993.tb00163.x},
	abstract = {Abstract.  This paper surveys a range of important developments in the area of preliminary-test inference in the context of econometric modelling. Both pre-test estimation and pre-test testing are discussed. Special attention is given to recent contributions and results. These include analyses of pre-test strategies under model mis-specification and generalised regression errors; exact sampling distribution results; and pre-testing inequality constraints on the model's parameters. In many cases, practical advice is given to assist applied econometricians in appraising the relative merits of pre-testing. It is shown that there are situations where pre-testing can be advantageous in practice},
	language = {en},
	number = {2},
	journal = {Journal of Economic Surveys},
	author = {Giles, Judith A. and Giles, David E. A.},
	month = jun,
	year = {1993},
	keywords = {conditional inference, Preliminary testing, sequential inference, specification analysis},
	pages = {145--197},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/56MEBHZA/abstract.html:text/html}
}

@techreport{christensen_transparency_2016,
	type = {Working {Paper}},
	title = {Transparency, {Reproducibility}, and the {Credibility} of {Economics} {Research}},
	url = {http://www.nber.org/papers/w22989},
	abstract = {There is growing interest in enhancing research transparency and reproducibility in economics and other scientific fields. We survey existing work on these topics within economics, and discuss the evidence suggesting that publication bias, inability to replicate, and specification searching remain widespread in the discipline. We next discuss recent progress in this area, including through improved research design, study registration and pre-analysis plans, disclosure standards, and open sharing of data and materials, drawing on experiences in both economics and other social sciences. We discuss areas where consensus is emerging on new practices, as well as approaches that remain controversial, and speculate about the most effective ways to make economics research more credible in the future.},
	number = {22989},
	institution = {National Bureau of Economic Research},
	author = {Christensen, Garret S. and Miguel, Edward},
	month = dec,
	year = {2016},
	doi = {10.3386/w22989},
	file = {NBER Full Text PDF:/Users/jonathanroth/Zotero/storage/7WW9KN22/Christensen and Miguel - 2016 - Transparency, Reproducibility, and the Credibility.pdf:application/pdf}
}

@incollection{rothstein_publication_2005,
	title = {Publication {Bias} in {Meta}-{Analysis}},
	copyright = {Copyright © 2005 John Wiley \& Sons, Ltd},
	isbn = {978-0-470-87016-7},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/0470870168.ch1/summary},
	abstract = {This chapter contains sections titled:

* Publication Bias As a Threat to Validity
* Organization of the Book
* Our Modest Proposal
* References},
	language = {en},
	booktitle = {Publication {Bias} in {Meta}-{Analysis}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Rothstein, Hannah R. and Sutton, Alexander J. and Borenstein, Michael},
	editor = {Co-Chair, Hannah R. Rothstein and Co-Author, Alexander J. Sutton and PI, Michael Borenstein Director Associateessor Lecturer},
	year = {2005},
	doi = {10.1002/0470870168.ch1},
	keywords = {cost and familiarity bias, language and availability bias, meta-analytic review, potential information suppression mechanisms, publication bias, publication bias and dissemination biases, publication bias in meta-analysis, summarizing research findings, systematic review and meta-analysis},
	pages = {1--7},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/FSCNS72Z/summary.html:text/html}
}

@article{nyhan_increasing_2015,
	title = {Increasing the credibility of political science research: {A} proposal for journal reforms},
	volume = {48},
	number = {S1},
	journal = {PS: Political Science \& Politics},
	author = {Nyhan, Brendan},
	year = {2015},
	note = {bibtex: nyhan2015increasing},
	pages = {78--83}
}

@article{brodeur_star_2016,
	title = {Star {Wars}: {The} {Empirics} {Strike} {Back}},
	volume = {8},
	issn = {1945-7782},
	shorttitle = {Star {Wars}},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20150044},
	doi = {10.1257/app.20150044},
	abstract = {Using 50,000 tests published in the AER, JPE, and QJE, we identify a residual in the distribution of tests that cannot be explained solely by journals favoring rejection of the null hypothesis. We observe a two-humped camel shape with missing p-values between 0.25 and 0.10 that can be retrieved just after the 0.05 threshold and represent 10-20 percent of marginally rejected tests. Our interpretation is that researchers inflate the value of just-rejected tests by choosing "significant" specifications. We propose a method to measure this residual and describe how it varies by article and author characteristics. (JEL A11, C13)},
	language = {en},
	number = {1},
	urldate = {2018-03-14},
	journal = {American Economic Journal: Applied Economics},
	author = {Brodeur, Abel and Lé, Mathias and Sangnier, Marc and Zylberberg, Yanos},
	month = jan,
	year = {2016},
	pages = {1--32},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/XGH9IG29/articles.html:text/html}
}

@techreport{andrews_identification_2017,
	type = {Working {Paper}},
	title = {Identification of and {Correction} for {Publication} {Bias}},
	url = {http://www.nber.org/papers/w23298},
	abstract = {Some empirical results are more likely to be published than others. Such selective publication leads to biased estimates and distorted inference. This paper proposes two approaches for identifying the conditional probability of publication as a function of a study’s results, the first based on systematic replication studies and the second based on meta-studies. For known conditional publication probabilities, we propose median-unbiased estimators and associated confidence sets that correct for selective publication. We apply our methods to recent large-scale replication studies in experimental economics and psychology, and to meta-studies of the effects of minimum wages and de-worming programs.},
	number = {23298},
	institution = {National Bureau of Economic Research},
	author = {Andrews, Isaiah and Kasy, Maximilian},
	month = mar,
	year = {2017},
	doi = {10.3386/w23298},
	file = {NBER Full Text PDF:/Users/jonathanroth/Zotero/storage/6H7CX68F/Andrews and Kasy - 2017 - Identification of and Correction for Publication B.pdf:application/pdf}
}

@book{boyd_convex_2004,
	title = {Convex optimization},
	publisher = {Cambridge university press},
	author = {Boyd, Stephen and Vandenberghe, Lieven},
	year = {2004},
	note = {bibtex: boyd2004convex}
}

@article{saumard_log-concavity_2014,
	title = {Log-concavity and strong log-concavity: a review},
	shorttitle = {Log-concavity and strong log-concavity},
	url = {http://arxiv.org/abs/1404.5886},
	abstract = {We review and formulate results concerning log-concavity and strong-log-concavity in both discrete and continuous settings. We show how preservation of log-concavity and strongly log-concavity on \${\textbackslash}mathbb\{R\}\$ under convolution follows from a fundamental monotonicity result of Efron (1969). We provide a new proof of Efron's theorem using the recent asymmetric Brascamp-Lieb inequality due to Otto and Menz (2013). Along the way we review connections between log-concavity and other areas of mathematics and statistics, including concentration of measure, log-Sobolev inequalities, convex geometry, MCMC algorithms, Laplace approximations, and machine learning.},
	journal = {arXiv:1404.5886 [math, stat]},
	author = {Saumard, Adrien and Wellner, Jon A.},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.5886},
	keywords = {Mathematics - Statistics Theory, 60E15, 62E10, 62H05},
	annote = {Comment: 67 pages, 1 figure},
	file = {arXiv\:1404.5886 PDF:/Users/jonathanroth/Zotero/storage/DUZSVK6Q/Saumard and Wellner - 2014 - Log-concavity and strong log-concavity a review.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/7RNJRS6E/1404.html:text/html}
}

@article{manjunath_moments_2012,
	title = {Moments {Calculation} {For} the {Doubly} {Truncated} {Multivariate} {Normal} {Density}},
	url = {http://arxiv.org/abs/1206.5387},
	abstract = {In the present article we derive an explicit expression for the trun- cated mean and variance for the multivariate normal distribution with ar- bitrary rectangular double truncation. We use the moment generating ap- proach of Tallis (1961) and extend it to general \{{\textbackslash}mu\}, \{{\textbackslash}Sigma\} and all combinations of truncation. As part of the solution we also give a formula for the bivari- ate marginal density of truncated multinormal variates. We also prove an invariance property of some elements of the inverse covariance after trunca- tion. Computer algorithms for computing the truncated mean, variance and the bivariate marginal probabilities for doubly truncated multivariate normal variates have been written in R and are presented along with three examples.},
	journal = {arXiv:1206.5387 [stat]},
	author = {Manjunath, B.G and Wilhelm, Stefan},
	month = jun,
	year = {2012},
	note = {arXiv: 1206.5387},
	keywords = {Statistics - Applications, Statistics - Computation},
	file = {arXiv\:1206.5387 PDF:/Users/jonathanroth/Zotero/storage/73UIP3GK/G and Wilhelm - 2012 - Moments Calculation For the Doubly Truncated Multi.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/HSCW52KD/1206.html:text/html}
}

@article{lee_exact_2016,
	title = {Exact post-selection inference, with application to the lasso},
	volume = {44},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1460381681},
	doi = {10.1214/15-AOS1371},
	abstract = {We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event. We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model.},
	language = {EN},
	number = {3},
	urldate = {2018-02-18},
	journal = {The Annals of Statistics},
	author = {Lee, Jason D. and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E.},
	month = jun,
	year = {2016},
	mrnumber = {MR3485948},
	zmnumber = {1341.62061},
	keywords = {Lasso, confidence interval, hypothesis test, model selection},
	pages = {907--927},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/ZJGQD86H/1460381681.html:text/html}
}

@article{cartinhour_one-dimensional_1990,
	title = {One-dimensional marginal density functions of a truncated multivariate normal density function},
	volume = {19},
	doi = {10.1080/03610929008830197},
	abstract = {The single variable marginal density function of a truncated multivariate normal density function is derived in a form that can be evaluated using an available computer algorithm. It is shown that the marginal density function is a truncated normal density function multiplied by a “skew function”},
	journal = {Communications in Statistics-theory and Methods - COMMUN STATIST-THEOR METHOD},
	author = {Cartinhour, Jack},
	month = jan,
	year = {1990},
	pages = {197--203}
}

@article{hirano_impossibility_2012,
	title = {{IMPOSSIBILITY} {RESULTS} {FOR} {NONDIFFERENTIABLE} {FUNCTIONALS}},
	volume = {80},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/23271416},
	abstract = {We examine challenges to estimation and inference when the objects of interest are nondifferentiable functionals of the underlying data distribution. This situation arises in a number of applications of bounds analysis and moment inequality models, and in recent work on estimating optimal dynamic treatment regimes. Drawing on earlier work relating differentiability to the existence of unbiased and regular estimators, we show that if the target object is not differentiable in the parameters of the data distribution, there exist no estimator sequences that are locally asymptotically unbiased or α-quantile unbiased. This places strong limits on estimators, bias correction methods, and inference procedures, and provides motivation for considering other criteria for evaluating estimators and inference procedures, such as local asymptotic minimaxity and one-sided quantile unbiasedness.},
	number = {4},
	urldate = {2019-07-15},
	journal = {Econometrica},
	author = {Hirano, Keisuke and Porter, Jack R.},
	year = {2012},
	pages = {1769--1790}
}


@techreport{andrews_inference_2019,
	type = {Working {Paper}},
	title = {Inference for {Linear} {Conditional} {Moment} {Inequalities}},
	author = {Andrews, Isaiah and Pakes, Ariel and Roth, Jonathan},
	year = {2019}
}

@book{vaart_weak_1996,
	title = {Weak {Convergence} and {Empirical} {Processes}: {With} {Applications} to {Statistics}},
	isbn = {978-0-387-94640-5},
	shorttitle = {Weak {Convergence} and {Empirical} {Processes}},
	abstract = {This book tries to do three things. The first goal is to give an exposition of certain modes of stochastic convergence, in particular convergence in distribution. The classical theory of this subject was developed mostly in the 1950s and is well summarized in Billingsley (1968). During the last 15 years, the need for a more general theory allowing random elements that are not Borel measurable has become well established, particularly in developing the theory of empirical processes. Part 1 of the book, Stochastic Convergence, gives an exposition of such a theory following the ideas of J. Hoffmann-J!1Jrgensen and R. M. Dudley. A second goal is to use the weak convergence theory background devel oped in Part 1 to present an account of major components of the modern theory of empirical processes indexed by classes of sets and functions. The weak convergence theory developed in Part 1 is important for this, simply because the empirical processes studied in Part 2, Empirical Processes, are naturally viewed as taking values in nonseparable Banach spaces, even in the most elementary cases, and are typically not Borel measurable. Much of the theory presented in Part 2 has previously been scattered in the journal literature and has, as a result, been accessible only to a relatively small number of specialists. In view of the importance of this theory for statis tics, we hope that the presentation given here will make this theory more accessible to statisticians as well as to probabilists interested in statistical applications.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Vaart, AW van der and Wellner, Jon},
	month = mar,
	year = {1996},
	note = {Google-Books-ID: seH8dMrEgggC},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes}
}

@article{kivaranovic_expected_2018,
	title = {Expected length of post-model-selection confidence intervals conditional on polyhedral constraints},
	url = {http://arxiv.org/abs/1803.01665},
	abstract = {Valid inference after model selection is currently a very active area of research. The polyhedral method, pioneered by Lee et al. (2016), allows for valid inference after model selection if the model selection event can be described by polyhedral constraints. In that reference, the method is exemplified by constructing two valid confidence intervals when the Lasso estimator is used to select a model. We here study the expected length of these intervals. For one of these confidence intervals, that is easier to compute, we find that its expected length is always infinite. For the other of these confidence intervals, whose computation is more demanding, we give a necessary and sufficient condition for its expected length to be infinite. In simulations, we find that this condition is typically satisfied.},
	urldate = {2019-06-26},
	journal = {arXiv:1803.01665 [math, stat]},
	author = {Kivaranovic, Danijel and Leeb, Hannes},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.01665},
	keywords = {Mathematics - Statistics Theory},
	file = {arXiv\:1803.01665 PDF:/Users/jonathanroth/Zotero/storage/GSHDECWJ/Kivaranovic and Leeb - 2018 - Expected length of post-model-selection confidence.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/6DXKYPGW/1803.html:text/html}
}

@article{ujhelyi_civil_2014,
	title = {Civil {Service} {Rules} and {Policy} {Choices}: {Evidence} from {US} {State} {Governments}},
	volume = {6},
	issn = {1945-7731},
	shorttitle = {Civil {Service} {Rules} and {Policy} {Choices}},
	url = {https://www.aeaweb.org/articles?id=10.1257/pol.6.2.338},
	doi = {10.1257/pol.6.2.338},
	abstract = {This paper studies the policy impact of civil service regulations,
exploiting reforms undertaken by US state governments throughout
the twentieth century. These reforms replaced political patronage with
a civil service recruited based on merit and protected from politics.
I find that state politicians respond to these changes by spending
relatively less through the reformed state-level bureaucracies.
Instead, they allocate more funds to lower level governments. The
reallocation of expenditures leads to reduced long-term investment
by state governments.},
	language = {en},
	number = {2},
	urldate = {2019-06-26},
	journal = {American Economic Journal: Economic Policy},
	author = {Ujhelyi, Gergely},
	month = may,
	year = {2014},
	keywords = {Administrative Processes in Public Organizations, Bureaucracy, Corruption, State and Local Budget and Expenditures, Intergovernmental Relations, Federalism, Intergovernmental Relations: Other, Secession, State and Local Government},
	pages = {338--380},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/45TWQQQB/articles.html:text/html}
}

@article{tewari_distributive_2014,
	title = {The {Distributive} {Impacts} of {Financial} {Development}: {Evidence} from {Mortgage} {Markets} during {US} {Bank} {Branch} {Deregulation}},
	volume = {6},
	issn = {1945-7782},
	shorttitle = {The {Distributive} {Impacts} of {Financial} {Development}},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.6.4.175},
	doi = {10.1257/app.6.4.175},
	abstract = {Well-functioning credit markets play a key role in boosting overall
economic growth, but their impact on distributional outcomes
is much less clear. I use a quasi-experimental setting provided by
branch banking deregulation, an important episode of US financial
development, to study the distributive impacts of finance. Following
removal of geographic restrictions on banks in the 1980s and early
1990s, mortgage access increased for lower-middle income groups,
young, and also black households. These effects were driven by commercial banks, the only financial institutions subject to the policy.
Banks' new screening technologies may have been responsible for
this expansion of credit.},
	language = {en},
	number = {4},
	urldate = {2019-06-26},
	journal = {American Economic Journal: Applied Economics},
	author = {Tewari, Ishani},
	month = oct,
	year = {2014},
	keywords = {Bond Interest Rates, Financial Institutions and Services: Government Policy and Regulation, Household Saving, Personal Finance, Personal Income, Wealth, and Their Distributions, Asset Pricing, Trading Volume},
	pages = {175--196},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/2UMUBAM9/articles.html:text/html;Submitted Version:/Users/jonathanroth/Zotero/storage/5GTB4EU9/Tewari - 2014 - The Distributive Impacts of Financial Development.pdf:application/pdf}
}

@article{markevich_economic_2018,
	title = {The {Economic} {Effects} of the {Abolition} of {Serfdom}: {Evidence} from the {Russian} {Empire}},
	volume = {108},
	issn = {0002-8282},
	shorttitle = {The {Economic} {Effects} of the {Abolition} of {Serfdom}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20160144},
	doi = {10.1257/aer.20160144},
	abstract = {We document substantial increases in agricultural productivity, industrial output, and peasants' nutrition in Imperial Russia as a result of the abolition of serfdom in 1861. Before the emancipation, provinces where serfs constituted the majority of agricultural laborers lagged behind provinces that primarily relied on free labor. The emancipation led to a significant but partial catch up. Better incentives of peasants resulting from the cessation of ratchet effect were a likely mechanism behind a relatively fast positive effect of reform on agricultural productivity. The land reform, which instituted communal land tenure after the emancipation, diminished growth in productivity in repartition communes.},
	language = {en},
	number = {4-5},
	urldate = {2019-06-26},
	journal = {American Economic Review},
	author = {Markevich, Andrei and Zhuravskaya, Ekaterina},
	month = apr,
	year = {2018},
	keywords = {Coercive Labor Markets, Economic History: Macroeconomics and Monetary Economics, Growth, Industrial Structure, Prices},
	pages = {1074--1117},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/9GHXN8XJ/Markevich and Zhuravskaya - 2018 - The Economic Effects of the Abolition of Serfdom .pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/G8GJ822L/articles.html:text/html}
}

@article{lafortune_school_2018,
	title = {School {Finance} {Reform} and the {Distribution} of {Student} {Achievement}},
	volume = {10},
	issn = {1945-7782},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20160567},
	doi = {10.1257/app.20160567},
	abstract = {We study the impact of post-1990 school finance reforms, during the so-called "adequacy" era, on absolute and relative spending and achievement in low-income school districts. Using an event study research design that exploits the apparent randomness of reform timing, we show that reforms lead to sharp, immediate, and sustained increases in spending in low-income school districts. Using representative samples from the National Assessment of Educational Progress, we find that reforms cause increases in the achievement of students in these districts, phasing in gradually over the years following the reform. The implied effect of school resources on educational achievement is large.},
	language = {en},
	number = {2},
	urldate = {2019-06-26},
	journal = {American Economic Journal: Applied Economics},
	author = {Lafortune, Julien and Rothstein, Jesse and Schanzenbach, Diane Whitmore},
	month = apr,
	year = {2018},
	keywords = {Education, Financial Aid, Education and Inequality, Education: Government Policy, Public Pensions, Analysis of Education, Educational Finance, State and Local Government: Health, Welfare},
	pages = {1--26},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/WM5EIX4E/Lafortune et al. - 2018 - School Finance Reform and the Distribution of Stud.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/KJ7C2EU2/articles.html:text/html}
}

@article{kuziemko_does_2018,
	title = {Does {Managed} {Care} {Widen} {Infant} {Health} {Disparities}? {Evidence} from {Texas} {Medicaid}},
	volume = {10},
	issn = {1945-7731},
	shorttitle = {Does {Managed} {Care} {Widen} {Infant} {Health} {Disparities}?},
	url = {https://www.aeaweb.org/articles?id=10.1257/pol.20150262},
	doi = {10.1257/pol.20150262},
	abstract = {Medicaid programs increasingly finance competing, capitated managed care plans rather than administering fee-for-service (FFS) programs. We study how the transition from FFS to managed care affects high- and low-cost infants (blacks and Hispanics, respectively). We find that black-Hispanic disparities widen—e.g., black mortality and preterm birth rates increase by 15 percent and 7 percent, respectively, while Hispanic mortality and preterm birth rates decrease by 22 percent and 7 percent, respectively. Our results are consistent with a risk-selection model whereby capitation incentivizes competing plans to offer better (worse) care to low- (high-) cost clients to retain (avoid) them in the future.},
	language = {en},
	number = {3},
	urldate = {2019-06-26},
	journal = {American Economic Journal: Economic Policy},
	author = {Kuziemko, Ilyana and Meckel, Katherine and Rossin-Slater, Maya},
	month = aug,
	year = {2018},
	keywords = {Education, State and Local Government: Health, Welfare, Child Care, Children, Family Planning, Non-labor Discrimination, Provision and Effects of Welfare Programs, Fertility, Public Health, Welfare, Well-Being, and Poverty: Government Programs, Public Pensions, Health Behavior, Health: Government Policy, Regulation, Youth, Economics of Minorities, Races, Indigenous Peoples, and Immigrants},
	pages = {255--283},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/9IR2G8S7/articles.html:text/html}
}

@article{gallagher_learning_2014,
	title = {Learning about an {Infrequent} {Event}: {Evidence} from {Flood} {Insurance} {Take}-{Up} in the {United} {States}},
	volume = {6},
	issn = {1945-7782},
	shorttitle = {Learning about an {Infrequent} {Event}},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.6.3.206},
	doi = {10.1257/app.6.3.206},
	abstract = {I examine the learning process that economic agents use to update their expectation of an uncertain and infrequently observed event. I use a new nation-wide panel dataset of large regional floods and flood insurance policies to show that insurance take-up spikes the year after a flood and then steadily declines to baseline. Residents in nonflooded communities in the same television media market increase take-up at one-third the rate of flooded communities. I find that insurance take-up is most consistent with a Bayesian learning model that allows for forgetting or incomplete information about past floods.},
	language = {en},
	number = {3},
	urldate = {2019-06-26},
	journal = {American Economic Journal: Applied Economics},
	author = {Gallagher, Justin},
	month = jul,
	year = {2014},
	keywords = {Global Warming, Actuarial Studies, Climate, Belief, Expectations, Communication, Consumer Economics: Empirical Analysis, Search, Information and Knowledge, Insurance Companies, Learning, Natural Disasters, Speculations, Insurance},
	pages = {206--233},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/NNF5KJBZ/Gallagher - 2014 - Learning about an Infrequent Event Evidence from .pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/LXN3LBL5/articles.html:text/html}
}

@article{deschenes_defensive_2017,
	title = {Defensive {Investments} and the {Demand} for {Air} {Quality}: {Evidence} from the {NOx} {Budget} {Program}},
	volume = {107},
	issn = {0002-8282},
	shorttitle = {Defensive {Investments} and the {Demand} for {Air} {Quality}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20131002},
	doi = {10.1257/aer.20131002},
	abstract = {The demand for air quality depends on health impacts and defensive investments, but little research assesses the empirical importance of defenses. A rich quasi-experiment suggests that the Nitrogen Oxides (NOx) Budget Program (NBP), a cap-and-trade market, decreased NOx emissions, ambient ozone concentrations, pharmaceutical expenditures, and mortality rates. The annual reductions in pharmaceutical purchases, a key defensive investment, and mortality are valued at about \$800 million and \$1.3 billion, respectively, suggesting that defenses are over one-third of willingness-to-pay for reductions in NOx emissions. Further, estimates indicate that the NBP's benefits easily exceed its costs and that NOx reductions have substantial benefits.},
	language = {en},
	number = {10},
	urldate = {2019-06-26},
	journal = {American Economic Review},
	author = {Deschênes, Olivier and Greenstone, Michael and Shapiro, Joseph S.},
	month = oct,
	year = {2017},
	keywords = {Hazardous Waste, Health Behavior, Valuation of Environmental Effects, Air Pollution, Noise, Recycling, Environmental Economics: Government Policy, Solid Waste, Water Pollution},
	pages = {2958--2989},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/FLFAKEPD/Deschênes et al. - 2017 - Defensive Investments and the Demand for Air Quali.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/BZKJ4ZYZ/articles.html:text/html}
}

@article{bosch_trade-offs_2014-1,
	title = {The {Trade}-{Offs} of {Welfare} {Policies} in {Labor} {Markets} with {Informal} {Jobs}: {The} {Case} of the "{Seguro} {Popular}" {Program} in {Mexico}},
	volume = {6},
	issn = {1945-7731},
	shorttitle = {The {Trade}-{Offs} of {Welfare} {Policies} in {Labor} {Markets} with {Informal} {Jobs}},
	url = {https://www.aeaweb.org/articles?id=10.1257/pol.6.4.71},
	doi = {10.1257/pol.6.4.71},
	abstract = {In 2002, the Mexican government began an effort to improve health
access to the 50 million uninsured in Mexico, a program known as
Seguro Popular (SP). The SP offered virtually free health insurance
to informal workers, altering the incentives to operate in the formal
economy. We find that the SP program had a negative effect on the
number of employers and employees formally registered in small and
medium firms (up to 50 employees). Our results suggest that the positive gains of expanding health coverage should be weighed against the implications of the reallocation of labor away from the formal sector.},
	language = {en},
	number = {4},
	urldate = {2019-06-26},
	journal = {American Economic Journal: Economic Policy},
	author = {Bosch, Mariano and Campos-Vazquez, Raymundo M.},
	month = nov,
	year = {2014},
	keywords = {Public Health, Welfare, Well-Being, and Poverty: Government Programs, Regulation, Human Development, Income Distribution, Informal Economy, Institutional Arrangements, Migration, Formal and Informal Sectors, Provision and Effects of Welfare Programs, Informal Labor Markets, Economic Development: Human Resources, Shadow Economy, Underground Economy, Health Insurance, Public and Private, Health: Government Policy},
	pages = {71--99},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/DU47Y6TG/articles.html:text/html}
}

@article{bailey_war_2015,
	title = {The {War} on {Poverty}'s {Experiment} in {Public} {Medicine}: {Community} {Health} {Centers} and the {Mortality} of {Older} {Americans}},
	volume = {105},
	issn = {0002-8282},
	shorttitle = {The {War} on {Poverty}'s {Experiment} in {Public} {Medicine}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20120070},
	doi = {10.1257/aer.20120070},
	abstract = {This paper uses the rollout of the first Community Health Centers (CHCs) to study the longer-term health effects of increasing access to primary care. Within ten years, CHCs are associated with a reduction in age-adjusted mortality rates of 2 percent among those 50 and older. The implied 7 to 13 percent decrease in one-year mortality risk among beneficiaries amounts to 20 to 40 percent of the 1966 poor/non-poor mortality gap for this age group. Large effects for those 65 and older suggest that increased access to primary care has longer-term benefits, even for populations with near universal health insurance. (JEL H75, I12, I13, I18, I32, I38, J14)},
	language = {en},
	number = {3},
	urldate = {2019-06-26},
	journal = {American Economic Review},
	author = {Bailey, Martha J. and Goodman-Bacon, Andrew},
	month = mar,
	year = {2015},
	keywords = {Education, State and Local Government: Health, Welfare, Regulation, Economics of the Handicapped, Non-labor Market Discrimination, Provision and Effects of Welfare Programs, Economics of the Elderly, Public Health, Measurement and Analysis of Poverty, Welfare, Well-Being, and Poverty: Government Programs, Public Pensions, Health Behavior, Health Insurance, Public and Private, Health: Government Policy},
	pages = {1067--1104},
	file = {Accepted Version:/Users/jonathanroth/Zotero/storage/G3ZEDBQ5/Bailey and Goodman-Bacon - 2015 - The War on Poverty's Experiment in Public Medicine.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/BLUJQ9BI/articles.html:text/html}
}

@article{andrews_identification_nodate,
	title = {Identification of and {Correction} for {Publication} {Bias}},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20180310&&from=f},
	doi = {10.1257/aer.20180310},
	language = {en},
	urldate = {2019-06-26},
	journal = {American Economic Review},
	author = {Andrews, Isaiah and Kasy, Maximilian},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/3X9GM33X/articles.html:text/html}
}

@misc{noauthor_rstudio_nodate,
	title = {{RStudio}},
	url = {https://vdi.rc.fas.harvard.edu/rnode/holy7c05203.rc.fas.harvard.edu/10110/},
	urldate = {2019-06-21},
	file = {RStudio:/Users/jonathanroth/Zotero/storage/HRZQQ2Q5/10110.html:text/html}
}

@book{cohen_statistical_1988,
	title = {Statistical power analysis for the behavioral sciences},
	abstract = {Statistical Power Analysis is a nontechnical guide to power analysis in research planning that provides users of applied statistics with the tools they need for more effective analysis. The Second Edition includes: * a chapter covering power analysis in set correlation and multivariate methods; * a chapter considering effect size, psychometric reliability, and the efficacy of "qualifying" dependent variables and; * expanded power and sample size tables for multiple regression/correlation.},
	language = {en},
	publisher = {Academic Press},
	author = {Cohen, Jacob},
	year = {1988},
	note = {Google-Books-ID: YleCAAAAIAAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Multivariate Analysis, Probabilities, Psychology / Statistics, Social sciences}
}

@article{muller_efficient_2011,
	title = {Efficient tests under a weak convergence assumption},
	volume = {79},
	number = {2},
	journal = {Econometrica},
	author = {Müller, Ulrich K},
	year = {2011},
	note = {bibtex: muller2011efficient},
	pages = {395--435}
}

@book{lehmann_testing_1986,
	title = {Testing {Statistical} {Hypotheses}},
	isbn = {978-0-471-84083-1},
	abstract = {This book covers the theory of hypotheses testing and of estimation by confidence intervals. Accompanying Theory of Point Estimation (1983) to cover the main topics of classical statistics, including theory and its principal applications, this second edition contains more on confidence intervals, simultaneous inference, admissibility, and conditioning. The book is thoroughly updated throughout with a new section on conditional inference and an expansion of multivariate material.},
	language = {en},
	publisher = {Wiley},
	author = {Lehmann, Erich Leo},
	month = jan,
	year = {1986},
	note = {Google-Books-ID: jexQAAAAMAAJ}
}

@book{schrijver_theory_1986,
	title = {Theory of {Linear} and {Integer} {Programming}},
	publisher = {Wiley-Interscience},
	author = {Schrijver, Alexander},
	year = {1986},
	note = {bibtex: Schrijver1986 
bibtex[date-added=2017-07-19 18:00:09 +0000;date-modified=2017-07-19 18:10:27 +0000]}
}

@article{romano_practical_2014,
	title = {A {Practical} {Two}-{Step} {Method} for {Testing} {Moment} {Inequalities}},
	volume = {82},
	copyright = {© 2014 The Econometric Society},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11011},
	doi = {10.3982/ECTA11011},
	abstract = {This paper considers the problem of testing a finite number of moment inequalities. We propose a two-step approach. In the first step, a confidence region for the moments is constructed. In the second step, this set is used to provide information about which moments are “negative.” A Bonferonni-type correction is used to account for the fact that, with some probability, the moments may not lie in the confidence region. It is shown that the test controls size uniformly over a large class of distributions for the observed data. An important feature of the proposal is that it remains computationally feasible, even when the number of moments is large. The finite-sample properties of the procedure are examined via a simulation study, which demonstrates, among other things, that the proposal remains competitive with existing procedures while being computationally more attractive.},
	language = {en},
	number = {5},
	journal = {Econometrica},
	author = {Romano, Joseph P. and Shaikh, Azeem M. and Wolf, Michael},
	year = {2014},
	keywords = {Bonferonni inequality, bootstrap, moment inequalities, partial identification, uniform validity},
	pages = {1979--2002},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/XCUZCMHR/ECTA11011.html:text/html}
}

@article{kaido_confidence_2016,
	title = {Confidence {Intervals} for {Projections} of {Partially} {Identified} {Parameters}},
	url = {http://arxiv.org/abs/1601.00934},
	abstract = {We propose a bootstrap-based calibrated projection procedure to build confidence intervals for single components and for smooth functions of a partially identified parameter vector in moment (in)equality models. The method controls asymptotic coverage uniformly over a large class of data generating processes. The extreme points of the calibrated projection confidence interval are obtained by extremizing the value of the function of interest subject to a proper relaxation of studentized sample analogs of the moment (in)equality conditions. The degree of relaxation, or critical level, is calibrated so that the function of \${\textbackslash}theta \$, not \${\textbackslash}theta \$ itself, is uniformly asymptotically covered with prespecified probability. This calibration is based on repeatedly checking feasibility of linear programming problems, rendering it computationally attractive. Nonetheless, the program defining an extreme point of the confidence interval is generally nonlinear and potentially intricate. We provide an algorithm, based on the response surface method for global optimization, that approximates the solution rapidly and accurately, and we establish its rate of convergence. The algorithm is of independent interest for optimization problems with simple objectives and complicated constraints. An empirical application estimating an entry game illustrates the usefulness of the method. Monte Carlo simulations confirm the accuracy of the solution algorithm, the good statistical as well as computational performance of calibrated projection (including in comparison to other methods), and the algorithm's potential to greatly accelerate computation of other confidence intervals.},
	journal = {arXiv:1601.00934 [econ, math, stat]},
	author = {Kaido, Hiroaki and Molinari, Francesca and Stoye, Jörg},
	month = jan,
	year = {2016},
	note = {arXiv: 1601.00934},
	keywords = {Economics - Econometrics, Mathematics - Statistics Theory},
	file = {arXiv\:1601.00934 PDF:/Users/jonathanroth/Zotero/storage/ZZ7IVFUA/Kaido et al. - 2016 - Confidence Intervals for Projections of Partially .pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/IJ8HVAXF/1601.html:text/html}
}



@article{Chabe-Ferret2015,
abstract = {Matching and Difference in Difference (DID) are two widespread methods that use pre-treatment outcomes to correct for selection bias. I detail the sources of bias of both estimators in a model of earnings dynamics and entry into a Job Training Program (JTP) and I assess their performances using Monte Carlo simulations of the model calibrated with realistic parameter values. I find that Matching generally underestimates the average causal effect of the program and gets closer to the true effect when conditioning on an increasing number of pre-treatment outcomes. When selection bias is symmetric around the treatment date, DID is consistent when implemented symmetrically - i.e. comparing outcomes observed the same number of periods before and after the treatment date. When selection bias is not symmetric, Monte Carlo simulations show that Symmetric DID still performs better than Matching, especially in the middle of the life-cycle. These results are consistent with estimates of the bias of Matching and DID from randomly assigned JTPs. Some of the virtues of Symmetric DID extend to programs other than JTPs allocated according to a cutoff eligibility rule.},
author = {Chab{\'{e}}-Ferret, Sylvain},
doi = {10.1016/j.jeconom.2014.09.013},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chab{\'{e}}-Ferret - 2015 - Analysis of the bias of Matching and Difference-in-Difference under alternative earnings and selection processes.pdf:pdf},
isbn = {03044076},
issn = {18726895},
journal = {Journal of Econometrics},
keywords = {Difference in Difference,Job Training Programs,Matching},
number = {1},
pages = {110--123},
publisher = {Elsevier B.V.},
title = {{Analysis of the bias of Matching and Difference-in-Difference under alternative earnings and selection processes}},
volume = {185},
year = {2015}
}


@article{Ding2019,
author = {Ding, Peng and Li, Fan},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/others/bracketing{\_}relationship{\_}between{\_}differenceindifferences{\_}and{\_}laggeddependentvariable{\_}adjustment.pdf:pdf},
journal = {Political Analysis},
keywords = {causal inference,ignorability,nonparametric,panel data,parallel trends},
pages = {605----615},
title = {{A Bracketing Relationship between Difference-in-Differences and Lagged-Dependent-Variable Adjustment}},
volume = {27},
year = {2019}
}

@article{romano_practical_2014-1,
	title = {A {Practical} {Two}-{Step} {Method} for {Testing} {Moment} {Inequalities}},
	volume = {82},
	copyright = {© 2014 The Econometric Society},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA11011},
	doi = {10.3982/ECTA11011},
	abstract = {This paper considers the problem of testing a finite number of moment inequalities. We propose a two-step approach. In the first step, a confidence region for the moments is constructed. In the second step, this set is used to provide information about which moments are “negative.” A Bonferonni-type correction is used to account for the fact that, with some probability, the moments may not lie in the confidence region. It is shown that the test controls size uniformly over a large class of distributions for the observed data. An important feature of the proposal is that it remains computationally feasible, even when the number of moments is large. The finite-sample properties of the procedure are examined via a simulation study, which demonstrates, among other things, that the proposal remains competitive with existing procedures while being computationally more attractive.},
	language = {en},
	number = {5},
	journal = {Econometrica},
	author = {Romano, Joseph P. and Shaikh, Azeem M. and Wolf, Michael},
	year = {2014},
	keywords = {Bonferonni inequality, bootstrap, moment inequalities, partial identification, uniform validity},
	pages = {1979--2002},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/NGRP2W4P/ECTA11011.html:text/html}
}

@article{de_chaisemartin_fuzzy_2018,
	title = {Fuzzy {Differences}-in-{Differences}},
	volume = {85},
	issn = {0034-6527},
	url = {https://academic.oup.com/restud/article/85/2/999/4096388},
	doi = {10.1093/restud/rdx049},
	abstract = {Abstract.  Difference-in-differences (DID) is a method to evaluate the effect of a treatment. In its basic version, a “control group” is untreated at two dates,},
	language = {en},
	number = {2},
	urldate = {2018-09-21},
	journal = {The Review of Economic Studies},
	author = {de Chaisemartin, Clément and D'Haultfœuille, Xavier},
	year = {2018},
	pages = {999--1028},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/NPBRD2WJ/4096388.html:text/html}
}



@book{ingster_nonparametric_2003,
	address = {New York},
	series = {Lecture {Notes} in {Statistics}},
	title = {Nonparametric {Goodness}-of-{Fit} {Testing} {Under} {Gaussian} {Models}},
	isbn = {978-0-387-95531-5},
	url = {//www.springer.com/us/book/9780387955315},
	abstract = {From the reviews: "The book is self-contained, and the bibliography is very rich and in fact provides a comprehensive listing of references about minimax testing (something that heretofore had been missing from the field.) To get the best out of this book, the reader should be familiar with basic functional analysis, wavelet theory, and optimization for extreme problems…It is highly recommended to anyone who wants an introduction to hypothesis testing from the minimax approach–yet it is only a starting point, as Gaussian models are studied exclusively." Journal of the American Statistical Association, June 2004 "The book deals with nonparametric goodness-of-fit testing problems from the literature of the past twenty years. … It is a theoretical book with mathematical results … . The proofs of the theorems are very detailed and many details are in the appendix of more than one hundred pages." (N. D. C. Veraverbeke, Short Book Reviews, Vol. 24 (1), 2004) "The present book is devoted to a modern theory of nonparametric goodness-of-fit testing. … The level of the book meets a quite high standard. The book will certainly be of interest to mathematical statisticians interested in the theory of nonparametric statistical interference, and also to specialists dealing with applied nonparametric statistical problems in signal detection and transmission, technical and medical diagnostics, and other fields." (Marie Huškova, Zentralblatt MATH, Vol. 1013, 2003)},
	language = {en},
	urldate = {2019-01-28},
	publisher = {Springer-Verlag},
	author = {Ingster, Yuri and Suslina, I. A.},
	year = {2003},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/TESE9H9W/9780387955315.html:text/html}
}

@article{conley_plausibly_2010,
	title = {Plausibly {Exogenous}},
	volume = {94},
	issn = {0034-6535},
	url = {https://doi.org/10.1162/REST_a_00139},
	doi = {10.1162/REST_a_00139},
	abstract = {Abstract Instrumental variable (IV) methods are widely used to identify causal effects in models with endogenous explanatory variables. Often the instrument exclusion restriction that underlies the validity of the usual IV inference is suspect; that is, instruments are only plausibly exogenous. We present practical methods for performing inference while relaxing the exclusion restriction. We illustrate the approaches with empirical examples that examine the effect of 401(k) participation on asset accumulation, price elasticity of demand for margarine, and returns to schooling. We find that inference is informative even with a substantial relaxation of the exclusion restriction in two of the three cases.},
	number = {1},
	journal = {The Review of Economics and Statistics},
	author = {Conley, Timothy G. and Hansen, Christian B. and Rossi, Peter E.},
	month = oct,
	year = {2010},
	pages = {260--272},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/MGKURSVS/Conley et al. - 2010 - Plausibly Exogenous.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/ZCEW4W64/REST_a_00139.html:text/html}
}

@article{van_der_vaart_weak_1997,
	title = {Weak convergence and empirical processes with applications to statistics},
	author = {van der Vaart, Aad W and Wellner, Jon A},
	year = {1997},
	note = {bibtex: van1997weak}
}

@book{vaart_asymptotic_2000,
	title = {Asymptotic {Statistics}},
	isbn = {978-0-521-78450-4},
	abstract = {This book is an introduction to the field of asymptotic statistics. The treatment is both practical and mathematically rigorous. In addition to most of the standard topics of an asymptotics course, including likelihood inference, M-estimation, the theory of asymptotic efficiency, U-statistics, and rank procedures, the book also presents recent research topics such as semiparametric models, the bootstrap, and empirical processes and their applications. The topics are organized from the central idea of approximation by limit experiments, which gives the book one of its unifying themes. This entails mainly the local approximation of the classical i.i.d. set up with smooth parameters by location experiments involving a single, normally distributed observation. Thus, even the standard subjects of asymptotic statistics are presented in a novel way. Suitable as a graduate or Master s level statistics text, this book will also give researchers an overview of the latest research in asymptotic statistics.},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Vaart, A. W. van der},
	month = jun,
	year = {2000},
	note = {Google-Books-ID: UEuQEM5RjWgC},
	keywords = {Mathematics / Probability \& Statistics / General}
}



@article{jacob_impact_2011,
	title = {The impact of research grant funding on scientific productivity},
	volume = {95},
	url = {https://ideas.repec.org/a/eee/pubeco/v95y2011i9-10p1168-1177.html},
	abstract = {In this paper, we estimate the impact of receiving an NIH grant on subsequent publications and citations. Our sample consists of all applications (unsuccessful as well as successful) to the NIH from 1980 to 2000 for standard research grants (R01s). Both OLS and IV estimates show that receipt of an NIH research grant (worth roughly \$1.7Â million) leads to only one additional publication over the next five years, which corresponds to a 7\% increase. The limited impact of NIH grants is consistent with a model in which the market for research funding is competitive, so that the loss of an NIH grant simply causes researchers to shift to another source of funding.},
	language = {en},
	number = {9-10},
	urldate = {2018-10-08},
	journal = {Journal of Public Economics},
	author = {Jacob, Brian A. and Lefgren, Lars},
	year = {2011},
	note = {bibtex: jacob\_impact\_2011},
	keywords = {Scientific productivity Regression discontinuity Government funding},
	pages = {1168--1177},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/NRQHGCIZ/v95y2011i9-10p1168-1177.html:text/html}
}

@article{li_big_2015,
	title = {Big names or big ideas: {Do} peer-review panels select the best science proposals?},
	volume = {348},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Big names or big ideas},
	url = {http://science.sciencemag.org/content/348/6233/434},
	doi = {10.1126/science.aaa0185},
	abstract = {Proof that peer review picks promising proposals
A key issue in the economics of science is finding effective mechanisms for innovation. A concern about research grants and other research and development subsidies is that the public sector may make poor decisions about which projects to fund. Despite its importance, especially for the advancement of basic and early-stage science, there is currently no large-scale empirical evidence on how successfully governments select research investments. Li and Agha analyze more than 130,000 grants funded by the U.S. National Institutes of Health during 1980–2008 and find clear benefits of peer evaluations, particularly for distinguishing high-impact potential among the most competitive applications.
Science, this issue p. 434
This paper examines the success of peer-review panels in predicting the future quality of proposed research. We construct new data to track publication, citation, and patenting outcomes associated with more than 130,000 research project (R01) grants funded by the U.S. National Institutes of Health from 1980 to 2008. We find that better peer-review scores are consistently associated with better research outcomes and that this relationship persists even when we include detailed controls for an investigator’s publication history, grant history, institutional affiliations, career stage, and degree types. A one–standard deviation worse peer-review score among awarded grants is associated with 15\% fewer citations, 7\% fewer publications, 19\% fewer high-impact publications, and 14\% fewer follow-on patents.
Peer-review panels predict publications, citations, and patents from proposed research.
Peer-review panels predict publications, citations, and patents from proposed research.},
	language = {en},
	number = {6233},
	urldate = {2018-10-08},
	journal = {Science},
	author = {Li, Danielle and Agha, Leila},
	month = apr,
	year = {2015},
	pmid = {25908820},
	pages = {434--438},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/ANAFUPQ7/434.html:text/html}
}

@book{genz_mvtnorm:_2018,
	title = {mvtnorm: {Multivariate} {Normal} and t {Distributions}},
	url = {https://CRAN.R-project.org/package=mvtnorm},
	author = {Genz, Alan and Bretz, Frank and Miwa, Tetsuhisa and Mi, Xuefei and Leisch, Friedrich and Scheipl, Fabian and Hothorn, Torsten},
	year = {2018},
	annote = {R package version 1.0-8}
}

@article{he_college_2017,
	title = {Do {College} {Graduates} {Serving} as {Village} {Officials} {Help} {Rural} {China}?},
	volume = {9},
	issn = {1945-7782},
	url = {https://www.aeaweb.org/articles?id=10.1257/app.20160079},
	doi = {10.1257/app.20160079},
	abstract = {This study estimates the effect of improved bureaucrat quality on poverty alleviation by exploring a unique human capital reallocation policy in China—the College Graduate Village Officials (CGVOs) program. We find that introducing CGVOs into the village governance system improves the targeting and implementation of the central government's social assistance programs. CGVOs help eligible poor households understand and apply for relevant subsidies, thus increasing the number of pro-poor program beneficiaries. Further analysis suggests that CGVOs change bureaucrat quality, rather than quantity, of village governance, and their presence reduces elite capture of pro-poor programs.},
	language = {en},
	number = {4},
	urldate = {2018-09-28},
	journal = {American Economic Journal: Applied Economics},
	author = {He, Guojun and Wang, Shaoda},
	month = oct,
	year = {2017},
	keywords = {Administrative Processes in Public Organizations, Bureaucracy, Shadow Economy, Corruption, Public Administration, Housing, Infrastructure, Socialist Systems and Transitional Economies: Urban, Rural, and Regional Economics, Socialist Systems and Transitional Economies: Political Economy, Institutional Arrangements, Economic Development: Urban, Rural, Regional, and Transportation Analysis, Labor Productivity, Formal and Informal Sectors, Occupational Choice, Property Rights, Public Sector Accounting and Audits, Human Capital, Skills},
	pages = {186--215},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/PTMKMSSW/He and Wang - 2017 - Do College Graduates Serving as Village Officials .pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/VAJSC6ZB/articles.html:text/html}
}

@techreport{arismendi_zambrano_multivariate_2016,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Multivariate {Elliptical} {Truncated} {Moments}},
	url = {https://papers.ssrn.com/abstract=2841401},
	abstract = {In this study, we derived analytic expressions for the elliptical truncated moment generating function (MGF), the zeroth, first, and second-order moments of quadratic forms of the multivariate normal, Student’s t, and generalised hyperbolic distributions. The resulting formulae were tested in a numerical application to calculate an analytic expression of the expected shortfall of quadratic portfolios with the benefit that moment based sensitivity measures can be derived from the analytic expression. The convergence rate of the analytic expression is fast – one iteration – for small closed integration domains, and slower for open integration domains when compared to the Monte Carlo integration method. The analytic formulae provide a theoretical framework for calculations in robust estimation, robust regression, outlier detection, design of experiments, and stochastic extensions of deterministic elliptical curves results.},
	language = {en},
	number = {ID 2841401},
	urldate = {2018-09-28},
	institution = {Social Science Research Network},
	author = {Arismendi Zambrano, Juan and Broda, Simon A.},
	month = sep,
	year = {2016},
	keywords = {Elliptical Functions, Elliptical Truncation, Multivariate Truncated Moments, Parametric Distributions, Quadratic Forms, Tail Moments},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/ZJ8C4MEX/papers.html:text/html}
}

@article{tallis_elliptical_1963,
	title = {Elliptical and {Radial} {Truncation} in {Normal} {Populations}},
	volume = {34},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177704016},
	doi = {10.1214/aoms/1177704016},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	number = {3},
	urldate = {2018-09-28},
	journal = {The Annals of Mathematical Statistics},
	author = {Tallis, G. M.},
	month = sep,
	year = {1963},
	mrnumber = {MR152081},
	zmnumber = {0142.16104},
	pages = {940--944},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/5CJBUGSC/1177704016.html:text/html}
}

@article{kolesar_inference_2018,
	title = {Inference in {Regression} {Discontinuity} {Designs} with a {Discrete} {Running} {Variable}},
	volume = {108},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20160945},
	doi = {10.1257/aer.20160945},
	abstract = {We consider inference in regression discontinuity designs when the running variable only takes a moderate number of distinct values. In particular, we study the common practice of using confidence intervals (CIs) based on standard errors that are clustered by the running variable as a means to make inference robust to model misspecification (Lee and Card 2008). We derive theoretical results and present simulation and empirical evidence showing that these CIs do not guard against model misspecification, and that they have poor coverage properties. We 
therefore recommend against using these CIs in practice. We instead propose two alternative CIs with guaranteed coverage properties under easily interpretable restrictions on the conditional expectation function.},
	language = {en},
	number = {8},
	urldate = {2018-09-28},
	journal = {American Economic Review},
	author = {Kolesár, Michal and Rothe, Christoph},
	month = aug,
	year = {2018},
	keywords = {Severance Pay, Child Care, Children, Family Planning, Estimation: General, Model Construction and Estimation, Fertility, Plant Closings, Wage Differentials, Unemployment: Models, Duration, Incidence, and Job Search, Unemployment Insurance, Youth, Wage Level and Structure},
	pages = {2277--2304},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/39GTUAPV/articles.html:text/html}
}

@article{donoho_statistical_1994,
	title = {Statistical {Estimation} and {Optimal} {Recovery}},
	volume = {22},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176325367},
	doi = {10.1214/aos/1176325367},
	abstract = {New formulas are given for the minimax linear risk in estimating a linear functional of an unknown object from indirect data contaminated with random Gaussian noise. The formulas cover a variety of loss functions and do not require the symmetry of the convex a priori class. It is shown that affine minimax rules are within a few percent of minimax even among nonlinear rules, for a variety of loss functions. It is also shown that difficulty of estimation is measured by the modulus of continuity of the functional to be estimated. The method of proof exposes a correspondence between minimax affine estimates in the statistical estimation problem and optimal algorithms in the theory of optimal recovery.},
	language = {EN},
	number = {1},
	urldate = {2019-07-19},
	journal = {The Annals of Statistics},
	author = {Donoho, David L.},
	year = {1994},
	mrnumber = {MR1272082},
	zmnumber = {0805.62014},
	keywords = {Bounded normal mean, confidence statements for linear functionals, density estimation, estimation of linear functionals, minimax risk, modulus of continuity, nonparametric regression},
	pages = {238--270},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/KG397IZA/Donoho - 1994 - Statistical Estimation and Optimal Recovery.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/MNI2AWHC/1176325367.html:text/html}
}

@article{manski_how_2017,
	title = {How {Do} {Right}-to-{Carry} {Laws} {Affect} {Crime} {Rates}? {Coping} with {Ambiguity} {Using} {Bounded}-{Variation} {Assumptions}},
	volume = {100},
	issn = {0034-6535},
	shorttitle = {How {Do} {Right}-to-{Carry} {Laws} {Affect} {Crime} {Rates}?},
	url = {https://doi.org/10.1162/REST_a_00689},
	doi = {10.1162/REST_a_00689},
	abstract = {Despite dozens of studies, research on crime has struggled to reach consensus about the impact of right-to-carry (RTC) gun laws. With this in mind, we formalize and apply a class of bounded-variation assumptions that flexibly restrict the degree to which outcomes may vary across time and space. Using these assumptions, we present empirical analysis of the effect of RTC laws on violent and property crimes in Virginia, Maryland, and Illinois. Imposing specific assumptions that we believe worthy of consideration, we find that RTC laws increase some crimes, decrease other crimes, and have effects that vary over time for others.},
	number = {2},
	urldate = {2019-07-19},
	journal = {The Review of Economics and Statistics},
	author = {Manski, Charles F. and Pepper, John V.},
	year = {2018},
	pages = {232--244},
	file = {Full Text:/Users/jonathanroth/Zotero/storage/2SKGMRBL/Manski and Pepper - 2017 - How Do Right-to-Carry Laws Affect Crime Rates Cop.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/BDFRB7C8/REST_a_00689.html:text/html}
}

@article{altonji_selection_2005,
	title = {Selection on {Observed} and {Unobserved} {Variables}: {Assessing} the {Effectiveness} of {Catholic} {Schools}},
	volume = {113},
	issn = {0022-3808},
	shorttitle = {Selection on {Observed} and {Unobserved} {Variables}},
	url = {https://www.jstor.org/stable/10.1086/426036},
	doi = {10.1086/426036},
	abstract = {In this paper we measure the effect of Catholic high school attendance on educational attainment and test scores. Because we do not have a good instrumental variable for Catholic school attendance, we develop new estimation methods based on the idea that the amount of selection on the observed explanatory variables in a model provides a guide to the amount of selection on the unobservables. We also propose an informal way to assess selectivity bias based on measuring the ratio of selection on unobservables to selection on observables that would be required if one is to attribute the entire effect of Catholic school attendance to selection bias. We use our methods to estimate the effect of attending a Catholic high school on a variety of outcomes. Our main conclusion is that Catholic high schools substantially increase the probability of graduating from high school and, more tentatively, attending college. We find little evidence of an effect on test scores.},
	number = {1},
	urldate = {2019-07-19},
	journal = {Journal of Political Economy},
	author = {Altonji, Joseph G. and Elder, Todd E. and Taber, Christopher R.},
	year = {2005},
	pages = {151--184},
	file = {Submitted Version:/Users/jonathanroth/Zotero/storage/4AQRDHKN/Altonji et al. - 2005 - Selection on Observed and Unobserved Variables As.pdf:application/pdf}
}

@article{oster_unobservable_2019,
	title = {Unobservable {Selection} and {Coefficient} {Stability}: {Theory} and {Evidence}},
	volume = {37},
	issn = {0735-0015},
	shorttitle = {Unobservable {Selection} and {Coefficient} {Stability}},
	url = {https://amstat.tandfonline.com/doi/abs/10.1080/07350015.2016.1227711},
	doi = {10.1080/07350015.2016.1227711},
	abstract = {A common approach to evaluating robustness to omitted variable bias is to observe coefficient movements after inclusion of controls. This is informative only if selection on observables is informative about selection on unobservables. Although this link is known in theory in existing literature, very few empirical articles approach this formally. I develop an extension of the theory that connects bias explicitly to coefficient stability. I show that it is necessary to take into account coefficient and R-squared movements. I develop a formal bounding argument. I show two validation exercises and discuss application to the economics literature. Supplementary materials for this article are available online.},
	number = {2},
	urldate = {2019-07-19},
	journal = {Journal of Business \& Economic Statistics},
	author = {Oster, Emily},
	month = apr,
	year = {2019},
	pages = {187--204},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/PCUULH53/07350015.2016.html:text/html}
}

@misc{noauthor_low_nodate,
	title = {Low : {Bias}-{Variance} {Tradeoffs} in {Functional} {Estimation} {Problems}},
	url = {https://projecteuclid.org/euclid.aos/1176324624},
	urldate = {2019-07-19},
	file = {Low \: Bias-Variance Tradeoffs in Functional Estimation Problems:/Users/jonathanroth/Zotero/storage/VRXQMHC2/1176324624.html:text/html}
}

@article{low_bias-variance_1995,
	title = {Bias-{Variance} {Tradeoffs} in {Functional} {Estimation} {Problems}},
	volume = {23},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176324624},
	doi = {10.1214/aos/1176324624},
	abstract = {It is shown in infinite-dimensional Gaussian problems that affine estimators minimax the variance among all estimators of a linear functional subject to a constraint on the bias. Likewise, affine estimators also minimax the square of the bias among all estimates of a linear functional subject to a constraint on the variance.},
	language = {EN},
	number = {3},
	urldate = {2019-07-19},
	journal = {The Annals of Statistics},
	author = {Low, Mark G.},
	month = jun,
	year = {1995},
	mrnumber = {MR1345202},
	zmnumber = {0838.62006},
	keywords = {minimax risk, modulus of continuity, Bias-variance tradeoff, Cramer-Rao inequality, white noise model},
	pages = {824--835},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/BWD5XZ3P/Low - 1995 - Bias-Variance Tradeoffs in Functional Estimation P.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/BNX8EF42/1176324624.html:text/html}
}

@article{lovenheim_long-run_nodate,
	title = {The {Long}-{Run} {Effects} of {Teacher} {Collective} {Bargaining}},
	issn = {1945-7731},
	url = {https://www.aeaweb.org/articles?id=10.1257/pol.20170570&&from=f},
	doi = {10.1257/pol.20170570},
	language = {en},
	urldate = {2019-07-21},
	journal = {American Economic Journal: Economic Policy},
	author = {Lovenheim, Michael F. and Willén, Alexander},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/65KI9D2P/articles.html:text/html}
}

@article{andrews_inference_2010,
	title = {Inference for {Parameters} {Defined} by {Moment} {Inequalities} {Using} {Generalized} {Moment} {Selection}},
	volume = {78},
	copyright = {© 2010 The Econometric Society},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA7502},
	doi = {10.3982/ECTA7502},
	abstract = {The topic of this paper is inference in models in which parameters are defined by moment inequalities and/or equalities. The parameters may or may not be identified. This paper introduces a new class of confidence sets and tests based on generalized moment selection (GMS). GMS procedures are shown to have correct asymptotic size in a uniform sense and are shown not to be asymptotically conservative. The power of GMS tests is compared to that of subsampling, m out of n bootstrap, and “plug-in asymptotic” (PA) tests. The latter three procedures are the only general procedures in the literature that have been shown to have correct asymptotic size (in a uniform sense) for the moment inequality/equality model. GMS tests are shown to have asymptotic power that dominates that of subsampling, m out of n bootstrap, and PA tests. Subsampling and m out of n bootstrap tests are shown to have asymptotic power that dominates that of PA tests.},
	language = {en},
	number = {1},
	urldate = {2019-07-23},
	journal = {Econometrica},
	author = {Andrews, Donald W. K. and Soares, Gustavo},
	year = {2010},
	keywords = {moment inequalities, asymptotic power, Asymptotic size, confidence set, exact size, generalized moment selection, m out of n bootstrap, moment selection, subsampling, test},
	pages = {119--157},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/CXU6HC75/ECTA7502.html:text/html;Submitted Version:/Users/jonathanroth/Zotero/storage/MPSYTXXU/Andrews and Soares - 2010 - Inference for Parameters Defined by Moment Inequal.pdf:application/pdf}
}

@article{guggenberger_impact_2010,
	title = {{THE} {IMPACT} {OF} {A} {HAUSMAN} {PRETEST} {ON} {THE} {ASYMPTOTIC} {SIZE} {OF} {A} {HYPOTHESIS} {TEST}},
	volume = {26},
	issn = {0266-4666},
	url = {https://www.jstor.org/stable/40664469},
	abstract = {This paper investigates the asymptotic size properties of a two-stage test in the linear instrumental variables model when in the first stage a Hausman (1978) specification test is used as a pretest of exogeneity of a regressor. In the second stage, a simple hypothesis about a component of the structural parameter vector is tested, using a t-statistic that is based on either the ordinary least squares (OLS) or the twostage least squares estimator (2SLS), depending on the outcome of the Hausman pretest. The asymptotic size of the two-stage test is derived in a model where weak instruments are ruled out by imposing a positive lower bound on the strength of the instruments. The asymptotic size equals 1 for empirically relevant choices of the parameter space. The size distortion is caused by a discontinuity of the asymptotic distribution of the test statistic in the correlation parameter between the structural and reduced form error terms. The Hausman pretest does not have sufficient power against correlations that are local to zero while the OLS-based t-statistic takes on large values for such nonzero correlations. Instead of using the two-stage procedure, the recommendation then is to use a t-statistic based on the 2SLS estimator or, if weak instruments are a concern, the conditional likelihood ratio test by Moreira (2003).},
	number = {2},
	urldate = {2019-07-24},
	journal = {Econometric Theory},
	author = {Guggenberger, Patrik},
	year = {2010},
	pages = {369--382}
}

@article{pratt_length_1961,
	title = {Length of {Confidence} {Intervals}},
	volume = {56},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1961.10480644},
	doi = {10.1080/01621459.1961.10480644},
	abstract = {The expected length of a confidence interval is shown to equal the integral over false values of the probability each false value is included. Thus two desiderata for choosing among confidence procedures lead to the same measure of desirability. Furthermore, by common definitions of “optimum,” a procedure is optimum as regards including false values if and only if it is optimum as regards expected length. However, the procedure with minimum expected length ordinarily depends on the true value of the parameter. The possibility is explored of minimizing the average expected length, averaging according to some weighting on the possible parameter values. (This is not the same as assuming a prior distribution and using Bayes' Theorem.) The ideas are applied to the mean and variance of a normal distribution and the probability of success in binomial trials.},
	number = {295},
	urldate = {2019-07-29},
	journal = {Journal of the American Statistical Association},
	author = {Pratt, John W.},
	month = sep,
	year = {1961},
	pages = {549--567},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/5WLK699W/01621459.1961.html:text/html}
}

@book{mau_nam_convex_2019,
	title = {Convex {Analysis}: {An} introduction to convexity and nonsmooth analysis},
	url = {https://maunamn.wordpress.com/},
	author = {Mau Nam, Nguyen},
	year = {2019}
}

@article{ginther_alternative_2000,
	title = {Alternative {Estimates} of the {Effect} of {Schooling} on {Earnings}},
	volume = {82},
	issn = {0034-6535},
	url = {https://doi.org/10.1162/003465300558542},
	doi = {10.1162/003465300558542},
	abstract = {This paper examines how assumptions imposed on the data influence estimates of schooling's effect on earnings. The paper models schooling decisions as treatment effects and imposes assumptions about schooling selection to estimate bounds on the treatment effect. The study begins by using the worst-case bounds derived by Manski (1989, 1990, 1994, 1995) and adds assumptions from the Roy model of schooling self-selection to narrow the bounds on the schooling treatment effect. The bounds are narrowed further by using family structure, college proximity, and school-quality characteristics as exclusion restrictions. The selection problem requires the researcher to make explicit assumptions to estimate the effect of schooling on earnings. This paper demonstrates that different selection assumptions yield very different results.},
	number = {1},
	urldate = {2019-08-01},
	journal = {The Review of Economics and Statistics},
	author = {Ginther, Donna K.},
	month = feb,
	year = {2000},
	pages = {103--116},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/YYH5BXHA/003465300558542.html:text/html}
}

@article{Rubin1974,
author = {Rubin, Donald B.},
journal = {Journal of Educational Psychology},
number = {5},
pages = {688--70},
title = {{Estimating causal effects of treatments in randomized and nonrandomized studies}},
volume = {66},
year = {1974}
}

@article{Wooldridge2003,
author = {Wooldridge, Jeffrey M},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wooldridge - 2003 - Cluster-Sample Methods in Applied Econometrics.pdf:pdf},
journal = {American Economic Review P{\&}P},
number = {2},
pages = {133--138},
title = {{Cluster-Sample Methods in Applied Econometrics}},
volume = {93},
year = {2003}
}
@article{Robins1986,
author = {Robins, J. M.},
doi = {10.1016/0898-1221(87)90238-0},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Robins - 1986 - A New Approach To Causal Inference in Mortality Studies With a Sustained Exposure Period - Application To Control of the.pdf:pdf},
issn = {00974943},
journal = {Mathematical Modelling},
pages = {1393--1512},
title = {{A New Approach To Causal Inference in Mortality Studies With a Sustained Exposure Period - Application To Control of the Healthy Worker Survivor Effect}},
volume = {7},
year = {1986}
}
@article{Abbring2003,
author = {Abbring, Jaap H. and van den Berg, Gerard J.},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abbring, van den Berg - 2003 - The nonparametric identification of treatment effects in duration models.pdf:pdf},
journal = {Econometrica},
number = {5},
pages = {1491--1517},
title = {{The nonparametric identification of treatment effects in duration models}},
volume = {71},
year = {2003}
}
@article{manski_nonparametric_1990,
	title = {Nonparametric {Bounds} on {Treatment} {Effects}},
	volume = {80},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/2006592},
	number = {2},
	urldate = {2019-08-01},
	journal = {The American Economic Review},
	author = {Manski, Charles F.},
	year = {1990},
	pages = {319--323}
}

@misc{noauthor_local_nodate,
	title = {Local instrumental variables and latent variable models for identifying and bounding treatment effects {\textbar} {PNAS}},
	url = {https://www.pnas.org/content/96/8/4730},
	urldate = {2019-08-01},
	file = {Local instrumental variables and latent variable models for identifying and bounding treatment effects | PNAS:/Users/jonathanroth/Zotero/storage/Q4HGN8RE/4730.html:text/html}
}

@article{heckman_local_1999,
	title = {Local instrumental variables and latent variable models for identifying and bounding treatment effects},
	volume = {96},
	url = {http://www.pnas.org/content/96/8/4730.abstract},
	doi = {10.1073/pnas.96.8.4730},
	abstract = {This paper examines the relationship between various treatment parameters within a latent variable model when the effects of treatment depend on the recipient’s observed and unobserved characteristics. We show how this relationship can be used to identify the treatment parameters when they are identified and to bound the parameters when they are not identified. LIV,local instrumental variable;ATE,average treatment effect;TT,effect of treatment on the treated;LATE,local ATE},
	number = {8},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Heckman, James J. and Vytlacil, Edward J.},
	month = apr,
	year = {1999},
	pages = {4730}
}

@article{manski_bounding_1998,
	title = {Bounding {Disagreements} about {Treatment} {Effects}: {A} {Case} {Study} of {Sentencing} and {Recidivism}},
	volume = {28},
	issn = {0081-1750},
	shorttitle = {3. {Bounding} {Disagreements} about {Treatment} {Effects}},
	url = {https://doi.org/10.1111/0081-1750.00043},
	doi = {10.1111/0081-1750.00043},
	abstract = {Empirical inference on treatment effects is a core objective of social science research. The conventional practice is to obtain point estimates of treatment effects using models that make strong and thereby controversial assumptions about treatment selection and outcomes. In this paper we obtain bounds under weak nonparametric assumptions and explore how the bounds vary with the assumptions imposed. This mode of analysis clarifies the source of common disagreements about the magnitudes and signs of treatment effects. We use a treatment question facing the juvenile justice system to showcase the value of the approach in empirical social science research. We compare the impacts on recidivism of the two main sentencing options available to judges: confinement in residential treatment facilities and diversion to nonresidential treatment.},
	language = {en},
	number = {1},
	urldate = {2019-08-01},
	journal = {Sociological Methodology},
	author = {Manski, Charles F. and Nagin, Daniel S.},
	month = aug,
	year = {1998},
	pages = {99--137}
}

@article{manski_monotone_2000,
	title = {Monotone {Instrumental} {Variables}: {With} an {Application} to the {Returns} to {Schooling}},
	volume = {68},
	copyright = {Econometric Society 2000},
	issn = {1468-0262},
	shorttitle = {Monotone {Instrumental} {Variables}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1468-0262.t01-1-00144a},
	doi = {10.1111/1468-0262.t01-1-00144a},
	language = {en},
	number = {4},
	urldate = {2019-08-01},
	journal = {Econometrica},
	author = {Manski, Charles F. and Pepper, John V.},
	year = {2000},
	pages = {997--1010},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/GNCWWKHN/1468-0262.html:text/html}
}

@article{bhattacharya_treatment_2008,
	title = {Treatment {Effect} {Bounds} under {Monotonicity} {Assumptions}: {An} {Application} to {Swan}-{Ganz} {Catheterization}},
	volume = {98},
	issn = {0002-8282},
	shorttitle = {Treatment {Effect} {Bounds} under {Monotonicity} {Assumptions}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.98.2.351},
	doi = {10.1257/aer.98.2.351},
	language = {en},
	number = {2},
	urldate = {2019-08-01},
	journal = {American Economic Review},
	author = {Bhattacharya, Jay and Shaikh, Azeem M. and Vytlacil, Edward},
	month = may,
	year = {2008},
	keywords = {Single Equation Models, Single Variables: Cross-Sectional Models, Spatial Models, Treatment Effect Models, Health: General},
	pages = {351--356},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/2ICKWQ4F/articles.html:text/html}
}

@article{kreider_identifying_2012,
	title = {Identifying the {Effects} of {SNAP} ({Food} {Stamps}) on {Child} {Health} {Outcomes} {When} {Participation} {Is} {Endogenous} and {Misreported}},
	volume = {107},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2012.682828},
	doi = {10.1080/01621459.2012.682828},
	abstract = {The literature assessing the efficacy of the Supplemental Nutrition Assistance Program (SNAP), formerly known as the Food Stamp Program, has long puzzled over positive associations between SNAP receipt and various undesirable health outcomes such as food insecurity. Assessing the causal impacts of SNAP, however, is hampered by two key identification problems: endogenous selection into participation and extensive systematic underreporting of participation status. Using data from the National Health and Nutrition Examination Survey (NHANES), we extend partial identification bounding methods to account for these two identification problems in a single unifying framework. Specifically, we derive informative bounds on the average treatment effect (ATE) of SNAP on child food insecurity, poor general health, obesity, and anemia across a range of different assumptions used to address the selection and classification error problems. In particular, to address the selection problem, we apply relatively weak nonparametric assumptions on the latent outcomes, selected treatments, and observed covariates. To address the classification error problem, we formalize a new approach that uses auxiliary administrative data on the size of the SNAP caseload to restrict the magnitudes and patterns of SNAP reporting errors. Layering successively stronger assumptions, an objective of our analysis is to make transparent how the strength of the conclusions varies with the strength of the identifying assumptions. Under the weakest restrictions, there is substantial ambiguity; we cannot rule out the possibility that SNAP increases or decreases poor health. Under stronger but plausible assumptions used to address the selection and classification error problems, we find that commonly cited relationships between SNAP and poor health outcomes provide a misleading picture about the true impacts of the program. Our tightest bounds identify favorable impacts of SNAP on child health.},
	number = {499},
	urldate = {2019-08-01},
	journal = {Journal of the American Statistical Association},
	author = {Kreider, Brent and Pepper, John V. and Gundersen, Craig and Jolliffe, Dean},
	month = sep,
	year = {2012},
	keywords = {Food insecurity, Food Stamp Program, Health outcomes, Nonclassical measurement error, Nonparametric bounds, Partial identification, Supplemental Nutrition Assistance Program, Treatment effect},
	pages = {958--975},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/6SK4NJIR/01621459.2012.html:text/html;Submitted Version:/Users/jonathanroth/Zotero/storage/TQWYBFT5/Kreider et al. - 2012 - Identifying the Effects of SNAP (Food Stamps) on C.pdf:application/pdf}
}

@article{gonzalez_nonparametric_2005,
	title = {Nonparametric bounds on the returns to language skills},
	volume = {20},
	copyright = {Copyright © 2005 John Wiley \& Sons, Ltd.},
	issn = {1099-1255},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.795},
	doi = {10.1002/jae.795},
	abstract = {This paper applies the theoretical literature on nonparametric bounds on treatment effects to the estimation of how limited English proficiency (LEP) affects wages and employment opportunities for Hispanic workers in the United States. I analyse the identifying power of several weak assumptions on treatment response and selection, and stress the interactions between LEP and education, occupation and immigration status. I show that the combination of two weak but credible assumptions provides informative upper bounds on the returns to language skills for certain subgroups of the population. Adding age at arrival as a monotone instrumental variable also provides informative lower bounds. Copyright © 2005 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {6},
	urldate = {2019-08-01},
	journal = {Journal of Applied Econometrics},
	author = {González, Libertad},
	year = {2005},
	pages = {771--795},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/4BQGI26J/jae.html:text/html;Submitted Version:/Users/jonathanroth/Zotero/storage/QF6SLM6Q/González - 2005 - Nonparametric bounds on the returns to language sk.pdf:application/pdf}
}

@article{gafarov_inference_2019,
	title = {Inference in high-dimensional set-identified affine models},
	url = {https://arxiv.org/abs/1904.00111v1},
	abstract = {This paper proposes both point-wise and uniform confidence sets (CS) for an
element \$θ\_\{1\}\$ of a parameter vector \$θ{\textbackslash}in{\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\$ that is
partially identified by affine moment equality and inequality conditions. The
method is based on an estimator of a regularized support function of the
identified set. This estimator is {\textbackslash}emph\{half-median unbiased\} and has an
{\textbackslash}emph\{asymptotic linear representation\} which provides closed form standard
errors and enables optimization-free multiplier bootstrap. The proposed CS can
be computed as a solution to a finite number of linear and convex quadratic
programs, which leads to a substantial decrease in {\textbackslash}emph\{computation time\} and
{\textbackslash}emph\{guarantee of global optimum\}. As a result, the method provides uniformly
valid inference in applications with the dimension of the parameter space, \$d\$,
and the number of inequalities, \$k\$, that were previously computationally
unfeasible (\$d,k {\textgreater}100\$). The proposed approach is then extended to construct
polygon-shaped joint CS for multiple components of \$θ\$. Inference for
coefficients in the linear IV regression model with interval outcome is used as
an illustrative example.
  Key Words: Affine moment inequalities; Asymptotic linear representation;
Delta{\textbackslash}textendash Method; Interval data; Intersection bounds; Partial
identification; Regularization; Strong approximation; Stochastic Programming;
Subvector inference; Uniform inference.},
	language = {en},
	urldate = {2019-09-03},
	author = {Gafarov, Bulat},
	month = mar,
	year = {2019},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/2A6489HK/Gafarov - 2019 - Inference in high-dimensional set-identified affin.pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/FH9SUMMB/1904.html:text/html}
}

@article{cho_simple_2018,
	title = {Simple {Inference} on {Functionals} of {Set}-{Identified} {Parameters} {Defined} by {Linear} {Moments}},
	url = {https://arxiv.org/abs/1810.03180v6},
	abstract = {This paper considers uniformly valid (over a class of data generating
processes) inference for linear functionals of partially identified parameters
in cases where the identified set is defined by linear (in the parameter)
moment inequalities. We propose a bootstrap procedure for constructing
uniformly valid confidence sets for a linear functional of a partially
identified parameter. The proposed method amounts to bootstrapping the value
functions of a linear optimization problem, and subsumes subvector inference as
a special case. In other words, this paper shows the conditions under which
naively bootstrapping a linear program can be used to construct a confidence
set with uniform correct coverage for a partially identified linear functional.
Unlike other proposed subvector inference procedures, our procedure does not
require the researcher to repeatedly invert a hypothesis test, and is extremely
computationally efficient.},
	language = {en},
	urldate = {2019-09-03},
	author = {Cho, JoonHwan and Russell, Thomas M.},
	month = oct,
	year = {2018},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/KA5HVCLU/Cho and Russell - 2018 - Simple Inference on Functionals of Set-Identified .pdf:application/pdf;Snapshot:/Users/jonathanroth/Zotero/storage/4BFLQ83U/1810.html:text/html}
}


@article{liang_longitudinal_1986,
	title = {Longitudinal data analysis using generalized linear models},
	volume = {73},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/73/1/13/246001},
	doi = {10.1093/biomet/73.1.13},
	abstract = {Abstract.  This paper proposes an extension of generalized linear models to the analysis of longitudinal data. We introduce a class of estimating equations that},
	language = {en},
	number = {1},
	urldate = {2020-03-10},
	journal = {Biometrika},
	author = {Liang, Kung-Yee and Zeger, Scott L.},
	year = {1986},
	pages = {13--22}
}




@article{arellano_practitioners_1987,
	title = {{Practitioners}’ {Corner}: {Computing} {Robust} {Standard} {Errors} for {Within}-groups {Estimators}},
	volume = {49},
	copyright = {© 1987 Blackwell Publishing Ltd},
	issn = {1468-0084},
	shorttitle = {{PRACTITIONERS}’ {CORNER}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0084.1987.mp49004006.x},
	doi = {10.1111/j.1468-0084.1987.mp49004006.x},
	language = {en},
	number = {4},
	urldate = {2020-03-10},
	journal = {Oxford Bulletin of Economics and Statistics},
	author = {Arellano, M.},
	year = {1987},
	pages = {431--434},
	file = {Snapshot:/Users/jonathanroth/Zotero/storage/CWBLYGPG/j.1468-0084.1987.mp49004006.html:text/html}
}

@article{kezdi_robust_2004,
	title = {Robust {Standard} {Error} {Estimation} in {Fixed}-{Effects} {Panel} {Models}},
	volume = {9},
	issn = {1556-5068},
	url = {http://www.ssrn.com/abstract=596988},
	doi = {10.2139/ssrn.596988},
	language = {en},
	urldate = {2020-03-10},
	journal = {Hungarian Statistical Review},
	author = {Kezdi, Gabor},
	year = {2004},
	pages = {95--116},
	file = {Kezdi - 2003 - Robust Standard Error Estimation in Fixed-Effects .pdf:/Users/jonathanroth/Zotero/storage/K2RVA2C5/Kezdi - 2003 - Robust Standard Error Estimation in Fixed-Effects .pdf:application/pdf}
}


@article{malani_interpreting_2015,
	title = {Interpreting pre-trends as anticipation: {Impact} on estimated treatment effects from tort reform},
	volume = {124},
	issn = {0047-2727},
	shorttitle = {Interpreting pre-trends as anticipation},
	url = {http://www.sciencedirect.com/science/article/pii/S0047272715000122},
	doi = {10.1016/j.jpubeco.2015.01.001},
	abstract = {While conducting empirical work, researchers sometimes observe changes in outcomes before adoption of a new policy. The conventional diagnosis is that treatment is endogenous. This observation is also consistent, however, with anticipation effects that arise naturally out of many theoretical models. This paper illustrates that distinguishing endogeneity from anticipation matters greatly when estimating treatment effects. It provides a framework for comparing different methods for estimating anticipation effects and proposes a new set of instrumental variables to address the problem that subjects' expectations are unobservable. Finally, this paper examines a specific set of tort reforms that was not targeted at physicians but was likely anticipated by them. Interpreting pre-trends as evidence of anticipation increases the estimated effect of these reforms by a factor of two compared to a model that ignores anticipation.},
	language = {en},
	urldate = {2019-10-25},
	journal = {Journal of Public Economics},
	author = {Malani, Anup and Reif, Julian},
	year = {2015},
	keywords = {Anticipation, Endogeneity, Medical malpractice, Tort reform},
	pages = {1--17},
	file = {ScienceDirect Snapshot:/Users/jonathanroth/Zotero/storage/KFLCEJT9/S0047272715000122.html:text/html}
}

@article{ImaiKim(19),
    title={On the Use of Two-way Fixed Effects Regression Models for Causal Inference with Panel Data},
    author={Kosuke Imai and In Song Kim},
    journal = {Political Analysis},
    number = {3},
    volume = {29},
    pages = {405--415},
    year={2021}
}


@article{holland_statistics_1986,
	title = {Statistics and {Causal} {Inference}},
	volume = {81},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/2289064},
	doi = {10.2307/2289064},
	abstract = {Problems involving causal inference have dogged at the heels of statistics since its earliest days. Correlation does not imply causation, and yet causal conclusions drawn from a carefully designed experiment are often valid. What can a statistical model say about causation? This question is addressed by using a particular model for causal inference (Holland and Rubin 1983; Rubin 1974) to critique the discussions of other writers on causation and causal inference. These include selected philosophers, medical researchers, statisticians, econometricians, and proponents of causal modeling.},
	number = {396},
	urldate = {2020-04-07},
	journal = {Journal of the American Statistical Association},
	author = {Holland, Paul W.},
	year = {1986},
	pages = {945--960}
}

@article{strezhnev2018semiparametric,
  title={Semiparametric weighting estimators for
multi-period difference-in-differences designs},
  author={Strezhnev, Anton},
  year={2018},
  journal = {Working Paper}
}


@article{cengiz_effect_2019,
	title = {The {Effect} of {Minimum} {Wages} on {Low}-{Wage} {Jobs}},
	volume = {134},
	issn = {0033-5533},
	url = {https://academic.oup.com/qje/article/134/3/1405/5484905},
	doi = {10.1093/qje/qjz014},
	abstract = {Abstract.  We estimate the effect of minimum wages on low-wage jobs using 138 prominent state-level minimum wage changes between 1979 and 2016 in the United Sta},
	language = {en},
	number = {3},
	urldate = {2020-04-07},
	journal = {The Quarterly Journal of Economics},
	author = {Cengiz, Doruk and Dube, Arindrajit and Lindner, Attila and Zipperer, Ben},
	year = {2019},
	pages = {1405--1454},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/8UA7AVI4/Cengiz et al. - 2019 - The Effect of Minimum Wages on Low-Wage Jobs.pdf:application/pdf}
}


@Article{bonhomme2011recovering,
  author    = {Bonhomme, St{\'e}phane and Sauder, Ulrich},
  journal   = {Review of Economics and Statistics},
  title     = {Recovering distributions in difference-in-differences models: A comparison of selective and comprehensive schooling},
  year      = {2011},
  number    = {2},
  pages     = {479--494},
  volume    = {93},
  publisher = {MIT Press},
}

@Article{chernozhukov2013inference,
  author    = {Chernozhukov, Victor and Fern{\'a}ndez-Val, Iv{\'a}n and Melly, Blaise},
  journal   = {Econometrica},
  title     = {Inference on counterfactual distributions},
  year      = {2013},
  number    = {6},
  pages     = {2205--2268},
  volume    = {81},
  publisher = {Wiley Online Library},
}

@Article{melly2015changes,
  author  = {Melly, Blaise and Santangelo, Giulia},
  journal = {Universit{\"a}t Bern, Bern},
  title   = {The changes-in-changes model with covariates},
  year    = {2015},
}

@Article{glynn2019generalized,
  author  = {Glynn, Adam and Ichino, Nahomi},
  journal = {V-Dem Working Paper},
  title   = {Generalized nonlinear difference-in-difference-in-differences},
  year    = {2019},
  volume  = {90},
}

@Article{crest2018penalized,
title = {A {Penalized} {Synthetic} {Control} {Estimator} for {Disaggregated} {Data}},
	volume = {116},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2021.1971535},
	doi = {10.1080/01621459.2021.1971535},
	abstract = {Synthetic control methods are commonly applied in empirical research to estimate the effects of treatments or interventions on aggregate outcomes. A synthetic control estimator compares the outcome of a treated unit to the outcome of a weighted average of untreated units that best resembles the characteristics of the treated unit before the intervention. When disaggregated data are available, constructing separate synthetic controls for each treated unit may help avoid interpolation biases. However, the problem of finding a synthetic control that best reproduces the characteristics of a treated unit may not have a unique solution. Multiplicity of solutions is a particularly daunting challenge when the data include many treated and untreated units. To address this challenge, we propose a synthetic control estimator that penalizes the pairwise discrepancies between the characteristics of the treated units and the characteristics of the units that contribute to their synthetic controls. The penalization parameter trades off pairwise matching discrepancies with respect to the characteristics of each unit in the synthetic control against matching discrepancies with respect to the characteristics of the synthetic control unit as a whole. We study the properties of this estimator and propose data-driven choices of the penalization parameter.},
	number = {536},
	urldate = {2021-12-27},
	journal = {Journal of the American Statistical Association},
	author = {Abadie, Alberto and L’Hour, Jérémy},
	month = oct,
	year = {2021},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2021.1971535},
	keywords = {Causal inference, Observational studies, Treatment effects},
	pages = {1817--1834},
	file = {Snapshot:/Users/alyssabilinski/Zotero/storage/B3KUI675/01621459.2021.html:text/html}
}

@Article{stuart2014using,
  author    = {Stuart, Elizabeth A and Huskamp, Haiden A and Duckworth, Kenneth and Simmons, Jeffrey and Song, Zirui and Chernew, Michael E and Barry, Colleen L},
  journal   = {Health Services and Outcomes Research Methodology},
  title     = {Using propensity scores in difference-in-differences models to estimate the effects of a policy change},
  year      = {2014},
  number    = {4},
  pages     = {166--182},
  volume    = {14},
  publisher = {Springer},
}


@techreport{card_minimum_1993,
	address = {Cambridge, MA},
	title = {Minimum {Wages} and {Employment}: {A} {Case} {Study} of the {Fast} {Food} {Industry} in {New} {Jersey} and {Pennsylvania}},
	shorttitle = {Minimum {Wages} and {Employment}},
	url = {http://www.nber.org/papers/w4509.pdf},
	language = {en},
	number = {w4509},
	urldate = {2021-07-15},
	institution = {National Bureau of Economic Research},
	author = {Card, David and Krueger, Alan},
	month = oct,
	year = {1993},
	doi = {10.3386/w4509},
	pages = {w4509},
	file = {Card and Krueger - 1993 - Minimum Wages and Employment A Case Study of the .pdf:/Users/abilinski/Zotero/storage/DKKUBR9F/Card and Krueger - 1993 - Minimum Wages and Employment A Case Study of the .pdf:application/pdf},
}

@article{card_minimum_2000,
	title = {Minimum {Wages} and {Employment}: {A} {Case} {Study} of the {Fast}-{Food} {Industry} in {New} {Jersey} and {Pennsylvania}: {Reply}},
	volume = {90},
	issn = {0002-8282},
	shorttitle = {Minimum {Wages} and {Employment}},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.90.5.1397},
	doi = {10.1257/aer.90.5.1397},
	language = {en},
	number = {5},
	urldate = {2021-07-15},
	journal = {American Economic Review},
	author = {Card, David and Krueger, Alan B.},
	month = dec,
	year = {2000},
	keywords = {and Labor Costs: Public Policy, Compensation, Labor Demand, Wages},
	pages = {1397--1420},
	file = {Snapshot:/Users/abilinski/Zotero/storage/UL3KKCW7/articles.html:text/html},
}


@article{bilinski_no_2018,
	title = {No {Free} {Lunch}: {A} non-inferiority approach to model assumption tests},
	shorttitle = {Nothing to see here?},
	url = {http://arxiv.org/abs/1805.03273},
	abstract = {Many causal models make assumptions of "no difference" or "no effect." For example, difference-in-differences (DID) assumes that there is no trend difference between treatment and comparison groups' untreated potential outcomes ("parallel trends"). Tests of these assumptions typically assume a null hypothesis that there is no violation. When researchers fail to reject the null, they consider the assumption to hold. We argue this approach is incorrect and frequently misleading. These tests reverse the roles of Type I and Type II error and have a high probability of missing assumption violations. Even when power is high, they may detect statistically significant violations too small to be of practical importance. We present test reformulations in a non-inferiority framework that rule out violations of model assumptions that exceed some threshold. We then focus on the parallel trends assumption, for which we propose a "one step up" method: 1) reporting treatment effect estimates from a model with a more complex trend difference than is believed to be the case and 2) testing that that the estimated treatment effect falls within a specified distance of the treatment effect from the simpler model. We show that this reduces bias while also considering power, controlling mean-squared error. Our base model also aligns power to detect a treatment effect with power to rule out meaningful violations of parallel trends. We apply our approach to 4 data sets used to analyze the Affordable Care Act's dependent coverage mandate and demonstrate that coverage gains may have been smaller than previously estimated.},
	urldate = {2021-07-16},
	journal = {arXiv:1805.03273 [stat]},
	author = {Bilinski, Alyssa and Hatfield, Laura A.},
	month =apr,
	year = {2018},
	keywords = {Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/abilinski/Zotero/storage/FT6H3BGX/Bilinski and Hatfield - 2020 - Nothing to see here Non-inferiority approaches to.pdf:application/pdf;arXiv.org Snapshot:/Users/abilinski/Zotero/storage/ABH326VR/1805.html:text/html},
}

@article{freyaldenhoven_pre-event_2019,
	title = {Pre-event {Trends} in the {Panel} {Event}-{Study} {Design}},
	volume = {109},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20180609},
	doi = {10.1257/aer.20180609},
	abstract = {We consider a linear panel event-study design in which unobserved confounds may be related both to the outcome and to the policy variable of interest. We provide sufficient conditions to identify the causal effect of the policy by exploiting covariates related to the policy only through the confounds. Our model implies a set of moment equations that are linear in parameters. The effect of the policy can be estimated by 2SLS, and causal inference is valid even when endogeneity leads to pre-event trends ("pre-trends") in the outcome. Alternative approaches perform poorly in our simulations.},
	language = {en},
	number = {9},
	urldate = {2021-07-16},
	journal = {American Economic Review},
	author = {Freyaldenhoven, Simon and Hansen, Christian and Shapiro, Jesse M.},
	year = {2019},
	keywords = {Single Equation Models, Single Variables: Panel Data Models, Spatio-temporal Models, Single Equation Models: Single Variables: Instrumental Variables (IV) Estimation},
	pages = {3307--3338},
	file = {Full Text:/Users/abilinski/Zotero/storage/L3G8ISSY/Freyaldenhoven et al. - 2019 - Pre-event Trends in the Panel Event-Study Design.pdf:application/pdf;Snapshot:/Users/abilinski/Zotero/storage/Z5PLMIFQ/articles.html:text/html},
}

@article{kahn-lang_promise_2020,
	title = {The {Promise} and {Pitfalls} of {Differences}-in-{Differences}: {Reflections} on 16 and {Pregnant} and {Other} {Applications}},
	volume = {38},
	issn = {0735-0015},
	shorttitle = {The {Promise} and {Pitfalls} of {Differences}-in-{Differences}},
	url = {https://doi.org/10.1080/07350015.2018.1546591},
	doi = {10.1080/07350015.2018.1546591},
	abstract = {We use the exchange between Kearney/Levine and Jaeger/Joyce/Kaestner on 16 and Pregnant to reexamine the use of DiD as a response to the failure of nature to properly design an experiment for us. We argue that (1) any DiD paper should address why the original levels of the experimental and control groups differed, and why this would not impact trends, (2) the parallel trends argument requires a justification of the chosen functional form and that the use of the interaction coefficients in probit and logit may be justified in some cases, and (3) parallel trends in the period prior to treatment is suggestive of counterfactual parallel trends, but parallel pre-trends is neither necessary nor sufficient for the parallel counterfactual trends condition to hold. Importantly, the purely statistical approach uses pretesting and thus, generates the wrong standard errors. Moreover, we underline the dangers of implicitly or explicitly accepting the null hypothesis when failing to reject the absence of a differential pre-trend.},
	number = {3},
	urldate = {2021-07-16},
	journal = {Journal of Business \& Economic Statistics},
	author = {Kahn-Lang, Ariella and Lang, Kevin},

	year = {2020},
	pages = {613--620},
	file = {Full Text:/Users/abilinski/Zotero/storage/QNJMWAMS/Kahn-Lang and Lang - 2020 - The Promise and Pitfalls of Differences-in-Differe.pdf:application/pdf;Snapshot:/Users/abilinski/Zotero/storage/66ZXURKC/07350015.2018.html:text/html},
}

@article{roth_pre-test_2021,
	title = {Pre-test with {Caution}: {Event}-study {Estimates} {After} {Testing} for {Parallel} {Trends}},
	year = {2022},
	author = {Roth, Jonathan},
	journal = {American Economic Review: Insights},
	volume = {4},
	number = {3},
	pages = {305--322}
}

@article{wickham_graphical_2010,
	title = {Graphical inference for infovis},
	volume = {16},
	issn = {1077-2626},
	url = {http://ieeexplore.ieee.org/document/5613434/},
	doi = {10.1109/TVCG.2010.161},
	abstract = {How do we know if what we see is really there? When visualizing data, how do we avoid falling into the trap of apophenia where we see patterns in random noise? Traditionally, infovis has been concerned with discovering new relationships, and statistics with preventing spurious relationships from being reported. We pull these opposing poles closer with two new techniques for rigorous statistical inference of visual discoveries. The “Rorschach” helps the analyst calibrate their understanding of uncertainty and the “lineup” provides a protocol for assessing the signiﬁcance of visual discoveries, protecting against the discovery of spurious structure.},
	language = {en},
	number = {6},
	urldate = {2021-07-16},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Wickham, Hadley and Cook, Dianne and Hofmann, Heike and Buja, Andreas},
	month = nov,
	year = {2010},
	pages = {973--979},
	file = {Wickham et al. - 2010 - Graphical inference for infovis.pdf:/Users/abilinski/Zotero/storage/JE3KJ87J/Wickham et al. - 2010 - Graphical inference for infovis.pdf:application/pdf},
}

@article{hasegawa_evaluating_2019,
	title = {Evaluating {Missouri}’s {Handgun} {Purchaser} {Law}: {A} {Bracketing} {Method} for {Addressing} {Concerns} {About} {History} {Interacting} with {Group}},
	volume = {30},
	issn = {1044-3983},
	number = {3},
	urldate = {2021-09-23},
	journal = {Epidemiology},
	author = {Hasegawa, Raiden B. and Webster, Daniel W. and Small, Dylan S.},
	year = {2019},
	pages = {371--379},
	file = {Snapshot:/Users/alyssabilinski/Zotero/storage/FWS4A3AE/Evaluating_Missouri_s_Handgun_Purchaser_Law__A.11.html:text/html},
}

@article{ye_negative_2021,
	title = {A {Negative} {Correlation} {Strategy} for {Bracketing} in {Difference}-in-{Differences}},
	url = {http://arxiv.org/abs/2006.02423},
	abstract = {The method of difference-in-differences (DID) is widely used to study the causal effect of policy interventions in observational studies. DID employs a before and after comparison of the treated and control units to remove bias due to time-invariant unmeasured confounders under the parallel trends assumption. Estimates from DID, however, will be biased if the outcomes for the treated and control units evolve differently in the absence of treatment, namely if the parallel trends assumption is violated. We propose a general identification strategy that leverages two groups of control units whose outcomes relative to the treated units exhibit a negative correlation, and achieves partial identification of the average treatment effect for the treated. The identified set is of a union bounds form that involves the minimum and maximum operators, which makes the canonical bootstrap generally inconsistent and naive methods overly conservative. By utilizing the directional inconsistency of the bootstrap distribution, we develop a novel bootstrap method to construct uniformly valid confidence intervals for the identified set and parameter of interest when the identified set is of a union bounds form, and we establish the method's theoretical properties. We develop a simple falsification test and sensitivity analysis. We apply the proposed strategy for bracketing to study whether minimum wage laws affect employment levels.},
	urldate = {2021-09-24},
	journal = {arXiv:2006.02423 [econ, stat]},
	author = {Ye, Ting and Keele, Luke and Hasegawa, Raiden and Small, Dylan S.},
	year = {2021},
	keywords = {Economics - Econometrics, Statistics - Applications, Statistics - Methodology},
	file = {arXiv Fulltext PDF:/Users/alyssabilinski/Zotero/storage/MDK66SE8/Ye et al. - 2021 - A Negative Correlation Strategy for Bracketing in .pdf:application/pdf;arXiv.org Snapshot:/Users/alyssabilinski/Zotero/storage/54WMFTXP/2006.html:text/html},
}


@article{keele_patterns_2019,
	title = {Patterns of {Effects} and {Sensitivity} {Analysis} for {Differences}-in-{Differences}},
	url = {http://arxiv.org/abs/1901.01869},
	abstract = {Applied analysts often use the differences-in-differences (DID) method to estimate the causal effect of policy interventions with observational data. The method is widely used, as the required before and after comparison of a treated and control group is commonly encountered in practice. DID removes bias from unobserved time-invariant confounders. While DID removes bias from time-invariant confounders, bias from time-varying confounders may be present. Hence, like any observational comparison, DID studies remain susceptible to bias from hidden confounders. Here, we develop a method of sensitivity analysis that allows investigators to quantify the amount of bias necessary to change a study's conclusions. Our method operates within a matched design that removes bias from observed baseline covariates. We develop methods for both binary and continuous outcomes. We then apply our methods to two different empirical examples from the social sciences. In the first application, we study the effect of changes to disability payments in Germany. In the second, we re-examine whether election day registration increased turnout in Wisconsin.},
	urldate = {2021-09-27},
	journal = {arXiv:1901.01869 [stat]},
	author = {Keele, Luke J. and Small, Dylan S. and Hsu, Jesse Y. and Fogarty, Colin B.},
	month = feb,
	year = {2019},
	note = {arXiv: 1901.01869},
	keywords = {Statistics - Applications},
	file = {arXiv Fulltext PDF:/Users/alyssabilinski/Zotero/storage/V3XRA39F/Keele et al. - 2019 - Patterns of Effects and Sensitivity Analysis for D.pdf:application/pdf;arXiv.org Snapshot:/Users/alyssabilinski/Zotero/storage/UV8L8YI4/1901.html:text/html},
}

@article{blackwelder_proving_1982,
	title = {"{Proving} the null hypothesis" in clinical trials},
	volume = {3},
	issn = {0197-2456},
	doi = {10.1016/0197-2456(82)90024-1},
	abstract = {When designing a clinical trial to show whether a new or experimental therapy is as effective as a standard therapy (but not necessarily more effective), the usual null hypothesis of equality is inappropriate and leads to logical difficulties. Since therapies cannot be shown to be literally equivalent, the appropriate null hypothesis is that the standard therapy is more effective than the experimental therapy by at least some specified amount. The problem is presented in terms of a trial in which the outcome of interest is dichotomous; test statistics, confidence intervals, and sample size calculations are discussed. The required sample size may be larger for either null hypothesis formulation than for the other, depending on the specific assumptions made. Reporting results in terms of confidence intervals is especially useful for this type of trial.},
	language = {eng},
	number = {4},
	journal = {Controlled Clinical Trials},
	author = {Blackwelder, W. C.},
	month = dec,
	year = {1982},
	pmid = {7160191},
	keywords = {Clinical Trials as Topic, Decision Theory, Humans, Random Allocation, Statistics as Topic, Therapeutics},
	pages = {345--353},
}


@book{wellek_testing_2010,
	address = {Boca Raton},
	edition = {2nd edition},
	title = {Testing {Statistical} {Hypotheses} of {Equivalence} and {Noninferiority}},
	isbn = {978-1-4398-0818-4},
	abstract = {While continuing to focus on methods of testing for two-sided equivalence, Testing Statistical Hypotheses of Equivalence and Noninferiority, Second Edition gives much more attention to noninferiority testing. It covers a spectrum of equivalence testing problems of both types, ranging from a one-sample problem with normally distributed observations of fixed known variance to problems involving several dependent or independent samples and multivariate data. Along with expanding the material on noninferiority problems, this edition includes new chapters on equivalence tests for multivariate data and tests for relevant differences between treatments. A majority of the computer programs offered online are now available not only in SAS or Fortran but also as R scripts or as shared objects that can be called within the R system.  This book provides readers with a rich repertoire of efficient solutions to specific equivalence and noninferiority testing problems frequently encountered in the analysis of real data sets. It first presents general approaches to problems of testing for noninferiority and two-sided equivalence. Each subsequent chapter then focuses on a specific procedure and its practical implementation. The last chapter describes basic theoretical results about tests for relevant differences as well as solutions for some specific settings often arising in practice. Drawing from real-life medical research, the author uses numerous examples throughout to illustrate the methods.},
	language = {English},
	publisher = {Chapman and Hall/CRC},
	author = {Wellek, Stefan},
	month = jun,
	year = {2010},
}

@article{hartman_equivalence_2018,
	title = {An {Equivalence} {Approach} to {Balance} and {Placebo} {Tests}},
	journal = {American Journal of Political Science},
	author = {Hartman, Erin and Hidalgo, F. Daniel},
	year={2018},
	doi={10.1111/ajps.12387},
	file = {Hartman and Hidalgo - An Equivalence Approach to Balance and Placebo Tes.pdf:/Users/abilinski/Zotero/storage/W9TSHA8H/Hartman and Hidalgo - An Equivalence Approach to Balance and Placebo Tes.pdf:application/pdf}
}

@techreport{liu_practical_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {A {Practical} {Guide} to {Counterfactual} {Estimators} for {Causal} {Inference} with {Time}-{Series} {Cross}-{Sectional} {Data}},
	url = {https://papers.ssrn.com/abstract=3555463},
	abstract = {This paper introduces a unified framework of counterfactual estimation for causal inference with time-series cross-sectional data, in which we estimate the average treatment effect on the treated by directly imputing counterfactual outcomes for treated observations. We discuss several novel estimators under this framework, including the fixed effects counterfactual estimator, interactive fixed effects counterfactual estimator, and matrix completion estimator. They provide more reliable causal estimates than conventional twoway fixed effects models when treatment effects are heterogeneous or unobserved time-varying confounders exist. Moreover, we propose a new dynamic treatment effects plot, along with several diagnostic tests, to help researchers gauge the validity of the identifying assumptions. We illustrate these methods  with two political economy examples and develop an open-source package, fect, in both R and Stata to facilitate implementation.},
	language = {en},
	number = {ID 3555463},
	urldate = {2021-10-11},
	institution = {Social Science Research Network},
	author = {Liu, Licheng and Wang, Ye and Xu, Yiqing},
	year = {2021},
	doi = {10.2139/ssrn.3555463}
}


@article{armstrong_optimal_2018,
	title = {Optimal {Inference} in a {Class} of {Regression} {Models}},
	volume = {86},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA14434},
	doi = {10.3982/ECTA14434},
	abstract = {We consider the problem of constructing confidence intervals (CIs) for a linear functional of a regression function, such as its value at a point, the regression discontinuity parameter, or a regression coefficient in a linear or partly linear regression. Our main assumption is that the regression function is known to lie in a convex function class, which covers most smoothness and/or shape assumptions used in econometrics. We derive finite-sample optimal CIs and sharp efficiency bounds under normal errors with known variance. We show that these results translate to uniform (over the function class) asymptotic results when the error distribution is not known. When the function class is centrosymmetric, these efficiency bounds imply that minimax CIs are close to efficient at smooth regression functions. This implies, in particular, that it is impossible to form CIs that are substantively tighter using data-dependent tuning parameters, and maintain coverage over the whole function class. We specialize our results to inference on the regression discontinuity parameter, and illustrate them in simulations and an empirical application.},
	language = {en},
	number = {2},
	urldate = {2021-12-27},
	journal = {Econometrica},
	author = {Armstrong, Timothy B. and Kolesár, Michal},
	year = {2018},
	keywords = {adaptation, efficiency bounds, finite-sample inference, Nonparametric inference, regression discontinuity},
	pages = {655--683},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/ATPCSYA2/Armstrong and Kolesár - 2018 - Optimal Inference in a Class of Regression Models.pdf:application/pdf},
}


@article{olea_simultaneous_2019,
	title = {Simultaneous confidence bands: {Theory}, implementation, and an application to {SVARs}},
	volume = {34},
	copyright = {© 2018 John Wiley \& Sons, Ltd.},
	issn = {1099-1255},
	shorttitle = {Simultaneous confidence bands},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.2656},
	doi = {https://doi.org/10.1002/jae.2656},
	abstract = {Simultaneous confidence bands are versatile tools for visualizing estimation uncertainty for parameter vectors, such as impulse response functions. In linear models, it is known that that the sup-t confidence band is narrower than commonly used alternatives—for example, Bonferroni and projection bands. We show that the same ranking applies asymptotically even in general nonlinear models, such as vector autoregressions (VARs). Moreover, we provide further justification for the sup-t band by showing that it is the optimal default choice when the researcher does not know the audience's preferences. Complementing existing plug-in and bootstrap implementations, we propose a computationally convenient Bayesian sup-t band with exact finite-sample simultaneous credibility. In an application to structural VAR impulse response function estimation, the sup-t band—which has been surprisingly overlooked in this setting—is at least 35\% narrower than other off-the-shelf simultaneous bands.},
	language = {en},
	number = {1},
	urldate = {2021-03-05},
	journal = {Journal of Applied Econometrics},
	author = {Olea, José Luis Montiel and Plagborg‐Møller, Mikkel},
	year = {2019},
	pages = {1--17},
}


@techreport{freyaldenhoven_visualization_2021,
	type = {Working {Paper}},
	title = {Visualization, {Identification}, and {Estimation} in the {Linear} {Panel} {Event}-{Study} {Design}},
	url = {https://www.nber.org/papers/w29170},
	abstract = {Linear panel models, and the “event-study plots” that often accompany them, are popular tools for learning about policy effects. We discuss the construction of event-study plots and suggest ways to make them more informative. We examine the economic content of different possible identifying assumptions. We explore the performance of the corresponding estimators in simulations, highlighting that a given estimator can perform well or poorly depending on the economic environment. An accompanying Stata package, xtevent, facilitates adoption of our suggestions.},
	number = {29170},
	urldate = {2021-12-27},
	institution = {National Bureau of Economic Research},
	author = {Freyaldenhoven, Simon and Hansen, Christian and Pérez Pérez, Jorge and Shapiro, Jesse M.},
	year = {2021},
	doi = {10.3386/w29170},
	file = {Full Text PDF:/Users/jonathanroth/Zotero/storage/JP6PZMQ6/Freyaldenhoven et al. - 2021 - Visualization, Identification, and Estimation in t.pdf:application/pdf},
}


@article{rambachan_honest_2021,
	title = {An {Honest} {Approach} to {Parallel} {Trends}},
	year = 2021,
	abstract = {This paper proposes tools for robust inference for diﬀerence-in-diﬀerences and eventstudy designs. Instead of requiring that the parallel trends assumption holds exactly, we impose that pre-treatment violations of parallel trends (“pre-trends”) are informative about the possible post-treatment violations of parallel trends. Such restrictions allow us to formalize the intuition behind the common practice of testing for pre-existing trends while avoiding issues related to pre-testing. The causal eﬀect of interest is partially identiﬁed under such restrictions. We introduce two approaches that guarantee uniformly valid (“honest”) inference under the imposed restrictions, and we derive novel results showing that they have good power properties in our context. We recommend that researchers conduct sensitivity analyses to show what conclusions can be drawn under various restrictions on the possible diﬀerences in trends.},
	language = {en},
	author = {Rambachan, Ashesh and Roth, Jonathan},
	journal = {Working paper},
	file = {Rambachan and Roth - An Honest Approach to Parallel Trends.pdf:/Users/alyssabilinski/Zotero/storage/VP9XHKIW/Rambachan and Roth - An Honest Approach to Parallel Trends.pdf:application/pdf},
}


@article{heckman_characterizing_1998,
	title = {Characterizing {Selection} {Bias} {Using} {Experimental} {Data}},
	volume = {66},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/2999630},
	doi = {10.2307/2999630},
	abstract = {Semiparametric methods are developed to estimate the bias that arises from using nonexperimental comparison groups to evaluate social programs and to test the identifying assumptions that justify matching, selection models, and the method of difference-in-differences. Using data from an experiment on a prototypical social program and data from nonexperimental comparison groups, we reject the assumptions justifying matching and our extensions of it. The evidence supports the selection bias model and the assumptions that justify a semiparametric version of the method of difference-in-differences. We extend our analysis to consider applications of the methods to ordinary observational data.},
	number = {5},
	urldate = {2021-12-23},
	journal = {Econometrica},
	author = {Heckman, James and Ichimura, Hidehiko and Smith, Jeffrey and Todd, Petra},
	year = {1998},
	pages = {1017--1098},
	file = {Submitted Version:/Users/alyssabilinski/Zotero/storage/TKC87ZMT/Heckman et al. - 1998 - Characterizing Selection Bias Using Experimental D.pdf:application/pdf},
}



@article{sun_estimating_2020,
	title = {Estimating dynamic treatment effects in event studies with heterogeneous treatment effects},
	doi = {10.1016/j.jeconom.2020.09.006},
	abstract = {To estimate the dynamic effects of an absorbing treatment, researchers often use two-way fixed effects regressions that include leads and lags of the treatment. We show that in settings with variation in treatment timing across units, the coefficient on a given lead or lag can be contaminated by effects from other periods, and apparent pretrends can arise solely from treatment effects heterogeneity. We propose an alternative estimator that is free of contamination, and illustrate the relative shortcomings of two-way fixed effects regressions with leads and lags through an empirical application.},
	language = {en},
	journal = {Journal of Econometrics},
	author = {Sun, Liyang and Abraham, Sarah},
	year = {2021},
	volume = {225},
	number = {2},
	pages = {175--199}
}


@article{santanna_doubly_2020,
	title = {Doubly robust difference-in-differences estimators},
	volume = {219},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407620301901},
	doi = {10.1016/j.jeconom.2020.06.003},
	abstract = {This article proposes doubly robust estimators for the average treatment effect on the treated (ATT) in difference-in-differences (DID) research designs. In contrast to alternative DID estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. We also derive the semiparametric efficiency bound for the ATT in DID designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. Furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. Finally, by paying particular attention to the estimation method used to estimate the nuisance parameters, we show that one can sometimes construct doubly robust DID estimators for the ATT that are also doubly robust for inference. Simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. Open-source software for implementing the proposed policy evaluation tools is available.},
	language = {en},
	number = {1},
	urldate = {2021-12-23},
	journal = {Journal of Econometrics},
	author = {Sant’Anna, Pedro H. C. and Zhao, Jun},
	year = {2020},
	keywords = {Causal inference, Difference-in-differences, Natural experiments, Panel data, Repeated cross-section data, Semiparametric efficiency},
	pages = {101--122},
	file = {Submitted Version:/Users/alyssabilinski/Zotero/storage/GLWFEHXV/Sant’Anna and Zhao - 2020 - Doubly robust difference-in-differences estimators.pdf:application/pdf;ScienceDirect Snapshot:/Users/alyssabilinski/Zotero/storage/MCKS9L77/S0304407620301901.html:text/html},
}


@article{zeldow_confounding_2021,
	title = {Confounding and regression adjustment in difference-in-differences studies},
	volume = {56},
	issn = {1475-6773},
	doi = {10.1111/1475-6773.13666},
	number = {5},
	journal = {Health Services Research},
	author = {Zeldow, Bret and Hatfield, Laura A.},
	year = {2021},
	pmid = {33978956},
	pmcid = {PMC8522571},
	keywords = {Bias, Computer Simulation, Confounding Factors, Epidemiologic, difference-in-differences, Humans, Linear Models, matching, Models, Statistical, parallel trends, regression adjustment, time-varying confounding},
	pages = {932--941},
	file = {Full Text:/Users/alyssabilinski/Zotero/storage/72U6QVNJ/Zeldow and Hatfield - 2021 - Confounding and regression adjustment in differenc.pdf:application/pdf},
}

@article{ryan_well-balanced_2018,
	title = {Well-{Balanced} or too {Matchy}–{Matchy}? {The} {Controversy} over {Matching} in {Difference}-in-{Differences}},
	volume = {53},
	issn = {1475-6773},
	shorttitle = {Well-{Balanced} or too {Matchy}–{Matchy}?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1475-6773.13015},
	doi = {10.1111/1475-6773.13015},
	language = {en},
	number = {6},
	urldate = {2021-12-23},
	journal = {Health Services Research},
	author = {Ryan, Andrew M.},
	year = {2018},
	pages = {4106--4110},
	file = {Snapshot:/Users/alyssabilinski/Zotero/storage/RW72DL56/1475-6773.html:text/html;Full Text:/Users/alyssabilinski/Zotero/storage/6S5P8M8L/Ryan - 2018 - Well-Balanced or too Matchy–Matchy The Controvers.pdf:application/pdf},
}



@article{jakiela_simple_2021,
	title = {Simple {Diagnostics} for {Two}-{Way} {Fixed} {Effects}},
	url = {http://arxiv.org/abs/2103.13229},
	abstract = {Difference-in-differences estimation is a widely used method of program evaluation. When treatment is implemented in different places at different times, researchers often use two-way fixed effects to control for location-specific and period-specific shocks. Such estimates can be severely biased when treatment effects change over time within treated units. I review the sources of this bias and propose several simple diagnostics for assessing its likely severity. I illustrate these tools through a case study of free primary education in Sub-Saharan Africa.},
	urldate = {2021-12-28},
	journal = {arXiv:2103.13229 [econ, q-fin]},
	author = {Jakiela, Pamela},
	month = mar,
	year = {2021},
	keywords = {Economics - General Economics},
	file = {arXiv Fulltext PDF:/Users/jonathanroth/Zotero/storage/X96VFBEM/Jakiela - 2021 - Simple Diagnostics for Two-Way Fixed Effects.pdf:application/pdf;arXiv.org Snapshot:/Users/jonathanroth/Zotero/storage/96BSN999/2103.html:text/html},
}


@article{arkhangelsky_synthetic_2021,
	title = {Synthetic {Difference}-in-{Differences}},
	volume = {111},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20190159},
	doi = {10.1257/aer.20190159},
	abstract = {We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference-in-differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this "synthetic difference-in-differences" estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality.},
	language = {en},
	number = {12},
	urldate = {2021-12-28},
	journal = {American Economic Review},
	author = {Arkhangelsky, Dmitry and Athey, Susan and Hirshberg, David A. and Imbens, Guido W. and Wager, Stefan},
	year = {2021},
	keywords = {Beverages, Cosmetics, Public Health, Food, Regulation, Single Equation Models, Single Variables: Panel Data Models, Spatio-temporal Models, Business Taxes and Subsidies including sales and value-added (VAT), State and Local Taxation, Subsidies, and Revenue, Health: Government Policy, Tobacco, Wine and Spirits},
	pages = {4088--4118},
	file = {Full Text:/Users/alyssabilinski/Zotero/storage/9X2FDIWG/Arkhangelsky et al. - 2021 - Synthetic Difference-in-Differences.pdf:application/pdf;Snapshot:/Users/alyssabilinski/Zotero/storage/3VCXINRW/articles.html:text/html},
}



@article{Abadie2006,
author = {Abadie, Alberto and Imbens, Guido W.},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadie, Imbens - 2006 - Large sample properties of matching estimators for average treatment effects.pdf:pdf},
journal = {Econometrica},
number = {1},
pages = {235--267},
title = {{Large sample properties of matching estimators for average treatment effects}},
volume = {74},
year = {2006}
}


@article{Abadie2012,
abstract = {Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the ?hot deck,? a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials. Matching estimators are widely used in statistical data analysis. However, the large sample distribution of matching estimators has been derived only for particular cases. This article establishes a martingale representation for matching estimators. This representation allows the use of martingale limit theorems to derive the large sample distribution of matching estimators. As an illustration of the applicability of the theory, we derive the asymptotic distribution of a matching estimator when matching is carried out without replacement, a result previously unavailable in the literature. In addition, we apply the techniques proposed in this article to derive a correction to the standard error of a sample mean when missing data are imputed using the ?hot deck,? a matching imputation method widely used in the Current Population Survey (CPS) and other large surveys in the social sciences. We demonstrate the empirical relevance of our methods using two Monte Carlo designs based on actual datasets. In these Monte Carlo exercises, the large sample distribution of matching estimators derived in this article provides an accurate approximation to the small sample behavior of these estimators. In addition, our simulations show that standard errors that do not take into account hot-deck imputation of missing data may be severely downward biased, while standard errors that incorporate the correction for hot-deck imputation perform extremely well. This article has online supplementary materials.},
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.1080/01621459.2012.682537},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadie, Imbens - 2012 - A Martingale Representation for Matching Estimators.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {hot-deck imputation,martingales,overt bias,treatment effects},
number = {498},
pages = {833--843},
title = {{A Martingale Representation for Matching Estimators}},
volume = {107},
year = {2012}
}

@article{Abadie2011,
abstract = {In Abadie and Imbens (2006), it was shown that simple nearest-neighbor matching estimators include a conditional bias term that converges to zero at a rate that may be slower than N1/2. As a result, match- ing estimators are not N1/2-consistent in general. In this article, we propose a bias correction that ren- ders matching estimators N1/2-consistent and asymptotically normal. To demonstrate the methods pro- posed in this article, we apply them to the National Supported Work (NSW) data, originally analyzed in Lalonde (1986). We also carry out a small simulation study based on the NSW example. In this simula- tion study, a simple implementation of the bias-corrected matching estimator performs well compared to both simple matching estimators and to regression estimators in terms of bias, root-mean-squared-error, and coverage rates. Software to compute the estimators proposed in this article is available on the au- thors' web pages (http://www.economics.harvard.edu/faculty/imbens/software.html) and documented in Abadie et al. (2003).},
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.1198/jbes.2009.07333},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadie, Imbens - 2011 - Bias-Corrected Matching Estimators for Average Treatment Effects.pdf:pdf},
isbn = {978-90-481-8767-6},
issn = {0735-0015},
journal = {Journal of Business {\&} Economic Statistics},
keywords = {Selection on observables, Treatment effects,selection on observables,treatment effects},
number = {1},
pages = {1--11},
title = {{Bias-Corrected Matching Estimators for Average Treatment Effects}},
volume = {29},
year = {2011}
}

@article{Ferman2021,
archivePrefix = {arXiv},
arxivId = {2006.16997v6},
author = {Ferman, Bruno},
eprint = {2006.16997v6},
file = {:C$\backslash$:/Users/psantanna/OneDrive - Microsoft/Desktop/DiD inference/2006.16997.pdf:pdf},
journal = {arXiv: 2006.16997},
keywords = {and for providing the,and suggestions,c12,c21,c23,c33,causal inference,chris taber for comments,exceptional research assistance,hypothesis testing,i also thank benjamin,i would like to,jel codes,lucas barros provided,permutation tests,randomization inference,sommers for useful discussions,thank luis alvarez and},
pages = {1--39},
title = {{Inference in Difference-in-Differences with Few Treated Units and Spatial Correlation}},
year = {2021}
}
@article{Abadie2008,
author = {Abadie, Alberto and Imbens, Guido W.},
doi = {10.3982/ECTA6474},
file = {:C$\backslash$:/Users/psantanna/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Abadie, Imbens - 2008 - On the Failure of the Bootstrap for Matching Estimators.pdf:pdf},
issn = {0012-9682},
journal = {Econometrica},
number = {6},
pages = {1537--1557},
title = {{On the Failure of the Bootstrap for Matching Estimators}},
url = {http://doi.wiley.com/10.3982/ECTA6474},
volume = {76},
year = {2008}
}



@article{ben-michael_augmented_2021,
	title = {The {Augmented} {Synthetic} {Control} {Method}},
	volume = {116},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2021.1929245},
	doi = {10.1080/01621459.2021.1929245},
	abstract = {The synthetic control method (SCM) is a popular approach for estimating the impact of a treatment on a single unit in panel data settings. The “synthetic control” is a weighted average of control units that balances the treated unit’s pretreatment outcomes and other covariates as closely as possible. A critical feature of the original proposal is to use SCM only when the fit on pretreatment outcomes is excellent. We propose Augmented SCM as an extension of SCM to settings where such pretreatment fit is infeasible. Analogous to bias correction for inexact matching, augmented SCM uses an outcome model to estimate the bias due to imperfect pretreatment fit and then de-biases the original SCM estimate. Our main proposal, which uses ridge regression as the outcome model, directly controls pretreatment fit while minimizing extrapolation from the convex hull. This estimator can also be expressed as a solution to a modified synthetic controls problem that allows negative weights on some donor units. We bound the estimation error of this approach under different data-generating processes, including a linear factor model, and show how regularization helps to avoid over-fitting to noise. We demonstrate gains from Augmented SCM with extensive simulation studies and apply this framework to estimate the impact of the 2012 Kansas tax cuts on economic growth. We implement the proposed method in the new augsynth R package.},
	number = {536},
	urldate = {2021-12-28},
	author = {Ben-Michael, Eli and Feller, Avi and Rothstein, Jesse},
	journal = {Journal of the American Statistical Association},
	year = {2021},
	pages = {1789--1803},
	file = {Submitted Version:/Users/alyssabilinski/Zotero/storage/69B9SCIS/Ben-Michael et al. - 2021 - The Augmented Synthetic Control Method.pdf:application/pdf;Snapshot:/Users/alyssabilinski/Zotero/storage/C9MJ3N4M/01621459.2021.html:text/html},
}


@article{abadie_using_2021,
	title = {Using {Synthetic} {Controls}: {Feasibility}, {Data} {Requirements}, and {Methodological} {Aspects}},
	volume = {59},
	issn = {0022-0515},
	shorttitle = {Using {Synthetic} {Controls}},
	url = {https://pubs.aeaweb.org/doi/10.1257/jel.20191450},
	doi = {10.1257/jel.20191450},
	abstract = {Probably because of their interpretability and transparent nature, synthetic controls have become widely applied in empirical research in economics and the social sciences. This article aims to provide practical guidance to researchers employing synthetic control methods. The article starts with an overview and an introduction to synthetic control estimation. The main sections discuss the advantages of the synthetic control framework as a research design, and describe the settings where synthetic controls provide reliable estimates and those where they may fail. The article closes with a discussion of recent extensions, related methods, and avenues for future research. (JEL B41, C32, C54, E23, F15, O47)},
	language = {en},
	number = {2},
	urldate = {2021-12-28},
	journal = {Journal of Economic Literature},
	author = {Abadie, Alberto},
	month = jun,
	year = {2021},
	pages = {391--425},
	file = {Abadie - 2021 - Using Synthetic Controls Feasibility, Data Requir.pdf:/Users/alyssabilinski/Zotero/storage/KXNASD2K/Abadie - 2021 - Using Synthetic Controls Feasibility, Data Requir.pdf:application/pdf},
}


@techreport{doudchenko_balancing_2016,
	type = {Working {Paper}},
	title = {Balancing, {Regression}, {Difference}-{In}-{Differences} and {Synthetic} {Control} {Methods}: {A} {Synthesis}},
	shorttitle = {Balancing, {Regression}, {Difference}-{In}-{Differences} and {Synthetic} {Control} {Methods}},
	url = {https://www.nber.org/papers/w22791},
	abstract = {In a seminal paper Abadie et al (2010) develop the synthetic control procedure for estimating the effect of a treatment, in the presence of a single treated unit and a number of control units, with pre-treatment outcomes observed for all units. The method constructs a set of weights such that covariates and pre-treatment outcomes of the treated unit are approximately matched by a weighted average of control units. The weights are restricted to be nonnegative and sum to one, which allows the procedure to obtain the weights even when the number of lagged outcomes is modest relative to the number of control units, a setting that is not uncommon in applications. In the current paper we propose a more general class of synthetic control estimators that allows researchers to relax some of the restrictions in the ADH method. We allow the weights to be negative, do not necessarily restrict the sum of the weights, and allow for a permanent additive difference between the treated unit and the controls, similar to difference-in-difference procedures. The weights directly minimize the distance between the lagged outcomes for the treated and the control units, using regularization methods to deal with a potentially large number of possible control units.},
	number = {22791},
	urldate = {2021-12-28},
	institution = {National Bureau of Economic Research},
	author = {Doudchenko, Nikolay and Imbens, Guido W.},
	year = {2016},
	doi = {10.3386/w22791},
	file = {Full Text PDF:/Users/alyssabilinski/Zotero/storage/49ECWZHP/Doudchenko and Imbens - 2016 - Balancing, Regression, Difference-In-Differences a.pdf:application/pdf},
}
